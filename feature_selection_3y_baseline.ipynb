{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85047a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2b7fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402f50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load concat features and metadata\n",
    "data = np.load(\"./data/doc_features/transcript_componenttext_2012_2_features.npz\", allow_pickle=True)\n",
    "\n",
    "Xc = data[\"X_concat\"]        # shape (N_docs, 2*D)\n",
    "tids = data[\"transcriptids\"]  # same order\n",
    "meta = pd.read_csv(\"./data/doc_features/transcript_componenttext_2012_2_features_meta.csv\")\n",
    "\n",
    "meta_unique = (\n",
    "    meta[[\"transcriptid\", \"SUESCORE\", \"label\"]]\n",
    "    .drop_duplicates(subset=\"transcriptid\", keep=\"first\")\n",
    "    .set_index(\"transcriptid\")\n",
    ")\n",
    "\n",
    "# 3) build a mask over tids\n",
    "mask = np.isin(tids, meta_unique.index)\n",
    "\n",
    "Xc = Xc[mask]\n",
    "tids = tids[mask]\n",
    "\n",
    "# 2) Build labels and mask\n",
    "meta = meta.assign(label=lambda df: df.SUESCORE.map(lambda s: 1 if s>=0.5 else (0 if s<=-0.5 else np.nan)))\n",
    "mask = meta.label.notna().values\n",
    "X_test_all_feat, y_test = Xc[mask], meta.loc[mask, \"label\"].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb869ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load concat features and metadata\n",
    "data = np.load(\"./data/doc_features/transcript_componenttext_2012_1_features.npz\", allow_pickle=True)\n",
    "\n",
    "Xc = data[\"X_concat\"]        # shape (N_docs, 2*D)\n",
    "tids = data[\"transcriptids\"]  # same order\n",
    "meta = pd.read_csv(\"./data/doc_features/transcript_componenttext_2012_1_features_meta.csv\")\n",
    "\n",
    "meta_unique = (\n",
    "    meta[[\"transcriptid\", \"SUESCORE\", \"label\"]]\n",
    "    .drop_duplicates(subset=\"transcriptid\", keep=\"first\")\n",
    "    .set_index(\"transcriptid\")\n",
    ")\n",
    "\n",
    "# 3) build a mask over tids\n",
    "mask = np.isin(tids, meta_unique.index)\n",
    "\n",
    "Xc = Xc[mask]\n",
    "tids = tids[mask]\n",
    "\n",
    "# 2) Build labels and mask\n",
    "meta = meta.assign(label=lambda df: df.SUESCORE.map(lambda s: 1 if s>=0.5 else (0 if s<=-0.5 else np.nan)))\n",
    "mask = meta.label.notna().values\n",
    "X_val_all_feat, y_val = Xc[mask], meta.loc[mask, \"label\"].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421cd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_paths = [\n",
    "    \"./data/doc_features/transcript_componenttext_2010_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2010_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2011_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2011_2_features.npz\",\n",
    "    # \"./data/doc_features/transcript_componenttext_2012_1_features.npz\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6c412b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Xc shape: (3561, 32768)\n",
      "Combined y shape:  (3561,)\n"
     ]
    }
   ],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for npz_path in npz_paths:\n",
    " \n",
    "    base = os.path.splitext(os.path.basename(npz_path))[0]      \n",
    "    csv_path = os.path.join(\n",
    "        os.path.dirname(npz_path),\n",
    "        base + \"_meta.csv\"                                       \n",
    "    )\n",
    "\n",
    "\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    # X_concat = data[\"X_concat\"]      # (N_docs, 2*D)\n",
    "    X_concat = data[\"X_concat\"]  \n",
    "    tids = data[\"transcriptids\"]    \n",
    "\n",
    "\n",
    "    meta = pd.read_csv(csv_path)\n",
    "\n",
    "    meta_unique = (\n",
    "        meta[[\"transcriptid\", \"SUESCORE\", \"label\"]]\n",
    "        .drop_duplicates(subset=\"transcriptid\", keep=\"first\")\n",
    "        .set_index(\"transcriptid\")\n",
    "    )\n",
    "\n",
    "    mask_ids = np.isin(tids, meta_unique.index)\n",
    "    X_filt = X_concat[mask_ids]\n",
    "    tids_filt = np.array(tids)[mask_ids]\n",
    "\n",
    "\n",
    "    lab_df = meta.assign(\n",
    "        label=lambda df: df.SUESCORE.map(\n",
    "            lambda s: 1 if s >= 0.5 else (0 if s <= -0.5 else np.nan)\n",
    "        )\n",
    "    )\n",
    "    mask_label = lab_df.label.notna().values\n",
    "    # apply the same mask in the same order as the CSV, so we use .loc on lab_df\n",
    "    # but first filter lab_df to only those transcriptids in tids_filt\n",
    "    Xc, y = X_filt[mask_label], meta.loc[mask_label, \"label\"].astype(int).values\n",
    "    \n",
    "    # now align X and y\n",
    "    # X_final = X_filt[lab_sub.label.notna()]\n",
    "    # y_final = lab_sub.label.astype(int).values\n",
    "\n",
    "    # collect\n",
    "    X_list.append(Xc)\n",
    "    y_list.append(y)\n",
    "\n",
    "# 2. concatenate all files together\n",
    "Xc = np.vstack(X_list)   # shape: (sum_i N_i, 2*D)\n",
    "y  = np.concatenate(y_list)  # shape: (sum_i N_i,)\n",
    "\n",
    "print(\"Combined Xc shape:\", Xc.shape)\n",
    "print(\"Combined y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6823001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced X shape: (1450, 32768)\n",
      "Balanced y counts: [725 725]\n"
     ]
    }
   ],
   "source": [
    "# forced resampling\n",
    "idx0 = np.where(y == 0)[0]\n",
    "idx1 = np.where(y == 1)[0]\n",
    "\n",
    "n = min(len(idx0), len(idx1))\n",
    "\n",
    "sel0 = np.random.choice(idx0, size=n, replace=False)\n",
    "sel1 = np.random.choice(idx1, size=n, replace=False)\n",
    "\n",
    "sel = np.concatenate([sel0, sel1])\n",
    "np.random.shuffle(sel)\n",
    "\n",
    "# slice out your balanced subset\n",
    "Xc = Xc[sel]\n",
    "y = y[sel]\n",
    "\n",
    "print(\"Balanced X shape:\", Xc.shape)\n",
    "print(\"Balanced y counts:\", np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e826e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  1: 'max' feature #3081 (t = 9.02)\n",
      "Rank  2: 'max' feature #6700 (t = 8.74)\n",
      "Rank  3: 'mean' feature #4983 (t = 8.61)\n",
      "Rank  4: 'max' feature #188 (t = 8.60)\n",
      "Rank  5: 'max' feature #7429 (t = 8.55)\n",
      "Rank  6: 'max' feature #6893 (t = 8.42)\n",
      "Rank  7: 'max' feature #3581 (t = 8.32)\n",
      "Rank  8: 'max' feature #2867 (t = 8.24)\n",
      "Rank  9: 'max' feature #957 (t = 8.18)\n",
      "Rank 10: 'mean' feature #1881 (t = 8.14)\n",
      "Rank 11: 'max' feature #2445 (t = 8.13)\n",
      "Rank 12: 'max' feature #4808 (t = 8.07)\n",
      "Rank 13: 'mean' feature #7456 (t = 8.01)\n",
      "Rank 14: 'max' feature #1818 (t = 7.92)\n",
      "Rank 15: 'max' feature #7546 (t = 7.92)\n",
      "Rank 16: 'mean' feature #1211 (t = 7.87)\n",
      "Rank 17: 'max' feature #5888 (t = 7.86)\n",
      "Rank 18: 'mean' feature #6902 (t = 7.79)\n",
      "Rank 19: 'mean' feature #3878 (t = 7.74)\n",
      "Rank 20: 'max' feature #4107 (t = 7.71)\n",
      "Rank 21: 'max' feature #1963 (t = 7.60)\n",
      "Rank 22: 'mean' feature #7357 (t = 7.57)\n",
      "Rank 23: 'max' feature #8086 (t = 7.54)\n",
      "Rank 24: 'max' feature #5425 (t = 7.46)\n",
      "Rank 25: 'mean' feature #1547 (t = 7.44)\n",
      "Rank 26: 'max' feature #5353 (t = 7.43)\n",
      "Rank 27: 'max' feature #4883 (t = 7.37)\n",
      "Rank 28: 'mean' feature #2845 (t = 7.34)\n",
      "Rank 29: 'mean' feature #1174 (t = 7.32)\n",
      "Rank 30: 'max' feature #7489 (t = 7.28)\n",
      "Rank 31: 'max' feature #4620 (t = 7.27)\n",
      "Rank 32: 'max' feature #6085 (t = 7.24)\n",
      "Rank 33: 'max' feature #7570 (t = 7.24)\n",
      "Rank 34: 'max' feature #4673 (t = 7.23)\n",
      "Rank 35: 'mean' feature #5240 (t = 7.22)\n",
      "Rank 36: 'mean' feature #1541 (t = 7.21)\n",
      "Rank 37: 'mean' feature #6703 (t = 7.18)\n",
      "Rank 38: 'mean' feature #3549 (t = 7.16)\n",
      "Rank 39: 'mean' feature #4013 (t = 7.12)\n",
      "Rank 40: 'mean' feature #2366 (t = 7.11)\n",
      "Rank 41: 'max' feature #3929 (t = 7.02)\n",
      "Rank 42: 'mean' feature #6770 (t = 7.00)\n",
      "Rank 43: 'max' feature #6386 (t = 6.97)\n",
      "Rank 44: 'mean' feature #133 (t = 6.97)\n",
      "Rank 45: 'max' feature #2368 (t = 6.94)\n",
      "Rank 46: 'mean' feature #1712 (t = 6.92)\n",
      "Rank 47: 'max' feature #7300 (t = 6.89)\n",
      "Rank 48: 'max' feature #3866 (t = 6.86)\n",
      "Rank 49: 'max' feature #6007 (t = 6.85)\n",
      "Rank 50: 'mean' feature #2488 (t = 6.84)\n",
      "Rank 51: 'max' feature #6178 (t = 6.83)\n",
      "Rank 52: 'max' feature #4257 (t = 6.82)\n",
      "Rank 53: 'mean' feature #4600 (t = 6.79)\n",
      "Rank 54: 'max' feature #6973 (t = 6.77)\n",
      "Rank 55: 'max' feature #7604 (t = 6.76)\n",
      "Rank 56: 'max' feature #2198 (t = 6.75)\n",
      "Rank 57: 'max' feature #1773 (t = 6.74)\n",
      "Rank 58: 'max' feature #1310 (t = 6.73)\n",
      "Rank 59: 'mean' feature #4646 (t = 6.71)\n",
      "Rank 60: 'mean' feature #6332 (t = 6.70)\n",
      "Rank 61: 'max' feature #6157 (t = 6.67)\n",
      "Rank 62: 'mean' feature #1457 (t = 6.63)\n",
      "Rank 63: 'max' feature #1621 (t = 6.62)\n",
      "Rank 64: 'mean' feature #3585 (t = 6.58)\n",
      "Rank 65: 'mean' feature #255 (t = 6.58)\n",
      "Rank 66: 'max' feature #4819 (t = 6.56)\n",
      "Rank 67: 'max' feature #2501 (t = 6.55)\n",
      "Rank 68: 'max' feature #1053 (t = 6.55)\n",
      "Rank 69: 'max' feature #2949 (t = 6.53)\n",
      "Rank 70: 'max' feature #2687 (t = 6.49)\n",
      "Rank 71: 'max' feature #3445 (t = 6.48)\n",
      "Rank 72: 'max' feature #4292 (t = 6.45)\n",
      "Rank 73: 'mean' feature #2281 (t = 6.44)\n",
      "Rank 74: 'mean' feature #1446 (t = 6.44)\n",
      "Rank 75: 'max' feature #2408 (t = 6.41)\n",
      "Rank 76: 'mean' feature #523 (t = 6.41)\n",
      "Rank 77: 'max' feature #7047 (t = 6.40)\n",
      "Rank 78: 'mean' feature #940 (t = 6.39)\n",
      "Rank 79: 'max' feature #351 (t = 6.39)\n",
      "Rank 80: 'mean' feature #5776 (t = 6.34)\n",
      "Rank 81: 'max' feature #6652 (t = 6.34)\n",
      "Rank 82: 'mean' feature #635 (t = 6.34)\n",
      "Rank 83: 'mean' feature #1635 (t = 6.33)\n",
      "Rank 84: 'max' feature #3026 (t = 6.33)\n",
      "Rank 85: 'mean' feature #7246 (t = 6.32)\n",
      "Rank 86: 'mean' feature #7797 (t = 6.30)\n",
      "Rank 87: 'mean' feature #4485 (t = 6.28)\n",
      "Rank 88: 'mean' feature #3749 (t = 6.27)\n",
      "Rank 89: 'mean' feature #1390 (t = 6.27)\n",
      "Rank 90: 'max' feature #5808 (t = 6.26)\n",
      "Rank 91: 'mean' feature #3874 (t = 6.25)\n",
      "Rank 92: 'max' feature #7070 (t = 6.24)\n",
      "Rank 93: 'mean' feature #2440 (t = 6.23)\n",
      "Rank 94: 'max' feature #4848 (t = 6.21)\n",
      "Rank 95: 'mean' feature #3494 (t = 6.18)\n",
      "Rank 96: 'max' feature #4707 (t = 6.14)\n",
      "Rank 97: 'mean' feature #2982 (t = 6.14)\n",
      "Rank 98: 'max' feature #5561 (t = 6.12)\n",
      "Rank 99: 'max' feature #2872 (t = 6.10)\n",
      "Rank 100: 'mean' feature #7589 (t = 6.09)\n",
      "Rank 101: 'max' feature #2518 (t = 6.09)\n",
      "Rank 102: 'mean' feature #720 (t = 6.09)\n",
      "Rank 103: 'max' feature #2857 (t = 6.09)\n",
      "Rank 104: 'max' feature #118 (t = 6.08)\n",
      "Rank 105: 'max' feature #4543 (t = 6.07)\n",
      "Rank 106: 'max' feature #4750 (t = 6.06)\n",
      "Rank 107: 'mean' feature #1477 (t = 6.05)\n",
      "Rank 108: 'mean' feature #6211 (t = 6.05)\n",
      "Rank 109: 'mean' feature #5048 (t = 6.04)\n",
      "Rank 110: 'mean' feature #615 (t = 6.04)\n",
      "Rank 111: 'max' feature #522 (t = 6.03)\n",
      "Rank 112: 'mean' feature #5485 (t = 6.03)\n",
      "Rank 113: 'max' feature #5690 (t = 6.01)\n",
      "Rank 114: 'mean' feature #6000 (t = 6.00)\n",
      "Rank 115: 'mean' feature #945 (t = 5.98)\n",
      "Rank 116: 'max' feature #3449 (t = 5.97)\n",
      "Rank 117: 'max' feature #970 (t = 5.96)\n",
      "Rank 118: 'max' feature #5848 (t = 5.95)\n",
      "Rank 119: 'mean' feature #77 (t = 5.95)\n",
      "Rank 120: 'max' feature #5928 (t = 5.93)\n",
      "Rank 121: 'mean' feature #6289 (t = 5.93)\n",
      "Rank 122: 'max' feature #3386 (t = 5.92)\n",
      "Rank 123: 'mean' feature #5103 (t = 5.91)\n",
      "Rank 124: 'mean' feature #3375 (t = 5.89)\n",
      "Rank 125: 'mean' feature #2925 (t = 5.89)\n",
      "Rank 126: 'mean' feature #5230 (t = 5.85)\n",
      "Rank 127: 'max' feature #4445 (t = 5.83)\n",
      "Rank 128: 'max' feature #5363 (t = 5.82)\n",
      "Rank 129: 'mean' feature #2415 (t = 5.80)\n",
      "Rank 130: 'max' feature #540 (t = 5.80)\n",
      "Rank 131: 'mean' feature #3609 (t = 5.78)\n",
      "Rank 132: 'max' feature #52 (t = 5.78)\n",
      "Rank 133: 'mean' feature #939 (t = 5.78)\n",
      "Rank 134: 'max' feature #76 (t = 5.76)\n",
      "Rank 135: 'mean' feature #6139 (t = 5.74)\n",
      "Rank 136: 'max' feature #5049 (t = 5.74)\n",
      "Rank 137: 'max' feature #219 (t = 5.73)\n",
      "Rank 138: 'max' feature #1925 (t = 5.72)\n",
      "Rank 139: 'mean' feature #7788 (t = 5.72)\n",
      "Rank 140: 'mean' feature #2310 (t = 5.71)\n",
      "Rank 141: 'max' feature #6550 (t = 5.71)\n",
      "Rank 142: 'max' feature #1733 (t = 5.71)\n",
      "Rank 143: 'max' feature #5997 (t = 5.70)\n",
      "Rank 144: 'max' feature #4532 (t = 5.70)\n",
      "Rank 145: 'max' feature #847 (t = 5.68)\n",
      "Rank 146: 'mean' feature #1074 (t = 5.68)\n",
      "Rank 147: 'max' feature #2213 (t = 5.68)\n",
      "Rank 148: 'mean' feature #5231 (t = 5.66)\n",
      "Rank 149: 'max' feature #1320 (t = 5.64)\n",
      "Rank 150: 'mean' feature #257 (t = 5.64)\n",
      "Rank 151: 'max' feature #7249 (t = 5.62)\n",
      "Rank 152: 'mean' feature #4730 (t = 5.62)\n",
      "Rank 153: 'mean' feature #1519 (t = 5.61)\n",
      "Rank 154: 'mean' feature #7722 (t = 5.61)\n",
      "Rank 155: 'max' feature #4585 (t = 5.60)\n",
      "Rank 156: 'mean' feature #2133 (t = 5.57)\n",
      "Rank 157: 'max' feature #5493 (t = 5.56)\n",
      "Rank 158: 'mean' feature #6254 (t = 5.56)\n",
      "Rank 159: 'mean' feature #8097 (t = 5.55)\n",
      "Rank 160: 'mean' feature #5767 (t = 5.55)\n",
      "Rank 161: 'mean' feature #6370 (t = 5.54)\n",
      "Rank 162: 'max' feature #4384 (t = 5.54)\n",
      "Rank 163: 'max' feature #4308 (t = 5.54)\n",
      "Rank 164: 'mean' feature #1051 (t = 5.54)\n",
      "Rank 165: 'max' feature #1708 (t = 5.53)\n",
      "Rank 166: 'mean' feature #4283 (t = 5.52)\n",
      "Rank 167: 'mean' feature #4177 (t = 5.51)\n",
      "Rank 168: 'mean' feature #4075 (t = 5.50)\n",
      "Rank 169: 'max' feature #4158 (t = 5.48)\n",
      "Rank 170: 'max' feature #83 (t = 5.48)\n",
      "Rank 171: 'mean' feature #6237 (t = 5.48)\n",
      "Rank 172: 'mean' feature #5343 (t = 5.47)\n",
      "Rank 173: 'max' feature #29 (t = 5.46)\n",
      "Rank 174: 'max' feature #4455 (t = 5.46)\n",
      "Rank 175: 'max' feature #7629 (t = 5.44)\n",
      "Rank 176: 'mean' feature #2798 (t = 5.44)\n",
      "Rank 177: 'mean' feature #6570 (t = 5.44)\n",
      "Rank 178: 'max' feature #8139 (t = 5.43)\n",
      "Rank 179: 'mean' feature #2442 (t = 5.43)\n",
      "Rank 180: 'max' feature #4884 (t = 5.42)\n",
      "Rank 181: 'max' feature #2541 (t = 5.40)\n",
      "Rank 182: 'mean' feature #2813 (t = 5.39)\n",
      "Rank 183: 'mean' feature #1090 (t = 5.38)\n",
      "Rank 184: 'mean' feature #5486 (t = 5.37)\n",
      "Rank 185: 'mean' feature #4152 (t = 5.36)\n",
      "Rank 186: 'mean' feature #4988 (t = 5.36)\n",
      "Rank 187: 'mean' feature #2319 (t = 5.36)\n",
      "Rank 188: 'mean' feature #5078 (t = 5.36)\n",
      "Rank 189: 'max' feature #7382 (t = 5.35)\n",
      "Rank 190: 'max' feature #6802 (t = 5.35)\n",
      "Rank 191: 'mean' feature #4007 (t = 5.32)\n",
      "Rank 192: 'mean' feature #4408 (t = 5.32)\n",
      "Rank 193: 'max' feature #4471 (t = 5.31)\n",
      "Rank 194: 'max' feature #7503 (t = 5.31)\n",
      "Rank 195: 'max' feature #4001 (t = 5.26)\n",
      "Rank 196: 'max' feature #3048 (t = 5.26)\n",
      "Rank 197: 'max' feature #3360 (t = 5.26)\n",
      "Rank 198: 'mean' feature #4060 (t = 5.25)\n",
      "Rank 199: 'mean' feature #4789 (t = 5.24)\n",
      "Rank 200: 'mean' feature #5746 (t = 5.24)\n",
      "Rank 201: 'mean' feature #5317 (t = 5.24)\n",
      "Rank 202: 'max' feature #903 (t = 5.24)\n",
      "Rank 203: 'max' feature #6508 (t = 5.23)\n",
      "Rank 204: 'mean' feature #5708 (t = 5.20)\n",
      "Rank 205: 'max' feature #2295 (t = 5.20)\n",
      "Rank 206: 'mean' feature #1092 (t = 5.20)\n",
      "Rank 207: 'mean' feature #6889 (t = 5.19)\n",
      "Rank 208: 'max' feature #1782 (t = 5.19)\n",
      "Rank 209: 'mean' feature #8122 (t = 5.19)\n",
      "Rank 210: 'max' feature #7555 (t = 5.18)\n",
      "Rank 211: 'max' feature #1787 (t = 5.18)\n",
      "Rank 212: 'max' feature #7261 (t = 5.18)\n",
      "Rank 213: 'mean' feature #7977 (t = 5.18)\n",
      "Rank 214: 'mean' feature #3050 (t = 5.18)\n",
      "Rank 215: 'max' feature #2790 (t = 5.17)\n",
      "Rank 216: 'max' feature #776 (t = 5.16)\n",
      "Rank 217: 'max' feature #2577 (t = 5.16)\n",
      "Rank 218: 'max' feature #1329 (t = 5.14)\n",
      "Rank 219: 'max' feature #4017 (t = 5.14)\n",
      "Rank 220: 'max' feature #7830 (t = 5.14)\n",
      "Rank 221: 'mean' feature #3624 (t = 5.14)\n",
      "Rank 222: 'mean' feature #7112 (t = 5.13)\n",
      "Rank 223: 'max' feature #7591 (t = 5.11)\n",
      "Rank 224: 'mean' feature #6654 (t = 5.11)\n",
      "Rank 225: 'max' feature #5734 (t = 5.11)\n",
      "Rank 226: 'mean' feature #1406 (t = 5.10)\n",
      "Rank 227: 'max' feature #6813 (t = 5.10)\n",
      "Rank 228: 'mean' feature #916 (t = 5.09)\n",
      "Rank 229: 'max' feature #1738 (t = 5.09)\n",
      "Rank 230: 'mean' feature #6486 (t = 5.08)\n",
      "Rank 231: 'mean' feature #4502 (t = 5.08)\n",
      "Rank 232: 'mean' feature #1640 (t = 5.08)\n",
      "Rank 233: 'max' feature #378 (t = 5.08)\n",
      "Rank 234: 'max' feature #7218 (t = 5.07)\n",
      "Rank 235: 'mean' feature #4418 (t = 5.07)\n",
      "Rank 236: 'mean' feature #5752 (t = 5.06)\n",
      "Rank 237: 'mean' feature #4892 (t = 5.06)\n",
      "Rank 238: 'max' feature #16 (t = 5.05)\n",
      "Rank 239: 'max' feature #109 (t = 5.04)\n",
      "Rank 240: 'max' feature #3448 (t = 5.04)\n",
      "Rank 241: 'mean' feature #6470 (t = 5.04)\n",
      "Rank 242: 'max' feature #2654 (t = 5.03)\n",
      "Rank 243: 'mean' feature #5523 (t = 5.02)\n",
      "Rank 244: 'mean' feature #6141 (t = 5.02)\n",
      "Rank 245: 'mean' feature #2004 (t = 5.01)\n",
      "Rank 246: 'mean' feature #585 (t = 5.00)\n",
      "Rank 247: 'max' feature #141 (t = 5.00)\n",
      "Rank 248: 'max' feature #1096 (t = 5.00)\n",
      "Rank 249: 'max' feature #5655 (t = 5.00)\n",
      "Rank 250: 'mean' feature #110 (t = 5.00)\n",
      "Rank 251: 'max' feature #1403 (t = 4.99)\n",
      "Rank 252: 'max' feature #5203 (t = 4.98)\n",
      "Rank 253: 'mean' feature #2542 (t = 4.97)\n",
      "Rank 254: 'mean' feature #7452 (t = 4.97)\n",
      "Rank 255: 'max' feature #7890 (t = 4.97)\n",
      "Rank 256: 'max' feature #3090 (t = 4.97)\n",
      "Rank 257: 'mean' feature #7335 (t = 4.96)\n",
      "Rank 258: 'mean' feature #7516 (t = 4.95)\n",
      "Rank 259: 'max' feature #4717 (t = 4.95)\n",
      "Rank 260: 'mean' feature #1462 (t = 4.94)\n",
      "Rank 261: 'mean' feature #7522 (t = 4.94)\n",
      "Rank 262: 'max' feature #1119 (t = 4.93)\n",
      "Rank 263: 'mean' feature #1562 (t = 4.93)\n",
      "Rank 264: 'max' feature #4828 (t = 4.92)\n",
      "Rank 265: 'mean' feature #4221 (t = 4.92)\n",
      "Rank 266: 'max' feature #972 (t = 4.92)\n",
      "Rank 267: 'mean' feature #1700 (t = 4.91)\n",
      "Rank 268: 'max' feature #6381 (t = 4.90)\n",
      "Rank 269: 'mean' feature #646 (t = 4.90)\n",
      "Rank 270: 'max' feature #3269 (t = 4.89)\n",
      "Rank 271: 'max' feature #2074 (t = 4.88)\n",
      "Rank 272: 'mean' feature #5041 (t = 4.86)\n",
      "Rank 273: 'max' feature #5142 (t = 4.86)\n",
      "Rank 274: 'max' feature #4525 (t = 4.86)\n",
      "Rank 275: 'mean' feature #4264 (t = 4.85)\n",
      "Rank 276: 'max' feature #220 (t = 4.85)\n",
      "Rank 277: 'mean' feature #7177 (t = 4.84)\n",
      "Rank 278: 'max' feature #244 (t = 4.84)\n",
      "Rank 279: 'mean' feature #3820 (t = 4.83)\n",
      "Rank 280: 'max' feature #1921 (t = 4.83)\n",
      "Rank 281: 'mean' feature #5867 (t = 4.82)\n",
      "Rank 282: 'mean' feature #7459 (t = 4.82)\n",
      "Rank 283: 'max' feature #4403 (t = 4.81)\n",
      "Rank 284: 'mean' feature #3471 (t = 4.80)\n",
      "Rank 285: 'mean' feature #979 (t = 4.80)\n",
      "Rank 286: 'max' feature #7891 (t = 4.79)\n",
      "Rank 287: 'max' feature #290 (t = 4.79)\n",
      "Rank 288: 'max' feature #7691 (t = 4.79)\n",
      "Rank 289: 'mean' feature #455 (t = 4.79)\n",
      "Rank 290: 'mean' feature #723 (t = 4.79)\n",
      "Rank 291: 'max' feature #2426 (t = 4.79)\n",
      "Rank 292: 'mean' feature #5701 (t = 4.79)\n",
      "Rank 293: 'mean' feature #540 (t = 4.78)\n",
      "Rank 294: 'mean' feature #5289 (t = 4.78)\n",
      "Rank 295: 'mean' feature #7462 (t = 4.78)\n",
      "Rank 296: 'mean' feature #4842 (t = 4.78)\n",
      "Rank 297: 'max' feature #2933 (t = 4.77)\n",
      "Rank 298: 'mean' feature #5448 (t = 4.77)\n",
      "Rank 299: 'mean' feature #2325 (t = 4.76)\n",
      "Rank 300: 'mean' feature #5250 (t = 4.75)\n",
      "Rank 301: 'mean' feature #4604 (t = 4.75)\n",
      "Rank 302: 'mean' feature #7093 (t = 4.75)\n",
      "Rank 303: 'mean' feature #8022 (t = 4.75)\n",
      "Rank 304: 'max' feature #2001 (t = 4.74)\n",
      "Rank 305: 'max' feature #3032 (t = 4.73)\n",
      "Rank 306: 'mean' feature #2792 (t = 4.73)\n",
      "Rank 307: 'mean' feature #1988 (t = 4.73)\n",
      "Rank 308: 'mean' feature #2591 (t = 4.73)\n",
      "Rank 309: 'mean' feature #3500 (t = 4.72)\n",
      "Rank 310: 'max' feature #5982 (t = 4.72)\n",
      "Rank 311: 'max' feature #4036 (t = 4.72)\n",
      "Rank 312: 'max' feature #5131 (t = 4.71)\n",
      "Rank 313: 'mean' feature #1315 (t = 4.71)\n",
      "Rank 314: 'mean' feature #4564 (t = 4.71)\n",
      "Rank 315: 'max' feature #7929 (t = 4.69)\n",
      "Rank 316: 'max' feature #1494 (t = 4.69)\n",
      "Rank 317: 'max' feature #3070 (t = 4.69)\n",
      "Rank 318: 'mean' feature #1739 (t = 4.69)\n",
      "Rank 319: 'mean' feature #6448 (t = 4.69)\n",
      "Rank 320: 'max' feature #4305 (t = 4.69)\n",
      "Rank 321: 'mean' feature #5817 (t = 4.68)\n",
      "Rank 322: 'mean' feature #4372 (t = 4.68)\n",
      "Rank 323: 'max' feature #6934 (t = 4.68)\n",
      "Rank 324: 'max' feature #7586 (t = 4.68)\n",
      "Rank 325: 'mean' feature #3082 (t = 4.67)\n",
      "Rank 326: 'mean' feature #2233 (t = 4.67)\n",
      "Rank 327: 'mean' feature #261 (t = 4.67)\n",
      "Rank 328: 'max' feature #5163 (t = 4.67)\n",
      "Rank 329: 'max' feature #1301 (t = 4.67)\n",
      "Rank 330: 'mean' feature #3990 (t = 4.67)\n",
      "Rank 331: 'mean' feature #2439 (t = 4.66)\n",
      "Rank 332: 'mean' feature #4838 (t = 4.66)\n",
      "Rank 333: 'max' feature #1990 (t = 4.66)\n",
      "Rank 334: 'mean' feature #3534 (t = 4.66)\n",
      "Rank 335: 'max' feature #844 (t = 4.65)\n",
      "Rank 336: 'max' feature #2528 (t = 4.65)\n",
      "Rank 337: 'max' feature #1641 (t = 4.65)\n",
      "Rank 338: 'max' feature #2513 (t = 4.65)\n",
      "Rank 339: 'max' feature #5945 (t = 4.65)\n",
      "Rank 340: 'max' feature #5937 (t = 4.65)\n",
      "Rank 341: 'mean' feature #6935 (t = 4.64)\n",
      "Rank 342: 'mean' feature #2625 (t = 4.64)\n",
      "Rank 343: 'max' feature #5590 (t = 4.64)\n",
      "Rank 344: 'max' feature #6836 (t = 4.64)\n",
      "Rank 345: 'mean' feature #3304 (t = 4.64)\n",
      "Rank 346: 'mean' feature #8121 (t = 4.64)\n",
      "Rank 347: 'mean' feature #5234 (t = 4.64)\n",
      "Rank 348: 'mean' feature #5142 (t = 4.63)\n",
      "Rank 349: 'max' feature #6021 (t = 4.63)\n",
      "Rank 350: 'max' feature #1916 (t = 4.63)\n",
      "Rank 351: 'max' feature #6458 (t = 4.63)\n",
      "Rank 352: 'max' feature #6359 (t = 4.63)\n",
      "Rank 353: 'mean' feature #7685 (t = 4.63)\n",
      "Rank 354: 'mean' feature #2479 (t = 4.63)\n",
      "Rank 355: 'max' feature #926 (t = 4.62)\n",
      "Rank 356: 'max' feature #2083 (t = 4.62)\n",
      "Rank 357: 'mean' feature #4543 (t = 4.62)\n",
      "Rank 358: 'max' feature #7734 (t = 4.62)\n",
      "Rank 359: 'max' feature #6416 (t = 4.62)\n",
      "Rank 360: 'mean' feature #2903 (t = 4.62)\n",
      "Rank 361: 'mean' feature #4655 (t = 4.62)\n",
      "Rank 362: 'max' feature #221 (t = 4.61)\n",
      "Rank 363: 'mean' feature #5612 (t = 4.61)\n",
      "Rank 364: 'max' feature #7870 (t = 4.60)\n",
      "Rank 365: 'mean' feature #2443 (t = 4.60)\n",
      "Rank 366: 'mean' feature #3758 (t = 4.60)\n",
      "Rank 367: 'max' feature #5066 (t = 4.60)\n",
      "Rank 368: 'mean' feature #5282 (t = 4.59)\n",
      "Rank 369: 'mean' feature #4531 (t = 4.59)\n",
      "Rank 370: 'max' feature #2031 (t = 4.59)\n",
      "Rank 371: 'mean' feature #3562 (t = 4.59)\n",
      "Rank 372: 'max' feature #7062 (t = 4.58)\n",
      "Rank 373: 'mean' feature #1078 (t = 4.58)\n",
      "Rank 374: 'max' feature #2904 (t = 4.57)\n",
      "Rank 375: 'max' feature #7320 (t = 4.56)\n",
      "Rank 376: 'mean' feature #4948 (t = 4.56)\n",
      "Rank 377: 'mean' feature #102 (t = 4.56)\n",
      "Rank 378: 'max' feature #5969 (t = 4.55)\n",
      "Rank 379: 'max' feature #606 (t = 4.55)\n",
      "Rank 380: 'max' feature #7714 (t = 4.55)\n",
      "Rank 381: 'max' feature #1245 (t = 4.55)\n",
      "Rank 382: 'max' feature #3986 (t = 4.54)\n",
      "Rank 383: 'mean' feature #4552 (t = 4.54)\n",
      "Rank 384: 'max' feature #3941 (t = 4.54)\n",
      "Rank 385: 'mean' feature #5370 (t = 4.54)\n",
      "Rank 386: 'max' feature #5780 (t = 4.54)\n",
      "Rank 387: 'mean' feature #2505 (t = 4.54)\n",
      "Rank 388: 'max' feature #7961 (t = 4.53)\n",
      "Rank 389: 'max' feature #4676 (t = 4.53)\n",
      "Rank 390: 'mean' feature #1645 (t = 4.53)\n",
      "Rank 391: 'mean' feature #284 (t = 4.53)\n",
      "Rank 392: 'mean' feature #3743 (t = 4.52)\n",
      "Rank 393: 'mean' feature #1773 (t = 4.52)\n",
      "Rank 394: 'max' feature #969 (t = 4.52)\n",
      "Rank 395: 'max' feature #1810 (t = 4.52)\n",
      "Rank 396: 'max' feature #3441 (t = 4.52)\n",
      "Rank 397: 'mean' feature #2775 (t = 4.51)\n",
      "Rank 398: 'mean' feature #5143 (t = 4.51)\n",
      "Rank 399: 'max' feature #5036 (t = 4.50)\n",
      "Rank 400: 'max' feature #2225 (t = 4.50)\n",
      "Rank 401: 'mean' feature #6034 (t = 4.50)\n",
      "Rank 402: 'max' feature #6237 (t = 4.50)\n",
      "Rank 403: 'max' feature #1262 (t = 4.50)\n",
      "Rank 404: 'max' feature #6418 (t = 4.49)\n",
      "Rank 405: 'max' feature #1756 (t = 4.49)\n",
      "Rank 406: 'max' feature #4811 (t = 4.49)\n",
      "Rank 407: 'max' feature #2838 (t = 4.49)\n",
      "Rank 408: 'mean' feature #1345 (t = 4.49)\n",
      "Rank 409: 'max' feature #2263 (t = 4.49)\n",
      "Rank 410: 'mean' feature #917 (t = 4.47)\n",
      "Rank 411: 'max' feature #7892 (t = 4.47)\n",
      "Rank 412: 'max' feature #3854 (t = 4.47)\n",
      "Rank 413: 'mean' feature #3366 (t = 4.47)\n",
      "Rank 414: 'mean' feature #1904 (t = 4.47)\n",
      "Rank 415: 'mean' feature #1443 (t = 4.47)\n",
      "Rank 416: 'max' feature #3939 (t = 4.47)\n",
      "Rank 417: 'max' feature #5281 (t = 4.47)\n",
      "Rank 418: 'mean' feature #8063 (t = 4.46)\n",
      "Rank 419: 'max' feature #7623 (t = 4.46)\n",
      "Rank 420: 'mean' feature #294 (t = 4.46)\n",
      "Rank 421: 'max' feature #898 (t = 4.46)\n",
      "Rank 422: 'max' feature #3823 (t = 4.45)\n",
      "Rank 423: 'max' feature #5148 (t = 4.45)\n",
      "Rank 424: 'mean' feature #2819 (t = 4.45)\n",
      "Rank 425: 'mean' feature #7274 (t = 4.44)\n",
      "Rank 426: 'max' feature #953 (t = 4.44)\n",
      "Rank 427: 'max' feature #3829 (t = 4.44)\n",
      "Rank 428: 'mean' feature #1704 (t = 4.43)\n",
      "Rank 429: 'max' feature #751 (t = 4.43)\n",
      "Rank 430: 'max' feature #5137 (t = 4.43)\n",
      "Rank 431: 'mean' feature #3982 (t = 4.43)\n",
      "Rank 432: 'max' feature #1013 (t = 4.43)\n",
      "Rank 433: 'mean' feature #1016 (t = 4.42)\n",
      "Rank 434: 'max' feature #6471 (t = 4.42)\n",
      "Rank 435: 'max' feature #5685 (t = 4.42)\n",
      "Rank 436: 'mean' feature #1664 (t = 4.42)\n",
      "Rank 437: 'mean' feature #249 (t = 4.41)\n",
      "Rank 438: 'mean' feature #698 (t = 4.41)\n",
      "Rank 439: 'max' feature #7575 (t = 4.41)\n",
      "Rank 440: 'max' feature #5592 (t = 4.40)\n",
      "Rank 441: 'mean' feature #4550 (t = 4.40)\n",
      "Rank 442: 'mean' feature #5241 (t = 4.40)\n",
      "Rank 443: 'mean' feature #3045 (t = 4.40)\n",
      "Rank 444: 'max' feature #4307 (t = 4.40)\n",
      "Rank 445: 'max' feature #5795 (t = 4.40)\n",
      "Rank 446: 'mean' feature #2628 (t = 4.40)\n",
      "Rank 447: 'mean' feature #1420 (t = 4.39)\n",
      "Rank 448: 'max' feature #2251 (t = 4.39)\n",
      "Rank 449: 'mean' feature #3604 (t = 4.39)\n",
      "Rank 450: 'mean' feature #2428 (t = 4.39)\n",
      "Rank 451: 'mean' feature #6802 (t = 4.38)\n",
      "Rank 452: 'mean' feature #353 (t = 4.38)\n",
      "Rank 453: 'max' feature #5153 (t = 4.38)\n",
      "Rank 454: 'mean' feature #7899 (t = 4.38)\n",
      "Rank 455: 'max' feature #5278 (t = 4.38)\n",
      "Rank 456: 'max' feature #3222 (t = 4.37)\n",
      "Rank 457: 'mean' feature #4668 (t = 4.37)\n",
      "Rank 458: 'mean' feature #7804 (t = 4.37)\n",
      "Rank 459: 'mean' feature #2958 (t = 4.37)\n",
      "Rank 460: 'max' feature #860 (t = 4.37)\n",
      "Rank 461: 'max' feature #6679 (t = 4.36)\n",
      "Rank 462: 'mean' feature #2445 (t = 4.36)\n",
      "Rank 463: 'mean' feature #4688 (t = 4.36)\n",
      "Rank 464: 'mean' feature #416 (t = 4.35)\n",
      "Rank 465: 'max' feature #7820 (t = 4.35)\n",
      "Rank 466: 'mean' feature #7097 (t = 4.35)\n",
      "Rank 467: 'mean' feature #3155 (t = 4.35)\n",
      "Rank 468: 'mean' feature #3101 (t = 4.35)\n",
      "Rank 469: 'max' feature #5465 (t = 4.34)\n",
      "Rank 470: 'mean' feature #6325 (t = 4.34)\n",
      "Rank 471: 'mean' feature #3036 (t = 4.34)\n",
      "Rank 472: 'max' feature #4605 (t = 4.34)\n",
      "Rank 473: 'mean' feature #7861 (t = 4.34)\n",
      "Rank 474: 'mean' feature #6030 (t = 4.32)\n",
      "Rank 475: 'max' feature #1752 (t = 4.31)\n",
      "Rank 476: 'mean' feature #473 (t = 4.30)\n",
      "Rank 477: 'mean' feature #5489 (t = 4.30)\n",
      "Rank 478: 'max' feature #685 (t = 4.30)\n",
      "Rank 479: 'mean' feature #7105 (t = 4.30)\n",
      "Rank 480: 'max' feature #5449 (t = 4.30)\n",
      "Rank 481: 'max' feature #5632 (t = 4.30)\n",
      "Rank 482: 'mean' feature #1995 (t = 4.30)\n",
      "Rank 483: 'mean' feature #7701 (t = 4.30)\n",
      "Rank 484: 'mean' feature #2789 (t = 4.30)\n",
      "Rank 485: 'max' feature #4670 (t = 4.29)\n",
      "Rank 486: 'max' feature #2517 (t = 4.28)\n",
      "Rank 487: 'max' feature #2701 (t = 4.28)\n",
      "Rank 488: 'mean' feature #1407 (t = 4.27)\n",
      "Rank 489: 'mean' feature #8154 (t = 4.27)\n",
      "Rank 490: 'max' feature #6783 (t = 4.27)\n",
      "Rank 491: 'max' feature #6556 (t = 4.27)\n",
      "Rank 492: 'mean' feature #4654 (t = 4.27)\n",
      "Rank 493: 'mean' feature #216 (t = 4.27)\n",
      "Rank 494: 'mean' feature #512 (t = 4.26)\n",
      "Rank 495: 'max' feature #2111 (t = 4.26)\n",
      "Rank 496: 'max' feature #4432 (t = 4.26)\n",
      "Rank 497: 'mean' feature #5629 (t = 4.25)\n",
      "Rank 498: 'mean' feature #88 (t = 4.25)\n",
      "Rank 499: 'mean' feature #2274 (t = 4.25)\n",
      "Rank 500: 'max' feature #3105 (t = 4.25)\n",
      "Rank 501: 'max' feature #5506 (t = 4.24)\n",
      "Rank 502: 'mean' feature #1023 (t = 4.23)\n",
      "Rank 503: 'mean' feature #5273 (t = 4.23)\n",
      "Rank 504: 'max' feature #3205 (t = 4.23)\n",
      "Rank 505: 'mean' feature #482 (t = 4.23)\n",
      "Rank 506: 'max' feature #625 (t = 4.23)\n",
      "Rank 507: 'max' feature #3132 (t = 4.22)\n",
      "Rank 508: 'mean' feature #2833 (t = 4.22)\n",
      "Rank 509: 'max' feature #7465 (t = 4.22)\n",
      "Rank 510: 'mean' feature #1256 (t = 4.22)\n",
      "Rank 511: 'max' feature #2700 (t = 4.22)\n",
      "Rank 512: 'mean' feature #1281 (t = 4.22)\n",
      "Rank 513: 'max' feature #5574 (t = 4.21)\n",
      "Rank 514: 'max' feature #4607 (t = 4.21)\n",
      "Rank 515: 'mean' feature #4195 (t = 4.21)\n",
      "Rank 516: 'mean' feature #3289 (t = 4.21)\n",
      "Rank 517: 'mean' feature #2527 (t = 4.21)\n",
      "Rank 518: 'mean' feature #1412 (t = 4.21)\n",
      "Rank 519: 'mean' feature #5375 (t = 4.20)\n",
      "Rank 520: 'mean' feature #6215 (t = 4.20)\n",
      "Rank 521: 'max' feature #2229 (t = 4.19)\n",
      "Rank 522: 'max' feature #261 (t = 4.18)\n",
      "Rank 523: 'max' feature #3087 (t = 4.18)\n",
      "Rank 524: 'mean' feature #938 (t = 4.18)\n",
      "Rank 525: 'mean' feature #4371 (t = 4.18)\n",
      "Rank 526: 'max' feature #7924 (t = 4.18)\n",
      "Rank 527: 'mean' feature #3124 (t = 4.18)\n",
      "Rank 528: 'mean' feature #2340 (t = 4.17)\n",
      "Rank 529: 'mean' feature #4964 (t = 4.17)\n",
      "Rank 530: 'max' feature #441 (t = 4.17)\n",
      "Rank 531: 'mean' feature #5245 (t = 4.17)\n",
      "Rank 532: 'max' feature #2487 (t = 4.16)\n",
      "Rank 533: 'max' feature #5756 (t = 4.16)\n",
      "Rank 534: 'max' feature #3077 (t = 4.16)\n",
      "Rank 535: 'mean' feature #3578 (t = 4.15)\n",
      "Rank 536: 'max' feature #2580 (t = 4.15)\n",
      "Rank 537: 'mean' feature #67 (t = 4.15)\n",
      "Rank 538: 'mean' feature #4023 (t = 4.15)\n",
      "Rank 539: 'max' feature #6716 (t = 4.14)\n",
      "Rank 540: 'max' feature #1633 (t = 4.14)\n",
      "Rank 541: 'max' feature #4490 (t = 4.14)\n",
      "Rank 542: 'mean' feature #7993 (t = 4.13)\n",
      "Rank 543: 'max' feature #3848 (t = 4.13)\n",
      "Rank 544: 'mean' feature #5266 (t = 4.13)\n",
      "Rank 545: 'max' feature #5907 (t = 4.12)\n",
      "Rank 546: 'max' feature #4914 (t = 4.12)\n",
      "Rank 547: 'max' feature #3020 (t = 4.12)\n",
      "Rank 548: 'mean' feature #3078 (t = 4.12)\n",
      "Rank 549: 'max' feature #4658 (t = 4.12)\n",
      "Rank 550: 'mean' feature #7550 (t = 4.12)\n",
      "Rank 551: 'mean' feature #2007 (t = 4.12)\n",
      "Rank 552: 'max' feature #947 (t = 4.12)\n",
      "Rank 553: 'max' feature #658 (t = 4.11)\n",
      "Rank 554: 'mean' feature #662 (t = 4.11)\n",
      "Rank 555: 'max' feature #5302 (t = 4.11)\n",
      "Rank 556: 'max' feature #4281 (t = 4.11)\n",
      "Rank 557: 'max' feature #4117 (t = 4.11)\n",
      "Rank 558: 'mean' feature #3179 (t = 4.10)\n",
      "Rank 559: 'max' feature #4926 (t = 4.10)\n",
      "Rank 560: 'mean' feature #107 (t = 4.10)\n",
      "Rank 561: 'max' feature #1898 (t = 4.09)\n",
      "Rank 562: 'mean' feature #2889 (t = 4.09)\n",
      "Rank 563: 'mean' feature #3328 (t = 4.09)\n",
      "Rank 564: 'mean' feature #1415 (t = 4.09)\n",
      "Rank 565: 'mean' feature #4016 (t = 4.09)\n",
      "Rank 566: 'mean' feature #2510 (t = 4.09)\n",
      "Rank 567: 'mean' feature #399 (t = 4.09)\n",
      "Rank 568: 'mean' feature #4595 (t = 4.08)\n",
      "Rank 569: 'mean' feature #2702 (t = 4.08)\n",
      "Rank 570: 'max' feature #4552 (t = 4.08)\n",
      "Rank 571: 'max' feature #3395 (t = 4.08)\n",
      "Rank 572: 'mean' feature #8025 (t = 4.08)\n",
      "Rank 573: 'max' feature #3471 (t = 4.08)\n",
      "Rank 574: 'max' feature #2453 (t = 4.08)\n",
      "Rank 575: 'max' feature #6061 (t = 4.07)\n",
      "Rank 576: 'max' feature #6238 (t = 4.07)\n",
      "Rank 577: 'mean' feature #2733 (t = 4.07)\n",
      "Rank 578: 'max' feature #2014 (t = 4.07)\n",
      "Rank 579: 'mean' feature #4546 (t = 4.07)\n",
      "Rank 580: 'max' feature #4989 (t = 4.07)\n",
      "Rank 581: 'max' feature #902 (t = 4.07)\n",
      "Rank 582: 'mean' feature #5636 (t = 4.07)\n",
      "Rank 583: 'max' feature #6905 (t = 4.06)\n",
      "Rank 584: 'max' feature #294 (t = 4.06)\n",
      "Rank 585: 'max' feature #2270 (t = 4.06)\n",
      "Rank 586: 'max' feature #1350 (t = 4.06)\n",
      "Rank 587: 'mean' feature #5697 (t = 4.06)\n",
      "Rank 588: 'mean' feature #3524 (t = 4.05)\n",
      "Rank 589: 'mean' feature #3102 (t = 4.05)\n",
      "Rank 590: 'mean' feature #1796 (t = 4.05)\n",
      "Rank 591: 'max' feature #5755 (t = 4.05)\n",
      "Rank 592: 'max' feature #6314 (t = 4.05)\n",
      "Rank 593: 'max' feature #4985 (t = 4.05)\n",
      "Rank 594: 'mean' feature #8113 (t = 4.05)\n",
      "Rank 595: 'max' feature #2290 (t = 4.05)\n",
      "Rank 596: 'mean' feature #1356 (t = 4.04)\n",
      "Rank 597: 'max' feature #258 (t = 4.04)\n",
      "Rank 598: 'max' feature #5850 (t = 4.04)\n",
      "Rank 599: 'max' feature #3641 (t = 4.04)\n",
      "Rank 600: 'mean' feature #4029 (t = 4.03)\n",
      "Rank 601: 'max' feature #6301 (t = 4.03)\n",
      "Rank 602: 'max' feature #7704 (t = 4.03)\n",
      "Rank 603: 'max' feature #8087 (t = 4.03)\n",
      "Rank 604: 'mean' feature #2593 (t = 4.03)\n",
      "Rank 605: 'max' feature #5243 (t = 4.03)\n",
      "Rank 606: 'mean' feature #4766 (t = 4.03)\n",
      "Rank 607: 'mean' feature #1852 (t = 4.03)\n",
      "Rank 608: 'mean' feature #3338 (t = 4.03)\n",
      "Rank 609: 'max' feature #1644 (t = 4.03)\n",
      "Rank 610: 'max' feature #7283 (t = 4.02)\n",
      "Rank 611: 'max' feature #7819 (t = 4.02)\n",
      "Rank 612: 'mean' feature #3456 (t = 4.02)\n",
      "Rank 613: 'mean' feature #7418 (t = 4.02)\n",
      "Rank 614: 'mean' feature #763 (t = 4.02)\n",
      "Rank 615: 'max' feature #1322 (t = 4.02)\n",
      "Rank 616: 'mean' feature #7606 (t = 4.02)\n",
      "Rank 617: 'mean' feature #6087 (t = 4.01)\n",
      "Rank 618: 'mean' feature #3789 (t = 4.01)\n",
      "Rank 619: 'mean' feature #2751 (t = 4.01)\n",
      "Rank 620: 'max' feature #6892 (t = 4.01)\n",
      "Rank 621: 'max' feature #1081 (t = 4.01)\n",
      "Rank 622: 'max' feature #7452 (t = 4.01)\n",
      "Rank 623: 'max' feature #7786 (t = 4.01)\n",
      "Rank 624: 'max' feature #4912 (t = 4.00)\n",
      "Rank 625: 'max' feature #8037 (t = 4.00)\n",
      "Rank 626: 'mean' feature #676 (t = 4.00)\n",
      "Rank 627: 'mean' feature #1418 (t = 4.00)\n",
      "Rank 628: 'mean' feature #4571 (t = 4.00)\n",
      "Rank 629: 'max' feature #7420 (t = 4.00)\n",
      "Rank 630: 'max' feature #361 (t = 4.00)\n",
      "Rank 631: 'max' feature #4674 (t = 3.99)\n",
      "Rank 632: 'mean' feature #6842 (t = 3.99)\n",
      "Rank 633: 'max' feature #6403 (t = 3.99)\n",
      "Rank 634: 'max' feature #5024 (t = 3.99)\n",
      "Rank 635: 'max' feature #2597 (t = 3.99)\n",
      "Rank 636: 'mean' feature #6785 (t = 3.99)\n",
      "Rank 637: 'max' feature #395 (t = 3.99)\n",
      "Rank 638: 'mean' feature #6773 (t = 3.99)\n",
      "Rank 639: 'max' feature #1419 (t = 3.99)\n",
      "Rank 640: 'max' feature #4396 (t = 3.99)\n",
      "Rank 641: 'max' feature #4222 (t = 3.99)\n",
      "Rank 642: 'mean' feature #3234 (t = 3.98)\n",
      "Rank 643: 'mean' feature #3970 (t = 3.98)\n",
      "Rank 644: 'mean' feature #5092 (t = 3.98)\n",
      "Rank 645: 'mean' feature #5852 (t = 3.98)\n",
      "Rank 646: 'max' feature #819 (t = 3.98)\n",
      "Rank 647: 'mean' feature #2357 (t = 3.98)\n",
      "Rank 648: 'max' feature #427 (t = 3.98)\n",
      "Rank 649: 'max' feature #6430 (t = 3.98)\n",
      "Rank 650: 'max' feature #4872 (t = 3.98)\n",
      "Rank 651: 'max' feature #5765 (t = 3.97)\n",
      "Rank 652: 'mean' feature #5006 (t = 3.97)\n",
      "Rank 653: 'max' feature #2787 (t = 3.97)\n",
      "Rank 654: 'mean' feature #7997 (t = 3.97)\n",
      "Rank 655: 'mean' feature #3771 (t = 3.97)\n",
      "Rank 656: 'max' feature #2346 (t = 3.97)\n",
      "Rank 657: 'mean' feature #1380 (t = 3.97)\n",
      "Rank 658: 'max' feature #2567 (t = 3.97)\n",
      "Rank 659: 'mean' feature #2612 (t = 3.96)\n",
      "Rank 660: 'mean' feature #7908 (t = 3.96)\n",
      "Rank 661: 'mean' feature #4329 (t = 3.96)\n",
      "Rank 662: 'max' feature #7735 (t = 3.96)\n",
      "Rank 663: 'mean' feature #2290 (t = 3.96)\n",
      "Rank 664: 'max' feature #1076 (t = 3.96)\n",
      "Rank 665: 'max' feature #6348 (t = 3.95)\n",
      "Rank 666: 'max' feature #3410 (t = 3.95)\n",
      "Rank 667: 'mean' feature #898 (t = 3.95)\n",
      "Rank 668: 'max' feature #6283 (t = 3.95)\n",
      "Rank 669: 'mean' feature #2616 (t = 3.95)\n",
      "Rank 670: 'mean' feature #2059 (t = 3.94)\n",
      "Rank 671: 'mean' feature #1679 (t = 3.94)\n",
      "Rank 672: 'mean' feature #5761 (t = 3.94)\n",
      "Rank 673: 'max' feature #1150 (t = 3.94)\n",
      "Rank 674: 'max' feature #7759 (t = 3.94)\n",
      "Rank 675: 'mean' feature #5414 (t = 3.93)\n",
      "Rank 676: 'mean' feature #4510 (t = 3.93)\n",
      "Rank 677: 'mean' feature #3533 (t = 3.93)\n",
      "Rank 678: 'max' feature #5704 (t = 3.93)\n",
      "Rank 679: 'max' feature #7534 (t = 3.93)\n",
      "Rank 680: 'max' feature #7421 (t = 3.93)\n",
      "Rank 681: 'mean' feature #1482 (t = 3.93)\n",
      "Rank 682: 'max' feature #1819 (t = 3.92)\n",
      "Rank 683: 'mean' feature #1459 (t = 3.92)\n",
      "Rank 684: 'max' feature #1722 (t = 3.92)\n",
      "Rank 685: 'mean' feature #6156 (t = 3.92)\n",
      "Rank 686: 'max' feature #4772 (t = 3.92)\n",
      "Rank 687: 'mean' feature #2946 (t = 3.92)\n",
      "Rank 688: 'max' feature #4824 (t = 3.92)\n",
      "Rank 689: 'mean' feature #7327 (t = 3.91)\n",
      "Rank 690: 'mean' feature #7046 (t = 3.91)\n",
      "Rank 691: 'max' feature #3256 (t = 3.91)\n",
      "Rank 692: 'mean' feature #1885 (t = 3.91)\n",
      "Rank 693: 'mean' feature #5033 (t = 3.91)\n",
      "Rank 694: 'max' feature #3583 (t = 3.90)\n",
      "Rank 695: 'mean' feature #2021 (t = 3.90)\n",
      "Rank 696: 'max' feature #771 (t = 3.90)\n",
      "Rank 697: 'max' feature #713 (t = 3.90)\n",
      "Rank 698: 'max' feature #936 (t = 3.90)\n",
      "Rank 699: 'mean' feature #1594 (t = 3.90)\n",
      "Rank 700: 'mean' feature #2148 (t = 3.90)\n",
      "Rank 701: 'mean' feature #5293 (t = 3.90)\n",
      "Rank 702: 'mean' feature #32 (t = 3.89)\n",
      "Rank 703: 'max' feature #5126 (t = 3.89)\n",
      "Rank 704: 'mean' feature #3763 (t = 3.89)\n",
      "Rank 705: 'max' feature #6346 (t = 3.89)\n",
      "Rank 706: 'mean' feature #1911 (t = 3.89)\n",
      "Rank 707: 'mean' feature #5315 (t = 3.89)\n",
      "Rank 708: 'max' feature #2180 (t = 3.88)\n",
      "Rank 709: 'mean' feature #7565 (t = 3.88)\n",
      "Rank 710: 'max' feature #5362 (t = 3.88)\n",
      "Rank 711: 'mean' feature #2322 (t = 3.88)\n",
      "Rank 712: 'max' feature #4225 (t = 3.88)\n",
      "Rank 713: 'max' feature #3457 (t = 3.88)\n",
      "Rank 714: 'mean' feature #3193 (t = 3.88)\n",
      "Rank 715: 'mean' feature #3993 (t = 3.88)\n",
      "Rank 716: 'mean' feature #6290 (t = 3.88)\n",
      "Rank 717: 'max' feature #3996 (t = 3.88)\n",
      "Rank 718: 'max' feature #1415 (t = 3.87)\n",
      "Rank 719: 'mean' feature #6916 (t = 3.87)\n",
      "Rank 720: 'mean' feature #1432 (t = 3.87)\n",
      "Rank 721: 'max' feature #3911 (t = 3.87)\n",
      "Rank 722: 'mean' feature #6058 (t = 3.87)\n",
      "Rank 723: 'mean' feature #7679 (t = 3.87)\n",
      "Rank 724: 'mean' feature #4483 (t = 3.87)\n",
      "Rank 725: 'mean' feature #1180 (t = 3.87)\n",
      "Rank 726: 'mean' feature #3625 (t = 3.87)\n",
      "Rank 727: 'max' feature #1704 (t = 3.87)\n",
      "Rank 728: 'max' feature #3233 (t = 3.87)\n",
      "Rank 729: 'max' feature #6562 (t = 3.87)\n",
      "Rank 730: 'max' feature #406 (t = 3.87)\n",
      "Rank 731: 'max' feature #1678 (t = 3.86)\n",
      "Rank 732: 'max' feature #2451 (t = 3.86)\n",
      "Rank 733: 'max' feature #2741 (t = 3.86)\n",
      "Rank 734: 'mean' feature #4517 (t = 3.86)\n",
      "Rank 735: 'mean' feature #3390 (t = 3.86)\n",
      "Rank 736: 'mean' feature #4412 (t = 3.86)\n",
      "Rank 737: 'mean' feature #6837 (t = 3.86)\n",
      "Rank 738: 'mean' feature #6088 (t = 3.86)\n",
      "Rank 739: 'max' feature #7596 (t = 3.85)\n",
      "Rank 740: 'max' feature #3003 (t = 3.85)\n",
      "Rank 741: 'max' feature #1605 (t = 3.85)\n",
      "Rank 742: 'max' feature #4122 (t = 3.85)\n",
      "Rank 743: 'mean' feature #7400 (t = 3.85)\n",
      "Rank 744: 'max' feature #5930 (t = 3.85)\n",
      "Rank 745: 'max' feature #2109 (t = 3.85)\n",
      "Rank 746: 'max' feature #7243 (t = 3.85)\n",
      "Rank 747: 'max' feature #4504 (t = 3.85)\n",
      "Rank 748: 'mean' feature #1159 (t = 3.85)\n",
      "Rank 749: 'max' feature #2527 (t = 3.85)\n",
      "Rank 750: 'max' feature #8012 (t = 3.85)\n",
      "Rank 751: 'mean' feature #151 (t = 3.84)\n",
      "Rank 752: 'mean' feature #3218 (t = 3.84)\n",
      "Rank 753: 'max' feature #7588 (t = 3.84)\n",
      "Rank 754: 'mean' feature #5284 (t = 3.84)\n",
      "Rank 755: 'mean' feature #4722 (t = 3.84)\n",
      "Rank 756: 'max' feature #7282 (t = 3.84)\n",
      "Rank 757: 'max' feature #5697 (t = 3.84)\n",
      "Rank 758: 'max' feature #2692 (t = 3.84)\n",
      "Rank 759: 'mean' feature #1899 (t = 3.84)\n",
      "Rank 760: 'max' feature #8148 (t = 3.84)\n",
      "Rank 761: 'mean' feature #5355 (t = 3.84)\n",
      "Rank 762: 'mean' feature #5647 (t = 3.84)\n",
      "Rank 763: 'max' feature #909 (t = 3.84)\n",
      "Rank 764: 'mean' feature #4979 (t = 3.84)\n",
      "Rank 765: 'mean' feature #1143 (t = 3.83)\n",
      "Rank 766: 'mean' feature #7735 (t = 3.83)\n",
      "Rank 767: 'max' feature #5535 (t = 3.83)\n",
      "Rank 768: 'mean' feature #4220 (t = 3.83)\n",
      "Rank 769: 'mean' feature #6061 (t = 3.83)\n",
      "Rank 770: 'max' feature #3246 (t = 3.83)\n",
      "Rank 771: 'max' feature #345 (t = 3.83)\n",
      "Rank 772: 'max' feature #3957 (t = 3.83)\n",
      "Rank 773: 'mean' feature #5290 (t = 3.83)\n",
      "Rank 774: 'max' feature #224 (t = 3.83)\n",
      "Rank 775: 'max' feature #1126 (t = 3.83)\n",
      "Rank 776: 'mean' feature #6697 (t = 3.83)\n",
      "Rank 777: 'max' feature #5887 (t = 3.83)\n",
      "Rank 778: 'max' feature #248 (t = 3.82)\n",
      "Rank 779: 'max' feature #7551 (t = 3.82)\n",
      "Rank 780: 'mean' feature #3724 (t = 3.82)\n",
      "Rank 781: 'max' feature #7702 (t = 3.82)\n",
      "Rank 782: 'mean' feature #5046 (t = 3.82)\n",
      "Rank 783: 'mean' feature #5778 (t = 3.82)\n",
      "Rank 784: 'max' feature #6326 (t = 3.82)\n",
      "Rank 785: 'mean' feature #1941 (t = 3.82)\n",
      "Rank 786: 'mean' feature #258 (t = 3.82)\n",
      "Rank 787: 'max' feature #1075 (t = 3.82)\n",
      "Rank 788: 'mean' feature #4271 (t = 3.81)\n",
      "Rank 789: 'mean' feature #6690 (t = 3.81)\n",
      "Rank 790: 'max' feature #2305 (t = 3.81)\n",
      "Rank 791: 'max' feature #3005 (t = 3.81)\n",
      "Rank 792: 'mean' feature #323 (t = 3.81)\n",
      "Rank 793: 'mean' feature #3110 (t = 3.81)\n",
      "Rank 794: 'mean' feature #3937 (t = 3.81)\n",
      "Rank 795: 'max' feature #6824 (t = 3.81)\n",
      "Rank 796: 'max' feature #2365 (t = 3.81)\n",
      "Rank 797: 'mean' feature #2152 (t = 3.81)\n",
      "Rank 798: 'max' feature #2560 (t = 3.80)\n",
      "Rank 799: 'mean' feature #5171 (t = 3.80)\n",
      "Rank 800: 'mean' feature #1595 (t = 3.80)\n",
      "Rank 801: 'max' feature #5832 (t = 3.80)\n",
      "Rank 802: 'max' feature #4302 (t = 3.80)\n",
      "Rank 803: 'mean' feature #935 (t = 3.80)\n",
      "Rank 804: 'max' feature #108 (t = 3.80)\n",
      "Rank 805: 'mean' feature #931 (t = 3.80)\n",
      "Rank 806: 'mean' feature #6355 (t = 3.80)\n",
      "Rank 807: 'mean' feature #3033 (t = 3.80)\n",
      "Rank 808: 'mean' feature #5120 (t = 3.80)\n",
      "Rank 809: 'mean' feature #4033 (t = 3.80)\n",
      "Rank 810: 'mean' feature #6832 (t = 3.80)\n",
      "Rank 811: 'mean' feature #3718 (t = 3.79)\n",
      "Rank 812: 'max' feature #7656 (t = 3.79)\n",
      "Rank 813: 'max' feature #374 (t = 3.79)\n",
      "Rank 814: 'max' feature #6102 (t = 3.79)\n",
      "Rank 815: 'max' feature #6100 (t = 3.79)\n",
      "Rank 816: 'max' feature #5417 (t = 3.79)\n",
      "Rank 817: 'max' feature #5485 (t = 3.79)\n",
      "Rank 818: 'max' feature #7010 (t = 3.79)\n",
      "Rank 819: 'mean' feature #4392 (t = 3.79)\n",
      "Rank 820: 'mean' feature #1080 (t = 3.79)\n",
      "Rank 821: 'max' feature #2672 (t = 3.79)\n",
      "Rank 822: 'mean' feature #4859 (t = 3.78)\n",
      "Rank 823: 'mean' feature #5191 (t = 3.78)\n",
      "Rank 824: 'mean' feature #5496 (t = 3.78)\n",
      "Rank 825: 'mean' feature #3590 (t = 3.78)\n",
      "Rank 826: 'max' feature #8061 (t = 3.78)\n",
      "Rank 827: 'mean' feature #4689 (t = 3.78)\n",
      "Rank 828: 'mean' feature #7104 (t = 3.78)\n",
      "Rank 829: 'max' feature #4397 (t = 3.77)\n",
      "Rank 830: 'max' feature #2902 (t = 3.77)\n",
      "Rank 831: 'mean' feature #5982 (t = 3.77)\n",
      "Rank 832: 'max' feature #7886 (t = 3.77)\n",
      "Rank 833: 'max' feature #5822 (t = 3.77)\n",
      "Rank 834: 'mean' feature #4716 (t = 3.77)\n",
      "Rank 835: 'max' feature #3821 (t = 3.77)\n",
      "Rank 836: 'mean' feature #3971 (t = 3.77)\n",
      "Rank 837: 'mean' feature #7764 (t = 3.77)\n",
      "Rank 838: 'mean' feature #4205 (t = 3.77)\n",
      "Rank 839: 'max' feature #4178 (t = 3.77)\n",
      "Rank 840: 'max' feature #7042 (t = 3.76)\n",
      "Rank 841: 'mean' feature #885 (t = 3.76)\n",
      "Rank 842: 'max' feature #1737 (t = 3.76)\n",
      "Rank 843: 'mean' feature #6182 (t = 3.76)\n",
      "Rank 844: 'mean' feature #1520 (t = 3.76)\n",
      "Rank 845: 'mean' feature #3697 (t = 3.76)\n",
      "Rank 846: 'mean' feature #212 (t = 3.76)\n",
      "Rank 847: 'mean' feature #1946 (t = 3.76)\n",
      "Rank 848: 'mean' feature #3170 (t = 3.75)\n",
      "Rank 849: 'mean' feature #1509 (t = 3.75)\n",
      "Rank 850: 'max' feature #113 (t = 3.75)\n",
      "Rank 851: 'max' feature #3440 (t = 3.75)\n",
      "Rank 852: 'max' feature #5125 (t = 3.75)\n",
      "Rank 853: 'mean' feature #4113 (t = 3.75)\n",
      "Rank 854: 'mean' feature #6270 (t = 3.75)\n",
      "Rank 855: 'max' feature #1592 (t = 3.75)\n",
      "Rank 856: 'mean' feature #8124 (t = 3.75)\n",
      "Rank 857: 'mean' feature #6504 (t = 3.75)\n",
      "Rank 858: 'max' feature #7075 (t = 3.75)\n",
      "Rank 859: 'mean' feature #672 (t = 3.75)\n",
      "Rank 860: 'mean' feature #6933 (t = 3.75)\n",
      "Rank 861: 'mean' feature #5024 (t = 3.75)\n",
      "Rank 862: 'max' feature #4746 (t = 3.75)\n",
      "Rank 863: 'mean' feature #7215 (t = 3.74)\n",
      "Rank 864: 'max' feature #7467 (t = 3.74)\n",
      "Rank 865: 'max' feature #2479 (t = 3.74)\n",
      "Rank 866: 'max' feature #6977 (t = 3.74)\n",
      "Rank 867: 'mean' feature #7171 (t = 3.74)\n",
      "Rank 868: 'max' feature #1999 (t = 3.74)\n",
      "Rank 869: 'mean' feature #5403 (t = 3.73)\n",
      "Rank 870: 'mean' feature #4317 (t = 3.73)\n",
      "Rank 871: 'max' feature #4170 (t = 3.73)\n",
      "Rank 872: 'max' feature #6631 (t = 3.73)\n",
      "Rank 873: 'mean' feature #3116 (t = 3.73)\n",
      "Rank 874: 'max' feature #8182 (t = 3.73)\n",
      "Rank 875: 'max' feature #3828 (t = 3.73)\n",
      "Rank 876: 'mean' feature #7345 (t = 3.73)\n",
      "Rank 877: 'max' feature #2246 (t = 3.73)\n",
      "Rank 878: 'mean' feature #3613 (t = 3.73)\n",
      "Rank 879: 'max' feature #1775 (t = 3.73)\n",
      "Rank 880: 'mean' feature #4214 (t = 3.72)\n",
      "Rank 881: 'max' feature #5995 (t = 3.72)\n",
      "Rank 882: 'mean' feature #5725 (t = 3.72)\n",
      "Rank 883: 'max' feature #971 (t = 3.72)\n",
      "Rank 884: 'max' feature #5410 (t = 3.72)\n",
      "Rank 885: 'max' feature #6992 (t = 3.72)\n",
      "Rank 886: 'max' feature #1401 (t = 3.71)\n",
      "Rank 887: 'mean' feature #6742 (t = 3.71)\n",
      "Rank 888: 'max' feature #511 (t = 3.71)\n",
      "Rank 889: 'max' feature #6899 (t = 3.71)\n",
      "Rank 890: 'mean' feature #5585 (t = 3.71)\n",
      "Rank 891: 'max' feature #7038 (t = 3.71)\n",
      "Rank 892: 'max' feature #7389 (t = 3.71)\n",
      "Rank 893: 'max' feature #1980 (t = 3.71)\n",
      "Rank 894: 'max' feature #2095 (t = 3.71)\n",
      "Rank 895: 'max' feature #2957 (t = 3.71)\n",
      "Rank 896: 'max' feature #2195 (t = 3.71)\n",
      "Rank 897: 'max' feature #5638 (t = 3.70)\n",
      "Rank 898: 'mean' feature #5013 (t = 3.70)\n",
      "Rank 899: 'mean' feature #7061 (t = 3.70)\n",
      "Rank 900: 'max' feature #7829 (t = 3.70)\n",
      "Rank 901: 'max' feature #5106 (t = 3.70)\n",
      "Rank 902: 'mean' feature #8155 (t = 3.70)\n",
      "Rank 903: 'max' feature #6038 (t = 3.70)\n",
      "Rank 904: 'mean' feature #2804 (t = 3.70)\n",
      "Rank 905: 'mean' feature #1199 (t = 3.70)\n",
      "Rank 906: 'mean' feature #6974 (t = 3.70)\n",
      "Rank 907: 'max' feature #3009 (t = 3.70)\n",
      "Rank 908: 'mean' feature #4147 (t = 3.70)\n",
      "Rank 909: 'max' feature #6666 (t = 3.70)\n",
      "Rank 910: 'max' feature #5088 (t = 3.70)\n",
      "Rank 911: 'mean' feature #4538 (t = 3.69)\n",
      "Rank 912: 'mean' feature #5553 (t = 3.69)\n",
      "Rank 913: 'max' feature #250 (t = 3.69)\n",
      "Rank 914: 'mean' feature #671 (t = 3.69)\n",
      "Rank 915: 'mean' feature #1405 (t = 3.68)\n",
      "Rank 916: 'max' feature #8120 (t = 3.68)\n",
      "Rank 917: 'mean' feature #62 (t = 3.68)\n",
      "Rank 918: 'max' feature #3344 (t = 3.68)\n",
      "Rank 919: 'max' feature #3143 (t = 3.68)\n",
      "Rank 920: 'mean' feature #1170 (t = 3.68)\n",
      "Rank 921: 'mean' feature #86 (t = 3.68)\n",
      "Rank 922: 'max' feature #1875 (t = 3.68)\n",
      "Rank 923: 'mean' feature #971 (t = 3.67)\n",
      "Rank 924: 'max' feature #3536 (t = 3.67)\n",
      "Rank 925: 'max' feature #7204 (t = 3.67)\n",
      "Rank 926: 'max' feature #6559 (t = 3.67)\n",
      "Rank 927: 'max' feature #155 (t = 3.67)\n",
      "Rank 928: 'max' feature #122 (t = 3.67)\n",
      "Rank 929: 'max' feature #1134 (t = 3.67)\n",
      "Rank 930: 'mean' feature #6384 (t = 3.67)\n",
      "Rank 931: 'max' feature #4810 (t = 3.67)\n",
      "Rank 932: 'max' feature #1371 (t = 3.67)\n",
      "Rank 933: 'max' feature #4106 (t = 3.67)\n",
      "Rank 934: 'max' feature #2162 (t = 3.67)\n",
      "Rank 935: 'mean' feature #2348 (t = 3.67)\n",
      "Rank 936: 'max' feature #14 (t = 3.66)\n",
      "Rank 937: 'mean' feature #4146 (t = 3.66)\n",
      "Rank 938: 'mean' feature #7377 (t = 3.66)\n",
      "Rank 939: 'mean' feature #3254 (t = 3.66)\n",
      "Rank 940: 'max' feature #4315 (t = 3.66)\n",
      "Rank 941: 'mean' feature #5996 (t = 3.66)\n",
      "Rank 942: 'max' feature #1440 (t = 3.66)\n",
      "Rank 943: 'mean' feature #7578 (t = 3.66)\n",
      "Rank 944: 'mean' feature #4021 (t = 3.66)\n",
      "Rank 945: 'max' feature #4476 (t = 3.66)\n",
      "Rank 946: 'max' feature #8111 (t = 3.66)\n",
      "Rank 947: 'mean' feature #443 (t = 3.66)\n",
      "Rank 948: 'max' feature #2458 (t = 3.65)\n",
      "Rank 949: 'mean' feature #4456 (t = 3.65)\n",
      "Rank 950: 'max' feature #6594 (t = 3.65)\n",
      "Rank 951: 'mean' feature #3908 (t = 3.65)\n",
      "Rank 952: 'mean' feature #7302 (t = 3.65)\n",
      "Rank 953: 'max' feature #3552 (t = 3.65)\n",
      "Rank 954: 'mean' feature #7528 (t = 3.65)\n",
      "Rank 955: 'max' feature #5100 (t = 3.65)\n",
      "Rank 956: 'max' feature #5477 (t = 3.65)\n",
      "Rank 957: 'mean' feature #3048 (t = 3.65)\n",
      "Rank 958: 'mean' feature #5144 (t = 3.65)\n",
      "Rank 959: 'max' feature #3557 (t = 3.65)\n",
      "Rank 960: 'max' feature #2311 (t = 3.65)\n",
      "Rank 961: 'mean' feature #143 (t = 3.65)\n",
      "Rank 962: 'max' feature #4759 (t = 3.64)\n",
      "Rank 963: 'mean' feature #2630 (t = 3.64)\n",
      "Rank 964: 'mean' feature #7245 (t = 3.64)\n",
      "Rank 965: 'mean' feature #5409 (t = 3.64)\n",
      "Rank 966: 'max' feature #5446 (t = 3.64)\n",
      "Rank 967: 'max' feature #1715 (t = 3.64)\n",
      "Rank 968: 'mean' feature #7373 (t = 3.64)\n",
      "Rank 969: 'mean' feature #1854 (t = 3.64)\n",
      "Rank 970: 'max' feature #3202 (t = 3.63)\n",
      "Rank 971: 'max' feature #7593 (t = 3.63)\n",
      "Rank 972: 'mean' feature #3047 (t = 3.63)\n",
      "Rank 973: 'mean' feature #3823 (t = 3.63)\n",
      "Rank 974: 'mean' feature #7203 (t = 3.63)\n",
      "Rank 975: 'mean' feature #3922 (t = 3.63)\n",
      "Rank 976: 'mean' feature #6405 (t = 3.63)\n",
      "Rank 977: 'mean' feature #2826 (t = 3.63)\n",
      "Rank 978: 'mean' feature #2267 (t = 3.62)\n",
      "Rank 979: 'mean' feature #6266 (t = 3.62)\n",
      "Rank 980: 'max' feature #7624 (t = 3.62)\n",
      "Rank 981: 'max' feature #1698 (t = 3.62)\n",
      "Rank 982: 'mean' feature #4206 (t = 3.62)\n",
      "Rank 983: 'max' feature #6420 (t = 3.62)\n",
      "Rank 984: 'mean' feature #6432 (t = 3.62)\n",
      "Rank 985: 'mean' feature #6948 (t = 3.62)\n",
      "Rank 986: 'max' feature #7007 (t = 3.61)\n",
      "Rank 987: 'mean' feature #7982 (t = 3.61)\n",
      "Rank 988: 'max' feature #5861 (t = 3.61)\n",
      "Rank 989: 'mean' feature #4344 (t = 3.61)\n",
      "Rank 990: 'max' feature #2214 (t = 3.61)\n",
      "Rank 991: 'max' feature #2592 (t = 3.61)\n",
      "Rank 992: 'max' feature #7632 (t = 3.61)\n",
      "Rank 993: 'max' feature #2886 (t = 3.61)\n",
      "Rank 994: 'max' feature #3674 (t = 3.61)\n",
      "Rank 995: 'max' feature #4734 (t = 3.61)\n",
      "Rank 996: 'max' feature #4555 (t = 3.61)\n",
      "Rank 997: 'mean' feature #1582 (t = 3.61)\n",
      "Rank 998: 'max' feature #2065 (t = 3.60)\n",
      "Rank 999: 'mean' feature #4886 (t = 3.60)\n",
      "Rank 1000: 'max' feature #1615 (t = 3.60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_415405/4004282485.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  t_stats = np.abs((X_pos.mean(0) - X_neg.mean(0)) /\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# meta = meta.assign(label=lambda df: df.SUESCORE.map(lambda s: 1 if s>=0.5 else (0 if s<=-0.5 else np.nan)))\n",
    "# mask = meta.label.notna().values\n",
    "# Xc, y = Xc_aligned[mask], meta.loc[mask, \"label\"].astype(int).values\n",
    "\n",
    "D2 = Xc.shape[1]\n",
    "D = D2 // 2\n",
    "X_pos, X_neg = Xc[y==1], Xc[y==0]\n",
    "t_stats = np.abs((X_pos.mean(0) - X_neg.mean(0)) /\n",
    "                 np.sqrt(X_pos.var(0)/len(X_pos) + X_neg.var(0)/len(X_neg)))\n",
    "\n",
    "ranked_idx = np.argsort(-t_stats)\n",
    "\n",
    "\n",
    "for rank, idx in enumerate(ranked_idx[:1000], start=1):\n",
    "    # print(idx)\n",
    "    part = \"mean\" if idx < D else \"max\"\n",
    "    # print(idx)\n",
    "    # print(D)\n",
    "    feat_id = idx if idx < D else idx-D\n",
    "    t_val   = t_stats[idx]\n",
    "    print(f\"Rank {rank:2d}: {part!r} feature #{feat_id} (t = {t_val:.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd0a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_idx = ranked_idx[:]\n",
    "# X_test = X_test_all_feat[:, :16384]\n",
    "# X_val = X_val_all_feat[:, :16384]\n",
    "# X_top = Xc[:, :16384]      \n",
    "X_test = X_test_all_feat[:, 16384:]\n",
    "X_val = X_val_all_feat[:, 16384:]\n",
    "X_top = Xc[:, 16384:]   \n",
    "\n",
    "X_train = X_top\n",
    "y_train = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c93d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train with L1 logistic regression & balanced class weights\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        # class_weight=\"balanced\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6) Evaluate\n",
    "y_pred   = clf.predict(X_test)\n",
    "y_probs  = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_probs))\n",
    "\n",
    "# 7) Inspect which of your top-1000 actually got nonzero weights\n",
    "lr = clf.named_steps[\"logisticregression\"]\n",
    "coefs = lr.coef_.ravel()\n",
    "nz    = np.where(coefs != 0)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfaa751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=10000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01459b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train loss = 0.5225\n",
      "Epoch  2: train loss = 0.3806\n",
      "Epoch  3: train loss = 0.2680\n",
      "Epoch  4: train loss = 0.1722\n",
      "Epoch  5: train loss = 0.1100\n",
      "Epoch  6: train loss = 0.0770\n",
      "Epoch  7: train loss = 0.1001\n",
      "Epoch  8: train loss = 0.0498\n",
      "Epoch  9: train loss = 0.0583\n",
      "Epoch 10: train loss = 0.0798\n",
      "Epoch 11: train loss = 0.0553\n",
      "Epoch 12: train loss = 0.0487\n",
      "Epoch 13: train loss = 0.0444\n",
      "Epoch 14: train loss = 0.0385\n",
      "Epoch 15: train loss = 0.0406\n",
      "Epoch 16: train loss = 0.0573\n",
      "Epoch 17: train loss = 0.0739\n",
      "Epoch 18: train loss = 0.0661\n",
      "Epoch 19: train loss = 0.0637\n",
      "Epoch 20: train loss = 0.0357\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.29      0.30       222\n",
      "           1       0.80      0.82      0.81       793\n",
      "\n",
      "    accuracy                           0.71      1015\n",
      "   macro avg       0.56      0.56      0.56      1015\n",
      "weighted avg       0.70      0.71      0.70      1015\n",
      "\n",
      "Test ROC AUC: 0.603447394431001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_t = torch.from_numpy(X_train).float().to(device)\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\n",
    "X_test_t  = torch.from_numpy(X_test).float().to(device)\n",
    "y_test_t  = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_top.shape[1]).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 4) Training loop\n",
    "n_epochs = 20\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch:2d}: train loss = {avg_loss:.4f}\")\n",
    "\n",
    "# 5) Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1333966a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "\n",
    "scale_pos_weight = float((y_train == 0).sum()) / (y_train == 1).sum()\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\":        \"binary:logistic\",\n",
    "    \"eval_metric\":      \"auc\",\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"tree_method\":      \"hist\",       \n",
    "    \"grow_policy\":      \"lossguide\",  \n",
    "    \"max_depth\":        6,\n",
    "    \"learning_rate\":    0.1,\n",
    "    \"subsample\":        0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\":     42,\n",
    "    \"verbosity\":        1\n",
    "}\n",
    "\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=20,\n",
    "    metrics=\"auc\",\n",
    "    seed=42,\n",
    "    as_pandas=True,\n",
    "    verbose_eval=50\n",
    ")\n",
    "best_rounds = len(cv_results)\n",
    "print(f\"Optimal boosting rounds: {best_rounds}\")\n",
    "\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=best_rounds\n",
    ")\n",
    "\n",
    "\n",
    "y_prob = bst.predict(dtest)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fd74a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train Loss: 0.7997  Val Loss: 0.7110\n",
      "Epoch  2  Train Loss: 0.8021  Val Loss: 0.7170\n",
      "Epoch  3  Train Loss: 0.7923  Val Loss: 0.7106\n",
      "Epoch  4  Train Loss: 0.7861  Val Loss: 0.7077\n",
      "Epoch  5  Train Loss: 0.7820  Val Loss: 0.7075\n",
      "Epoch  6  Train Loss: 0.7804  Val Loss: 0.7122\n",
      "Epoch  7  Train Loss: 0.7810  Val Loss: 0.7059\n",
      "Epoch  8  Train Loss: 0.7785  Val Loss: 0.6966\n",
      "Epoch  9  Train Loss: 0.7692  Val Loss: 0.7002\n",
      "Epoch 10  Train Loss: 0.7636  Val Loss: 0.6978\n",
      "Epoch 11  Train Loss: 0.7578  Val Loss: 0.6964\n",
      "Epoch 12  Train Loss: 0.7569  Val Loss: 0.6925\n",
      "Epoch 13  Train Loss: 0.7509  Val Loss: 0.6911\n",
      "Epoch 14  Train Loss: 0.7407  Val Loss: 0.6904\n",
      "Epoch 15  Train Loss: 0.7380  Val Loss: 0.6897\n",
      "Epoch 16  Train Loss: 0.7382  Val Loss: 0.6918\n",
      "Epoch 17  Train Loss: 0.7306  Val Loss: 0.6853\n",
      "Epoch 18  Train Loss: 0.7281  Val Loss: 0.6788\n",
      "Epoch 19  Train Loss: 0.7247  Val Loss: 0.6831\n",
      "Epoch 20  Train Loss: 0.7266  Val Loss: 0.6841\n",
      "Epoch 21  Train Loss: 0.7235  Val Loss: 0.6762\n",
      "Epoch 22  Train Loss: 0.7150  Val Loss: 0.6812\n",
      "Epoch 23  Train Loss: 0.7176  Val Loss: 0.6756\n",
      "Epoch 24  Train Loss: 0.7145  Val Loss: 0.6745\n",
      "Epoch 25  Train Loss: 0.7028  Val Loss: 0.6749\n",
      "Epoch 26  Train Loss: 0.7013  Val Loss: 0.6725\n",
      "Epoch 27  Train Loss: 0.6953  Val Loss: 0.6709\n",
      "Epoch 28  Train Loss: 0.7017  Val Loss: 0.6702\n",
      "Epoch 29  Train Loss: 0.6880  Val Loss: 0.6630\n",
      "Epoch 30  Train Loss: 0.6871  Val Loss: 0.6620\n",
      "Epoch 31  Train Loss: 0.6852  Val Loss: 0.6580\n",
      "Epoch 32  Train Loss: 0.6789  Val Loss: 0.6624\n",
      "Epoch 33  Train Loss: 0.6768  Val Loss: 0.6614\n",
      "Epoch 34  Train Loss: 0.6768  Val Loss: 0.6656\n",
      "Epoch 35  Train Loss: 0.6782  Val Loss: 0.6556\n",
      "Epoch 36  Train Loss: 0.6803  Val Loss: 0.6552\n",
      "Epoch 37  Train Loss: 0.6698  Val Loss: 0.6596\n",
      "Epoch 38  Train Loss: 0.6648  Val Loss: 0.6575\n",
      "Epoch 39  Train Loss: 0.6572  Val Loss: 0.6573\n",
      "Epoch 40  Train Loss: 0.6581  Val Loss: 0.6521\n",
      "Epoch 41  Train Loss: 0.6518  Val Loss: 0.6540\n",
      "Epoch 42  Train Loss: 0.6504  Val Loss: 0.6511\n",
      "Epoch 43  Train Loss: 0.6499  Val Loss: 0.6469\n",
      "Epoch 44  Train Loss: 0.6502  Val Loss: 0.6485\n",
      "Epoch 45  Train Loss: 0.6492  Val Loss: 0.6489\n",
      "Epoch 46  Train Loss: 0.6397  Val Loss: 0.6444\n",
      "Epoch 47  Train Loss: 0.6436  Val Loss: 0.6470\n",
      "Epoch 48  Train Loss: 0.6370  Val Loss: 0.6487\n",
      "Epoch 49  Train Loss: 0.6317  Val Loss: 0.6483\n",
      "Epoch 50  Train Loss: 0.6241  Val Loss: 0.6386\n",
      "Epoch 51  Train Loss: 0.6247  Val Loss: 0.6357\n",
      "Epoch 52  Train Loss: 0.6297  Val Loss: 0.6371\n",
      "Epoch 53  Train Loss: 0.6262  Val Loss: 0.6442\n",
      "Epoch 54  Train Loss: 0.6256  Val Loss: 0.6432\n",
      "Epoch 55  Train Loss: 0.6182  Val Loss: 0.6378\n",
      "Epoch 56  Train Loss: 0.6154  Val Loss: 0.6308\n",
      "Epoch 57  Train Loss: 0.6087  Val Loss: 0.6420\n",
      "Epoch 58  Train Loss: 0.6117  Val Loss: 0.6312\n",
      "Epoch 59  Train Loss: 0.6104  Val Loss: 0.6349\n",
      "Epoch 60  Train Loss: 0.6005  Val Loss: 0.6298\n",
      "Epoch 61  Train Loss: 0.6009  Val Loss: 0.6282\n",
      "Epoch 62  Train Loss: 0.6003  Val Loss: 0.6301\n",
      "Epoch 63  Train Loss: 0.5961  Val Loss: 0.6274\n",
      "Epoch 64  Train Loss: 0.5965  Val Loss: 0.6301\n",
      "Epoch 65  Train Loss: 0.5949  Val Loss: 0.6267\n",
      "Epoch 66  Train Loss: 0.5897  Val Loss: 0.6290\n",
      "Epoch 67  Train Loss: 0.5904  Val Loss: 0.6287\n",
      "Epoch 68  Train Loss: 0.5845  Val Loss: 0.6192\n",
      "Epoch 69  Train Loss: 0.5795  Val Loss: 0.6268\n",
      "Epoch 70  Train Loss: 0.5799  Val Loss: 0.6253\n",
      "Epoch 71  Train Loss: 0.5761  Val Loss: 0.6163\n",
      "Epoch 72  Train Loss: 0.5796  Val Loss: 0.6256\n",
      "Epoch 73  Train Loss: 0.5761  Val Loss: 0.6198\n",
      "Epoch 74  Train Loss: 0.5786  Val Loss: 0.6196\n",
      "Epoch 75  Train Loss: 0.5739  Val Loss: 0.6242\n",
      "Epoch 76  Train Loss: 0.5650  Val Loss: 0.6172\n",
      "Epoch 77  Train Loss: 0.5685  Val Loss: 0.6149\n",
      "Epoch 78  Train Loss: 0.5638  Val Loss: 0.6104\n",
      "Epoch 79  Train Loss: 0.5617  Val Loss: 0.6155\n",
      "Epoch 80  Train Loss: 0.5595  Val Loss: 0.6135\n",
      "Epoch 81  Train Loss: 0.5558  Val Loss: 0.6140\n",
      "Epoch 82  Train Loss: 0.5523  Val Loss: 0.6105\n",
      "Epoch 83  Train Loss: 0.5524  Val Loss: 0.6057\n",
      "Epoch 84  Train Loss: 0.5548  Val Loss: 0.6095\n",
      "Epoch 85  Train Loss: 0.5596  Val Loss: 0.6095\n",
      "Epoch 86  Train Loss: 0.5476  Val Loss: 0.6027\n",
      "Epoch 87  Train Loss: 0.5511  Val Loss: 0.6102\n",
      "Epoch 88  Train Loss: 0.5387  Val Loss: 0.6169\n",
      "Epoch 89  Train Loss: 0.5415  Val Loss: 0.6087\n",
      "Epoch 90  Train Loss: 0.5396  Val Loss: 0.6022\n",
      "Epoch 91  Train Loss: 0.5328  Val Loss: 0.6065\n",
      "Epoch 92  Train Loss: 0.5390  Val Loss: 0.6122\n",
      "Epoch 93  Train Loss: 0.5384  Val Loss: 0.6016\n",
      "Epoch 94  Train Loss: 0.5359  Val Loss: 0.5986\n",
      "Epoch 95  Train Loss: 0.5298  Val Loss: 0.5981\n",
      "Epoch 96  Train Loss: 0.5311  Val Loss: 0.6022\n",
      "Epoch 97  Train Loss: 0.5312  Val Loss: 0.6025\n",
      "Epoch 98  Train Loss: 0.5233  Val Loss: 0.5989\n",
      "Epoch 99  Train Loss: 0.5179  Val Loss: 0.6008\n",
      "Epoch 100  Train Loss: 0.5254  Val Loss: 0.6110\n",
      "Epoch 101  Train Loss: 0.5232  Val Loss: 0.5992\n",
      "Epoch 102  Train Loss: 0.5133  Val Loss: 0.5967\n",
      "Epoch 103  Train Loss: 0.5233  Val Loss: 0.5944\n",
      "Epoch 104  Train Loss: 0.5128  Val Loss: 0.5946\n",
      "Epoch 105  Train Loss: 0.5146  Val Loss: 0.5927\n",
      "Epoch 106  Train Loss: 0.5102  Val Loss: 0.5922\n",
      "Epoch 107  Train Loss: 0.5120  Val Loss: 0.5920\n",
      "Epoch 108  Train Loss: 0.5091  Val Loss: 0.5902\n",
      "Epoch 109  Train Loss: 0.5050  Val Loss: 0.5943\n",
      "Epoch 110  Train Loss: 0.5032  Val Loss: 0.5943\n",
      "Epoch 111  Train Loss: 0.5055  Val Loss: 0.5901\n",
      "Epoch 112  Train Loss: 0.4981  Val Loss: 0.5934\n",
      "Epoch 113  Train Loss: 0.5012  Val Loss: 0.5836\n",
      "Epoch 114  Train Loss: 0.5037  Val Loss: 0.5856\n",
      "Epoch 115  Train Loss: 0.4999  Val Loss: 0.5881\n",
      "Epoch 116  Train Loss: 0.5030  Val Loss: 0.5893\n",
      "Epoch 117  Train Loss: 0.4926  Val Loss: 0.5923\n",
      "Epoch 118  Train Loss: 0.4900  Val Loss: 0.5879\n",
      "Epoch 119  Train Loss: 0.4865  Val Loss: 0.5928\n",
      "Epoch 120  Train Loss: 0.4941  Val Loss: 0.5840\n",
      "Epoch 121  Train Loss: 0.4942  Val Loss: 0.5872\n",
      "Epoch 122  Train Loss: 0.4897  Val Loss: 0.5816\n",
      "Epoch 123  Train Loss: 0.4909  Val Loss: 0.5819\n",
      "Epoch 124  Train Loss: 0.4882  Val Loss: 0.5823\n",
      "Epoch 125  Train Loss: 0.4798  Val Loss: 0.5889\n",
      "Epoch 126  Train Loss: 0.4814  Val Loss: 0.5858\n",
      "Epoch 127  Train Loss: 0.4848  Val Loss: 0.5863\n",
      "Epoch 128  Train Loss: 0.4816  Val Loss: 0.5777\n",
      "Epoch 129  Train Loss: 0.4746  Val Loss: 0.5846\n",
      "Epoch 130  Train Loss: 0.4738  Val Loss: 0.5801\n",
      "Epoch 131  Train Loss: 0.4747  Val Loss: 0.5819\n",
      "Epoch 132  Train Loss: 0.4712  Val Loss: 0.5795\n",
      "Epoch 133  Train Loss: 0.4738  Val Loss: 0.5768\n",
      "Epoch 134  Train Loss: 0.4712  Val Loss: 0.5800\n",
      "Epoch 135  Train Loss: 0.4657  Val Loss: 0.5856\n",
      "Epoch 136  Train Loss: 0.4664  Val Loss: 0.5798\n",
      "Epoch 137  Train Loss: 0.4636  Val Loss: 0.5748\n",
      "Epoch 138  Train Loss: 0.4690  Val Loss: 0.5737\n",
      "Epoch 139  Train Loss: 0.4582  Val Loss: 0.5805\n",
      "Epoch 140  Train Loss: 0.4577  Val Loss: 0.5796\n",
      "Epoch 141  Train Loss: 0.4647  Val Loss: 0.5762\n",
      "Epoch 142  Train Loss: 0.4585  Val Loss: 0.5828\n",
      "Epoch 143  Train Loss: 0.4571  Val Loss: 0.5769\n",
      "Epoch 144  Train Loss: 0.4549  Val Loss: 0.5719\n",
      "Epoch 145  Train Loss: 0.4529  Val Loss: 0.5735\n",
      "Epoch 146  Train Loss: 0.4566  Val Loss: 0.5876\n",
      "Epoch 147  Train Loss: 0.4481  Val Loss: 0.5658\n",
      "Epoch 148  Train Loss: 0.4521  Val Loss: 0.5720\n",
      "Epoch 149  Train Loss: 0.4528  Val Loss: 0.5794\n",
      "Epoch 150  Train Loss: 0.4491  Val Loss: 0.5745\n",
      "Epoch 151  Train Loss: 0.4400  Val Loss: 0.5725\n",
      "Epoch 152  Train Loss: 0.4447  Val Loss: 0.5786\n",
      "Epoch 153  Train Loss: 0.4408  Val Loss: 0.5693\n",
      "Epoch 154  Train Loss: 0.4470  Val Loss: 0.5742\n",
      "Epoch 155  Train Loss: 0.4387  Val Loss: 0.5784\n",
      "Epoch 156  Train Loss: 0.4400  Val Loss: 0.5725\n",
      "Epoch 157  Train Loss: 0.4396  Val Loss: 0.5767\n",
      "Epoch 158  Train Loss: 0.4363  Val Loss: 0.5727\n",
      "Epoch 159  Train Loss: 0.4365  Val Loss: 0.5667\n",
      "Epoch 160  Train Loss: 0.4354  Val Loss: 0.5732\n",
      "Epoch 161  Train Loss: 0.4327  Val Loss: 0.5712\n",
      "Epoch 162  Train Loss: 0.4324  Val Loss: 0.5690\n",
      "Epoch 163  Train Loss: 0.4333  Val Loss: 0.5728\n",
      "Epoch 164  Train Loss: 0.4306  Val Loss: 0.5710\n",
      "Epoch 165  Train Loss: 0.4298  Val Loss: 0.5724\n",
      "Epoch 166  Train Loss: 0.4256  Val Loss: 0.5673\n",
      "Epoch 167  Train Loss: 0.4245  Val Loss: 0.5651\n",
      "Epoch 168  Train Loss: 0.4215  Val Loss: 0.5775\n",
      "Epoch 169  Train Loss: 0.4177  Val Loss: 0.5691\n",
      "Epoch 170  Train Loss: 0.4232  Val Loss: 0.5659\n",
      "Epoch 171  Train Loss: 0.4155  Val Loss: 0.5686\n",
      "Epoch 172  Train Loss: 0.4206  Val Loss: 0.5729\n",
      "Epoch 173  Train Loss: 0.4140  Val Loss: 0.5657\n",
      "Epoch 174  Train Loss: 0.4191  Val Loss: 0.5626\n",
      "Epoch 175  Train Loss: 0.4077  Val Loss: 0.5634\n",
      "Epoch 176  Train Loss: 0.4092  Val Loss: 0.5721\n",
      "Epoch 177  Train Loss: 0.4107  Val Loss: 0.5665\n",
      "Epoch 178  Train Loss: 0.4025  Val Loss: 0.5683\n",
      "Epoch 179  Train Loss: 0.4013  Val Loss: 0.5729\n",
      "Epoch 180  Train Loss: 0.4083  Val Loss: 0.5644\n",
      "Epoch 181  Train Loss: 0.3982  Val Loss: 0.5701\n",
      "Epoch 182  Train Loss: 0.4049  Val Loss: 0.5657\n",
      "Epoch 183  Train Loss: 0.3995  Val Loss: 0.5742\n",
      "Epoch 184  Train Loss: 0.3983  Val Loss: 0.5651\n",
      "Epoch 185  Train Loss: 0.4023  Val Loss: 0.5680\n",
      "Epoch 186  Train Loss: 0.3970  Val Loss: 0.5675\n",
      "Epoch 187  Train Loss: 0.3954  Val Loss: 0.5673\n",
      "Epoch 188  Train Loss: 0.3953  Val Loss: 0.5719\n",
      "Epoch 189  Train Loss: 0.3940  Val Loss: 0.5679\n",
      "Epoch 190  Train Loss: 0.3907  Val Loss: 0.5591\n",
      "Epoch 191  Train Loss: 0.3888  Val Loss: 0.5661\n",
      "Epoch 192  Train Loss: 0.3926  Val Loss: 0.5656\n",
      "Epoch 193  Train Loss: 0.3886  Val Loss: 0.5688\n",
      "Epoch 194  Train Loss: 0.3876  Val Loss: 0.5652\n",
      "Epoch 195  Train Loss: 0.3873  Val Loss: 0.5652\n",
      "Epoch 196  Train Loss: 0.3838  Val Loss: 0.5640\n",
      "Epoch 197  Train Loss: 0.3823  Val Loss: 0.5619\n",
      "Epoch 198  Train Loss: 0.3812  Val Loss: 0.5732\n",
      "Epoch 199  Train Loss: 0.3808  Val Loss: 0.5664\n",
      "Epoch 200  Train Loss: 0.3711  Val Loss: 0.5651\n",
      "Epoch 201  Train Loss: 0.3742  Val Loss: 0.5601\n",
      "Epoch 202  Train Loss: 0.3701  Val Loss: 0.5548\n",
      "Epoch 203  Train Loss: 0.3746  Val Loss: 0.5686\n",
      "Epoch 204  Train Loss: 0.3703  Val Loss: 0.5596\n",
      "Epoch 205  Train Loss: 0.3719  Val Loss: 0.5608\n",
      "Epoch 206  Train Loss: 0.3664  Val Loss: 0.5674\n",
      "Epoch 207  Train Loss: 0.3660  Val Loss: 0.5664\n",
      "Epoch 208  Train Loss: 0.3594  Val Loss: 0.5605\n",
      "Epoch 209  Train Loss: 0.3636  Val Loss: 0.5620\n",
      "Epoch 210  Train Loss: 0.3646  Val Loss: 0.5599\n",
      "Epoch 211  Train Loss: 0.3623  Val Loss: 0.5643\n",
      "Epoch 212  Train Loss: 0.3613  Val Loss: 0.5669\n",
      "Epoch 213  Train Loss: 0.3622  Val Loss: 0.5718\n",
      "Epoch 214  Train Loss: 0.3537  Val Loss: 0.5624\n",
      "Epoch 215  Train Loss: 0.3575  Val Loss: 0.5654\n",
      "Epoch 216  Train Loss: 0.3486  Val Loss: 0.5670\n",
      "Epoch 217  Train Loss: 0.3512  Val Loss: 0.5629\n",
      "Epoch 218  Train Loss: 0.3544  Val Loss: 0.5650\n",
      "Epoch 219  Train Loss: 0.3535  Val Loss: 0.5680\n",
      "Epoch 220  Train Loss: 0.3479  Val Loss: 0.5802\n",
      "Epoch 221  Train Loss: 0.3501  Val Loss: 0.5554\n",
      "Epoch 222  Train Loss: 0.3466  Val Loss: 0.5688\n",
      "Epoch 223  Train Loss: 0.3384  Val Loss: 0.5718\n",
      "Epoch 224  Train Loss: 0.3422  Val Loss: 0.5642\n",
      "Epoch 225  Train Loss: 0.3356  Val Loss: 0.5656\n",
      "Epoch 226  Train Loss: 0.3350  Val Loss: 0.5691\n",
      "Epoch 227  Train Loss: 0.3355  Val Loss: 0.5616\n",
      "Epoch 228  Train Loss: 0.3287  Val Loss: 0.5707\n",
      "Epoch 229  Train Loss: 0.3296  Val Loss: 0.5697\n",
      "Epoch 230  Train Loss: 0.3277  Val Loss: 0.5680\n",
      "Epoch 231  Train Loss: 0.3278  Val Loss: 0.5785\n",
      "Epoch 232  Train Loss: 0.3194  Val Loss: 0.5730\n",
      "Epoch 233  Train Loss: 0.3237  Val Loss: 0.5700\n",
      "Epoch 234  Train Loss: 0.3244  Val Loss: 0.5729\n",
      "Epoch 235  Train Loss: 0.3230  Val Loss: 0.5632\n",
      "Epoch 236  Train Loss: 0.3192  Val Loss: 0.5679\n",
      "Epoch 237  Train Loss: 0.3151  Val Loss: 0.5696\n",
      "Epoch 238  Train Loss: 0.3113  Val Loss: 0.5657\n",
      "Epoch 239  Train Loss: 0.3122  Val Loss: 0.5576\n",
      "Epoch 240  Train Loss: 0.3158  Val Loss: 0.5654\n",
      "Epoch 241  Train Loss: 0.3133  Val Loss: 0.5648\n",
      "Epoch 242  Train Loss: 0.3080  Val Loss: 0.5687\n",
      "Epoch 243  Train Loss: 0.3065  Val Loss: 0.5732\n",
      "Epoch 244  Train Loss: 0.3060  Val Loss: 0.5663\n",
      "Epoch 245  Train Loss: 0.3026  Val Loss: 0.5670\n",
      "Epoch 246  Train Loss: 0.3036  Val Loss: 0.5631\n",
      "Epoch 247  Train Loss: 0.3031  Val Loss: 0.5655\n",
      "Epoch 248  Train Loss: 0.2923  Val Loss: 0.5744\n",
      "Epoch 249  Train Loss: 0.2934  Val Loss: 0.5673\n",
      "Epoch 250  Train Loss: 0.2928  Val Loss: 0.5742\n",
      "Epoch 251  Train Loss: 0.2931  Val Loss: 0.5635\n",
      "Epoch 252  Train Loss: 0.2946  Val Loss: 0.5760\n",
      "Epoch 253  Train Loss: 0.2947  Val Loss: 0.5587\n",
      "Epoch 254  Train Loss: 0.2925  Val Loss: 0.5794\n",
      "Epoch 255  Train Loss: 0.2897  Val Loss: 0.5659\n",
      "Epoch 256  Train Loss: 0.2875  Val Loss: 0.5722\n",
      "Epoch 257  Train Loss: 0.2879  Val Loss: 0.5745\n",
      "Epoch 258  Train Loss: 0.2826  Val Loss: 0.5741\n",
      "Epoch 259  Train Loss: 0.2820  Val Loss: 0.5808\n",
      "Epoch 260  Train Loss: 0.2784  Val Loss: 0.5832\n",
      "Epoch 261  Train Loss: 0.2785  Val Loss: 0.5675\n",
      "Epoch 262  Train Loss: 0.2804  Val Loss: 0.5713\n",
      "Epoch 263  Train Loss: 0.2748  Val Loss: 0.5751\n",
      "Epoch 264  Train Loss: 0.2674  Val Loss: 0.5770\n",
      "Epoch 265  Train Loss: 0.2689  Val Loss: 0.5712\n",
      "Epoch 266  Train Loss: 0.2676  Val Loss: 0.5652\n",
      "Epoch 267  Train Loss: 0.2729  Val Loss: 0.5838\n",
      "Epoch 268  Train Loss: 0.2694  Val Loss: 0.5687\n",
      "Epoch 269  Train Loss: 0.2635  Val Loss: 0.5678\n",
      "Epoch 270  Train Loss: 0.2638  Val Loss: 0.5813\n",
      "Epoch 271  Train Loss: 0.2617  Val Loss: 0.5837\n",
      "Epoch 272  Train Loss: 0.2657  Val Loss: 0.5733\n",
      "Epoch 273  Train Loss: 0.2559  Val Loss: 0.5889\n",
      "Epoch 274  Train Loss: 0.2598  Val Loss: 0.5756\n",
      "Epoch 275  Train Loss: 0.2550  Val Loss: 0.5814\n",
      "Epoch 276  Train Loss: 0.2561  Val Loss: 0.5675\n",
      "Epoch 277  Train Loss: 0.2529  Val Loss: 0.5706\n",
      "Epoch 278  Train Loss: 0.2498  Val Loss: 0.5806\n",
      "Epoch 279  Train Loss: 0.2483  Val Loss: 0.5805\n",
      "Epoch 280  Train Loss: 0.2453  Val Loss: 0.5820\n",
      "Epoch 281  Train Loss: 0.2464  Val Loss: 0.5856\n",
      "Epoch 282  Train Loss: 0.2493  Val Loss: 0.5691\n",
      "Epoch 283  Train Loss: 0.2450  Val Loss: 0.5756\n",
      "Epoch 284  Train Loss: 0.2357  Val Loss: 0.5727\n",
      "Epoch 285  Train Loss: 0.2392  Val Loss: 0.5897\n",
      "Epoch 286  Train Loss: 0.2389  Val Loss: 0.5654\n",
      "Epoch 287  Train Loss: 0.2344  Val Loss: 0.5849\n",
      "Epoch 288  Train Loss: 0.2299  Val Loss: 0.5887\n",
      "Epoch 289  Train Loss: 0.2311  Val Loss: 0.5766\n",
      "Epoch 290  Train Loss: 0.2343  Val Loss: 0.5849\n",
      "Epoch 291  Train Loss: 0.2299  Val Loss: 0.5712\n",
      "Epoch 292  Train Loss: 0.2240  Val Loss: 0.5869\n",
      "Epoch 293  Train Loss: 0.2227  Val Loss: 0.5740\n",
      "Epoch 294  Train Loss: 0.2264  Val Loss: 0.5744\n",
      "Epoch 295  Train Loss: 0.2285  Val Loss: 0.5869\n",
      "Epoch 296  Train Loss: 0.2227  Val Loss: 0.5913\n",
      "Epoch 297  Train Loss: 0.2228  Val Loss: 0.5823\n",
      "Epoch 298  Train Loss: 0.2224  Val Loss: 0.6041\n",
      "Epoch 299  Train Loss: 0.2242  Val Loss: 0.5901\n",
      "Epoch 300  Train Loss: 0.2155  Val Loss: 0.5922\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      0.20      0.25       222\n",
      "         1.0       0.80      0.88      0.84       793\n",
      "\n",
      "    accuracy                           0.73      1015\n",
      "   macro avg       0.56      0.54      0.54      1015\n",
      "weighted avg       0.69      0.73      0.71      1015\n",
      "\n",
      "Test ROC AUC: 0.6027515535712258\n"
     ]
    }
   ],
   "source": [
    "# 3 layer mlp\n",
    "\n",
    "def to_tensor_dataset(X, y):\n",
    "    Xt = torch.from_numpy(X).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return TensorDataset(Xt, yt)\n",
    "\n",
    "train_ds = to_tensor_dataset(X_train, y_train)\n",
    "val_ds   = to_tensor_dataset(X_val, y_val)\n",
    "test_ds  = to_tensor_dataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3) Define the 3-layer MLP\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ThreeLayerMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 4) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5) Training & Validation Loop\n",
    "n_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # -- Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # -- Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 6) Load best model and test evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_probs = []\n",
    "y_true  = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        probs = model(xb)\n",
    "        y_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "        y_true .extend(yb.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_pred = (np.array(y_probs) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_true, y_probs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
