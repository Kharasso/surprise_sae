{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85047a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c99e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6701b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(npz_paths: str, data_type: str):\n",
    "    if data_type not in [\"X_mean\", \"X_max\", \"X_concat\"]:\n",
    "        raise Exception(\"data type in valid\")\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for npz_path in npz_paths:\n",
    "    \n",
    "        base = os.path.splitext(os.path.basename(npz_path))[0]      \n",
    "        csv_path = os.path.join(\n",
    "            os.path.dirname(npz_path),\n",
    "            base + \"_meta.csv\"                                       \n",
    "        )\n",
    "\n",
    "\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        X = data[data_type]      # (N_docs, 2*D)\n",
    "        # X_concat = data[\"X_mean\"]\n",
    "        tids = data[\"transcriptids\"]    \n",
    "\n",
    "\n",
    "        meta = pd.read_csv(csv_path)\n",
    "\n",
    "        meta_unique = (\n",
    "            meta[[\"transcriptid\", \"SUESCORE\", \"label\"]]\n",
    "            .drop_duplicates(subset=\"transcriptid\", keep=\"first\")\n",
    "            .set_index(\"transcriptid\")\n",
    "        )\n",
    "\n",
    "        mask_ids = np.isin(tids, meta_unique.index)\n",
    "        X_filt = X[mask_ids]\n",
    "        tids_filt = np.array(tids)[mask_ids]\n",
    "\n",
    "\n",
    "        lab_df = meta.assign(\n",
    "            label=lambda df: df.SUESCORE.map(\n",
    "                lambda s: 1 if s >= 0.5 else (0 if s <= -0.5 else np.nan)\n",
    "            )\n",
    "        )\n",
    "        mask_label = lab_df.label.notna().values\n",
    "        # apply the same mask in the same order as the CSV, so we use .loc on lab_df\n",
    "        # but first filter lab_df to only those transcriptids in tids_filt\n",
    "        Xc, y = X_filt[mask_label], meta.loc[mask_label, \"label\"].astype(int).values\n",
    "        \n",
    "        # now align X and y\n",
    "        # X_final = X_filt[lab_sub.label.notna()]\n",
    "        # y_final = lab_sub.label.astype(int).values\n",
    "\n",
    "        # collect\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(y)\n",
    "\n",
    "    # 2. concatenate all files together\n",
    "    Xc = np.vstack(X_list)   # shape: (sum_i N_i, 2*D)\n",
    "    y  = np.concatenate(y_list)  # shape: (sum_i N_i,)\n",
    "\n",
    "    print(\"Combined Xc shape:\", Xc.shape)\n",
    "    print(\"Combined y shape: \", y.shape)\n",
    "\n",
    "    return Xc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439345d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_balance(Xc_unbalanced, y_unbalanced):\n",
    "    # forced resampling\n",
    "    idx0 = np.where(y_unbalanced == 0)[0]\n",
    "    idx1 = np.where(y_unbalanced == 1)[0]\n",
    "\n",
    "    n = min(len(idx0), len(idx1))\n",
    "\n",
    "    sel0 = np.random.choice(idx0, size=n, replace=False)\n",
    "    sel1 = np.random.choice(idx1, size=n, replace=False)\n",
    "\n",
    "    sel = np.concatenate([sel0, sel1])\n",
    "    np.random.shuffle(sel)\n",
    "\n",
    "    # slice out your balanced subset\n",
    "    Xc_out = Xc_unbalanced[sel]\n",
    "    y_out = y_unbalanced[sel]\n",
    "\n",
    "    print(\"Balanced X shape:\", Xc_out.shape)\n",
    "    print(\"Balanced y counts:\", np.bincount(y_out))\n",
    "    return Xc_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7d34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz_paths = [\n",
    "    \"./data/doc_features/transcript_componenttext_2008_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2008_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2009_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2009_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2010_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2010_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2011_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2011_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2012_1_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2012_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2013_1_features.npz\",\n",
    "]\n",
    "\n",
    "val_npz_paths = [\n",
    "    \"./data/doc_features/transcript_componenttext_2013_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2014_1_features.npz\",\n",
    "]\n",
    "\n",
    "test_npz_paths = [\n",
    "    \"./data/doc_features/transcript_componenttext_2014_2_features.npz\",\n",
    "    \"./data/doc_features/transcript_componenttext_2015_1_features.npz\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6823001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Xc shape: (10209, 16384)\n",
      "Combined y shape:  (10209,)\n",
      "Combined Xc shape: (2473, 16384)\n",
      "Combined y shape:  (2473,)\n",
      "Combined Xc shape: (2508, 16384)\n",
      "Combined y shape:  (2508,)\n"
     ]
    }
   ],
   "source": [
    "Xc, y = load_data(train_npz_paths, \"X_mean\")\n",
    "X_val_all_feat, y_val = load_data(val_npz_paths, \"X_mean\")\n",
    "X_test_all_feat, y_test = load_data(test_npz_paths, \"X_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d55b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced X shape: (4526, 16384)\n",
      "Balanced y counts: [2263 2263]\n",
      "Balanced X shape: (1108, 16384)\n",
      "Balanced y counts: [554 554]\n",
      "Balanced X shape: (1008, 16384)\n",
      "Balanced y counts: [504 504]\n"
     ]
    }
   ],
   "source": [
    "# optional downsampling for balancing data\n",
    "Xc, y = downsample_balance(Xc, y)\n",
    "X_val_all_feat, y_val = downsample_balance(X_val_all_feat, y_val)\n",
    "X_test_all_feat, y_test = downsample_balance(X_test_all_feat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e826e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  1: 'max' feature #1053 (t = 10.56)\n",
      "Rank  2: 'mean' feature #3147 (t = 10.34)\n",
      "Rank  3: 'max' feature #7429 (t = 10.16)\n",
      "Rank  4: 'max' feature #2232 (t = 9.96)\n",
      "Rank  5: 'mean' feature #6470 (t = 9.55)\n",
      "Rank  6: 'max' feature #4673 (t = 9.49)\n",
      "Rank  7: 'mean' feature #2775 (t = 9.40)\n",
      "Rank  8: 'mean' feature #482 (t = 9.35)\n",
      "Rank  9: 'max' feature #4926 (t = 9.26)\n",
      "Rank 10: 'max' feature #6178 (t = 9.24)\n",
      "Rank 11: 'mean' feature #2488 (t = 9.13)\n",
      "Rank 12: 'mean' feature #5612 (t = 9.00)\n",
      "Rank 13: 'mean' feature #2366 (t = 8.93)\n",
      "Rank 14: 'max' feature #4107 (t = 8.88)\n",
      "Rank 15: 'mean' feature #1520 (t = 8.82)\n",
      "Rank 16: 'mean' feature #1712 (t = 8.79)\n",
      "Rank 17: 'max' feature #5685 (t = 8.79)\n",
      "Rank 18: 'mean' feature #1390 (t = 8.63)\n",
      "Rank 19: 'mean' feature #2281 (t = 8.57)\n",
      "Rank 20: 'mean' feature #5553 (t = 8.48)\n",
      "Rank 21: 'max' feature #1848 (t = 8.46)\n",
      "Rank 22: 'max' feature #1543 (t = 8.33)\n",
      "Rank 23: 'max' feature #631 (t = 8.31)\n",
      "Rank 24: 'max' feature #4471 (t = 8.30)\n",
      "Rank 25: 'mean' feature #7755 (t = 8.29)\n",
      "Rank 26: 'max' feature #522 (t = 8.28)\n",
      "Rank 27: 'mean' feature #7861 (t = 8.26)\n",
      "Rank 28: 'mean' feature #2310 (t = 8.23)\n",
      "Rank 29: 'mean' feature #3101 (t = 8.20)\n",
      "Rank 30: 'mean' feature #6122 (t = 8.11)\n",
      "Rank 31: 'max' feature #4445 (t = 8.10)\n",
      "Rank 32: 'max' feature #1782 (t = 8.06)\n",
      "Rank 33: 'mean' feature #6281 (t = 8.01)\n",
      "Rank 34: 'max' feature #351 (t = 8.00)\n",
      "Rank 35: 'mean' feature #1541 (t = 7.94)\n",
      "Rank 36: 'max' feature #238 (t = 7.94)\n",
      "Rank 37: 'max' feature #2213 (t = 7.93)\n",
      "Rank 38: 'mean' feature #4412 (t = 7.91)\n",
      "Rank 39: 'max' feature #8060 (t = 7.89)\n",
      "Rank 40: 'mean' feature #8022 (t = 7.83)\n",
      "Rank 41: 'mean' feature #4750 (t = 7.79)\n",
      "Rank 42: 'max' feature #2527 (t = 7.76)\n",
      "Rank 43: 'mean' feature #3990 (t = 7.74)\n",
      "Rank 44: 'mean' feature #7357 (t = 7.71)\n",
      "Rank 45: 'max' feature #6893 (t = 7.69)\n",
      "Rank 46: 'mean' feature #1547 (t = 7.69)\n",
      "Rank 47: 'max' feature #2368 (t = 7.68)\n",
      "Rank 48: 'max' feature #125 (t = 7.55)\n",
      "Rank 49: 'mean' feature #6451 (t = 7.54)\n",
      "Rank 50: 'max' feature #3070 (t = 7.54)\n",
      "Rank 51: 'mean' feature #1980 (t = 7.51)\n",
      "Rank 52: 'max' feature #2111 (t = 7.49)\n",
      "Rank 53: 'mean' feature #6520 (t = 7.49)\n",
      "Rank 54: 'mean' feature #7804 (t = 7.49)\n",
      "Rank 55: 'mean' feature #3808 (t = 7.48)\n",
      "Rank 56: 'mean' feature #97 (t = 7.44)\n",
      "Rank 57: 'max' feature #2548 (t = 7.44)\n",
      "Rank 58: 'mean' feature #4206 (t = 7.44)\n",
      "Rank 59: 'mean' feature #1174 (t = 7.43)\n",
      "Rank 60: 'max' feature #896 (t = 7.43)\n",
      "Rank 61: 'max' feature #5888 (t = 7.42)\n",
      "Rank 62: 'max' feature #1733 (t = 7.39)\n",
      "Rank 63: 'mean' feature #1739 (t = 7.38)\n",
      "Rank 64: 'max' feature #3441 (t = 7.38)\n",
      "Rank 65: 'max' feature #2857 (t = 7.37)\n",
      "Rank 66: 'max' feature #4518 (t = 7.36)\n",
      "Rank 67: 'mean' feature #2625 (t = 7.34)\n",
      "Rank 68: 'max' feature #7423 (t = 7.32)\n",
      "Rank 69: 'mean' feature #1092 (t = 7.31)\n",
      "Rank 70: 'max' feature #188 (t = 7.31)\n",
      "Rank 71: 'max' feature #2031 (t = 7.30)\n",
      "Rank 72: 'max' feature #2323 (t = 7.30)\n",
      "Rank 73: 'max' feature #4622 (t = 7.30)\n",
      "Rank 74: 'max' feature #7905 (t = 7.29)\n",
      "Rank 75: 'mean' feature #3998 (t = 7.25)\n",
      "Rank 76: 'max' feature #7789 (t = 7.20)\n",
      "Rank 77: 'mean' feature #1885 (t = 7.20)\n",
      "Rank 78: 'max' feature #3445 (t = 7.20)\n",
      "Rank 79: 'mean' feature #856 (t = 7.19)\n",
      "Rank 80: 'mean' feature #1637 (t = 7.19)\n",
      "Rank 81: 'max' feature #6700 (t = 7.18)\n",
      "Rank 82: 'max' feature #7820 (t = 7.17)\n",
      "Rank 83: 'mean' feature #2725 (t = 7.16)\n",
      "Rank 84: 'mean' feature #3697 (t = 7.15)\n",
      "Rank 85: 'max' feature #62 (t = 7.14)\n",
      "Rank 86: 'max' feature #2268 (t = 7.12)\n",
      "Rank 87: 'mean' feature #7982 (t = 7.09)\n",
      "Rank 88: 'mean' feature #1907 (t = 7.09)\n",
      "Rank 89: 'mean' feature #6689 (t = 7.08)\n",
      "Rank 90: 'mean' feature #1363 (t = 7.08)\n",
      "Rank 91: 'mean' feature #911 (t = 7.08)\n",
      "Rank 92: 'mean' feature #6703 (t = 7.07)\n",
      "Rank 93: 'mean' feature #523 (t = 7.05)\n",
      "Rank 94: 'max' feature #2229 (t = 7.03)\n",
      "Rank 95: 'mean' feature #6770 (t = 7.03)\n",
      "Rank 96: 'max' feature #2303 (t = 7.03)\n",
      "Rank 97: 'max' feature #378 (t = 7.02)\n",
      "Rank 98: 'mean' feature #1412 (t = 7.02)\n",
      "Rank 99: 'max' feature #2464 (t = 7.02)\n",
      "Rank 100: 'mean' feature #3846 (t = 7.00)\n",
      "Rank 101: 'mean' feature #5273 (t = 7.00)\n",
      "Rank 102: 'mean' feature #1863 (t = 6.98)\n",
      "Rank 103: 'max' feature #4022 (t = 6.98)\n",
      "Rank 104: 'mean' feature #7024 (t = 6.97)\n",
      "Rank 105: 'mean' feature #5295 (t = 6.96)\n",
      "Rank 106: 'max' feature #3929 (t = 6.96)\n",
      "Rank 107: 'max' feature #3727 (t = 6.95)\n",
      "Rank 108: 'mean' feature #240 (t = 6.94)\n",
      "Rank 109: 'mean' feature #1511 (t = 6.94)\n",
      "Rank 110: 'max' feature #2305 (t = 6.93)\n",
      "Rank 111: 'max' feature #7213 (t = 6.92)\n",
      "Rank 112: 'mean' feature #484 (t = 6.87)\n",
      "Rank 113: 'mean' feature #5375 (t = 6.86)\n",
      "Rank 114: 'max' feature #7420 (t = 6.85)\n",
      "Rank 115: 'max' feature #957 (t = 6.85)\n",
      "Rank 116: 'max' feature #903 (t = 6.85)\n",
      "Rank 117: 'mean' feature #2415 (t = 6.83)\n",
      "Rank 118: 'mean' feature #6745 (t = 6.82)\n",
      "Rank 119: 'max' feature #2974 (t = 6.80)\n",
      "Rank 120: 'mean' feature #1924 (t = 6.78)\n",
      "Rank 121: 'max' feature #1494 (t = 6.78)\n",
      "Rank 122: 'max' feature #1633 (t = 6.77)\n",
      "Rank 123: 'max' feature #1980 (t = 6.77)\n",
      "Rank 124: 'max' feature #4257 (t = 6.77)\n",
      "Rank 125: 'max' feature #8111 (t = 6.77)\n",
      "Rank 126: 'max' feature #8086 (t = 6.73)\n",
      "Rank 127: 'max' feature #5778 (t = 6.72)\n",
      "Rank 128: 'mean' feature #1754 (t = 6.70)\n",
      "Rank 129: 'max' feature #2933 (t = 6.70)\n",
      "Rank 130: 'max' feature #2957 (t = 6.69)\n",
      "Rank 131: 'max' feature #7604 (t = 6.69)\n",
      "Rank 132: 'max' feature #4292 (t = 6.67)\n",
      "Rank 133: 'max' feature #7452 (t = 6.65)\n",
      "Rank 134: 'max' feature #4717 (t = 6.63)\n",
      "Rank 135: 'mean' feature #2352 (t = 6.62)\n",
      "Rank 136: 'mean' feature #2133 (t = 6.61)\n",
      "Rank 137: 'max' feature #4867 (t = 6.61)\n",
      "Rank 138: 'mean' feature #6083 (t = 6.59)\n",
      "Rank 139: 'max' feature #6135 (t = 6.56)\n",
      "Rank 140: 'mean' feature #3093 (t = 6.54)\n",
      "Rank 141: 'max' feature #7382 (t = 6.54)\n",
      "Rank 142: 'mean' feature #6615 (t = 6.54)\n",
      "Rank 143: 'max' feature #1412 (t = 6.53)\n",
      "Rank 144: 'mean' feature #2819 (t = 6.53)\n",
      "Rank 145: 'mean' feature #7979 (t = 6.52)\n",
      "Rank 146: 'mean' feature #6045 (t = 6.51)\n",
      "Rank 147: 'mean' feature #6902 (t = 6.50)\n",
      "Rank 148: 'max' feature #3521 (t = 6.49)\n",
      "Rank 149: 'max' feature #6222 (t = 6.48)\n",
      "Rank 150: 'max' feature #2737 (t = 6.47)\n",
      "Rank 151: 'max' feature #7365 (t = 6.47)\n",
      "Rank 152: 'mean' feature #5690 (t = 6.45)\n",
      "Rank 153: 'mean' feature #5817 (t = 6.45)\n",
      "Rank 154: 'max' feature #3793 (t = 6.45)\n",
      "Rank 155: 'max' feature #6416 (t = 6.45)\n",
      "Rank 156: 'max' feature #7608 (t = 6.42)\n",
      "Rank 157: 'max' feature #6936 (t = 6.41)\n",
      "Rank 158: 'max' feature #4090 (t = 6.41)\n",
      "Rank 159: 'mean' feature #3604 (t = 6.40)\n",
      "Rank 160: 'max' feature #6063 (t = 6.40)\n",
      "Rank 161: 'mean' feature #8097 (t = 6.39)\n",
      "Rank 162: 'max' feature #1096 (t = 6.39)\n",
      "Rank 163: 'mean' feature #5647 (t = 6.39)\n",
      "Rank 164: 'max' feature #475 (t = 6.38)\n",
      "Rank 165: 'mean' feature #3585 (t = 6.38)\n",
      "Rank 166: 'mean' feature #1988 (t = 6.37)\n",
      "Rank 167: 'mean' feature #7456 (t = 6.36)\n",
      "Rank 168: 'max' feature #2233 (t = 6.35)\n",
      "Rank 169: 'max' feature #6430 (t = 6.35)\n",
      "Rank 170: 'max' feature #797 (t = 6.34)\n",
      "Rank 171: 'mean' feature #3957 (t = 6.32)\n",
      "Rank 172: 'mean' feature #1880 (t = 6.31)\n",
      "Rank 173: 'max' feature #2346 (t = 6.30)\n",
      "Rank 174: 'max' feature #4808 (t = 6.30)\n",
      "Rank 175: 'mean' feature #931 (t = 6.30)\n",
      "Rank 176: 'max' feature #6973 (t = 6.29)\n",
      "Rank 177: 'max' feature #4036 (t = 6.29)\n",
      "Rank 178: 'max' feature #6220 (t = 6.29)\n",
      "Rank 179: 'mean' feature #416 (t = 6.28)\n",
      "Rank 180: 'max' feature #5879 (t = 6.28)\n",
      "Rank 181: 'mean' feature #3874 (t = 6.27)\n",
      "Rank 182: 'mean' feature #527 (t = 6.27)\n",
      "Rank 183: 'max' feature #5771 (t = 6.26)\n",
      "Rank 184: 'mean' feature #5828 (t = 6.25)\n",
      "Rank 185: 'max' feature #2934 (t = 6.24)\n",
      "Rank 186: 'max' feature #7456 (t = 6.22)\n",
      "Rank 187: 'max' feature #1038 (t = 6.21)\n",
      "Rank 188: 'max' feature #1419 (t = 6.21)\n",
      "Rank 189: 'max' feature #5454 (t = 6.20)\n",
      "Rank 190: 'max' feature #5942 (t = 6.20)\n",
      "Rank 191: 'max' feature #771 (t = 6.20)\n",
      "Rank 192: 'mean' feature #266 (t = 6.19)\n",
      "Rank 193: 'max' feature #6427 (t = 6.18)\n",
      "Rank 194: 'mean' feature #7589 (t = 6.18)\n",
      "Rank 195: 'mean' feature #1477 (t = 6.18)\n",
      "Rank 196: 'mean' feature #4512 (t = 6.17)\n",
      "Rank 197: 'max' feature #1028 (t = 6.17)\n",
      "Rank 198: 'max' feature #6414 (t = 6.17)\n",
      "Rank 199: 'max' feature #2686 (t = 6.14)\n",
      "Rank 200: 'mean' feature #4606 (t = 6.13)\n",
      "Rank 201: 'max' feature #5206 (t = 6.13)\n",
      "Rank 202: 'max' feature #1858 (t = 6.12)\n",
      "Rank 203: 'max' feature #7326 (t = 6.11)\n",
      "Rank 204: 'max' feature #1756 (t = 6.11)\n",
      "Rank 205: 'max' feature #7691 (t = 6.10)\n",
      "Rank 206: 'max' feature #183 (t = 6.07)\n",
      "Rank 207: 'max' feature #4819 (t = 6.07)\n",
      "Rank 208: 'mean' feature #4838 (t = 6.05)\n",
      "Rank 209: 'max' feature #4323 (t = 6.04)\n",
      "Rank 210: 'mean' feature #7093 (t = 6.04)\n",
      "Rank 211: 'mean' feature #1785 (t = 6.04)\n",
      "Rank 212: 'mean' feature #216 (t = 6.04)\n",
      "Rank 213: 'max' feature #37 (t = 6.03)\n",
      "Rank 214: 'max' feature #221 (t = 6.03)\n",
      "Rank 215: 'mean' feature #6790 (t = 6.02)\n",
      "Rank 216: 'max' feature #76 (t = 6.02)\n",
      "Rank 217: 'max' feature #2198 (t = 6.02)\n",
      "Rank 218: 'mean' feature #2440 (t = 6.02)\n",
      "Rank 219: 'max' feature #794 (t = 6.01)\n",
      "Rank 220: 'max' feature #3823 (t = 6.00)\n",
      "Rank 221: 'mean' feature #7227 (t = 5.99)\n",
      "Rank 222: 'max' feature #3417 (t = 5.99)\n",
      "Rank 223: 'max' feature #3618 (t = 5.98)\n",
      "Rank 224: 'mean' feature #7302 (t = 5.98)\n",
      "Rank 225: 'mean' feature #5134 (t = 5.98)\n",
      "Rank 226: 'mean' feature #2236 (t = 5.98)\n",
      "Rank 227: 'max' feature #7546 (t = 5.98)\n",
      "Rank 228: 'max' feature #5279 (t = 5.98)\n",
      "Rank 229: 'mean' feature #3507 (t = 5.97)\n",
      "Rank 230: 'mean' feature #4264 (t = 5.95)\n",
      "Rank 231: 'mean' feature #6038 (t = 5.94)\n",
      "Rank 232: 'max' feature #7232 (t = 5.94)\n",
      "Rank 233: 'max' feature #4548 (t = 5.94)\n",
      "Rank 234: 'max' feature #5786 (t = 5.93)\n",
      "Rank 235: 'max' feature #108 (t = 5.93)\n",
      "Rank 236: 'max' feature #5166 (t = 5.93)\n",
      "Rank 237: 'mean' feature #1405 (t = 5.92)\n",
      "Rank 238: 'max' feature #5353 (t = 5.92)\n",
      "Rank 239: 'max' feature #734 (t = 5.92)\n",
      "Rank 240: 'mean' feature #3461 (t = 5.91)\n",
      "Rank 241: 'mean' feature #2770 (t = 5.91)\n",
      "Rank 242: 'max' feature #5203 (t = 5.89)\n",
      "Rank 243: 'mean' feature #2735 (t = 5.87)\n",
      "Rank 244: 'max' feature #29 (t = 5.87)\n",
      "Rank 245: 'mean' feature #585 (t = 5.86)\n",
      "Rank 246: 'max' feature #3360 (t = 5.86)\n",
      "Rank 247: 'max' feature #83 (t = 5.86)\n",
      "Rank 248: 'max' feature #264 (t = 5.86)\n",
      "Rank 249: 'max' feature #3734 (t = 5.85)\n",
      "Rank 250: 'max' feature #3062 (t = 5.85)\n",
      "Rank 251: 'mean' feature #6055 (t = 5.85)\n",
      "Rank 252: 'mean' feature #4084 (t = 5.84)\n",
      "Rank 253: 'mean' feature #7503 (t = 5.84)\n",
      "Rank 254: 'max' feature #5296 (t = 5.84)\n",
      "Rank 255: 'max' feature #1738 (t = 5.84)\n",
      "Rank 256: 'mean' feature #4502 (t = 5.83)\n",
      "Rank 257: 'mean' feature #5998 (t = 5.82)\n",
      "Rank 258: 'mean' feature #2733 (t = 5.81)\n",
      "Rank 259: 'max' feature #8091 (t = 5.81)\n",
      "Rank 260: 'max' feature #2528 (t = 5.81)\n",
      "Rank 261: 'max' feature #4001 (t = 5.81)\n",
      "Rank 262: 'mean' feature #3982 (t = 5.80)\n",
      "Rank 263: 'mean' feature #2041 (t = 5.80)\n",
      "Rank 264: 'mean' feature #4013 (t = 5.80)\n",
      "Rank 265: 'max' feature #5780 (t = 5.79)\n",
      "Rank 266: 'mean' feature #945 (t = 5.79)\n",
      "Rank 267: 'mean' feature #1581 (t = 5.78)\n",
      "Rank 268: 'max' feature #6553 (t = 5.77)\n",
      "Rank 269: 'mean' feature #3785 (t = 5.77)\n",
      "Rank 270: 'mean' feature #5445 (t = 5.75)\n",
      "Rank 271: 'max' feature #5414 (t = 5.75)\n",
      "Rank 272: 'max' feature #7196 (t = 5.75)\n",
      "Rank 273: 'mean' feature #218 (t = 5.75)\n",
      "Rank 274: 'max' feature #232 (t = 5.74)\n",
      "Rank 275: 'max' feature #2883 (t = 5.74)\n",
      "Rank 276: 'max' feature #4158 (t = 5.74)\n",
      "Rank 277: 'mean' feature #4149 (t = 5.73)\n",
      "Rank 278: 'mean' feature #4999 (t = 5.73)\n",
      "Rank 279: 'mean' feature #615 (t = 5.73)\n",
      "Rank 280: 'mean' feature #6535 (t = 5.72)\n",
      "Rank 281: 'mean' feature #2232 (t = 5.71)\n",
      "Rank 282: 'max' feature #5677 (t = 5.71)\n",
      "Rank 283: 'mean' feature #1399 (t = 5.71)\n",
      "Rank 284: 'mean' feature #7287 (t = 5.69)\n",
      "Rank 285: 'max' feature #7400 (t = 5.69)\n",
      "Rank 286: 'max' feature #3957 (t = 5.69)\n",
      "Rank 287: 'max' feature #6344 (t = 5.68)\n",
      "Rank 288: 'max' feature #7597 (t = 5.68)\n",
      "Rank 289: 'max' feature #5985 (t = 5.67)\n",
      "Rank 290: 'mean' feature #1700 (t = 5.66)\n",
      "Rank 291: 'max' feature #6132 (t = 5.66)\n",
      "Rank 292: 'mean' feature #1243 (t = 5.65)\n",
      "Rank 293: 'mean' feature #5974 (t = 5.64)\n",
      "Rank 294: 'max' feature #1554 (t = 5.64)\n",
      "Rank 295: 'max' feature #839 (t = 5.64)\n",
      "Rank 296: 'max' feature #6211 (t = 5.63)\n",
      "Rank 297: 'mean' feature #3917 (t = 5.63)\n",
      "Rank 298: 'max' feature #1641 (t = 5.63)\n",
      "Rank 299: 'mean' feature #269 (t = 5.63)\n",
      "Rank 300: 'mean' feature #8154 (t = 5.63)\n",
      "Rank 301: 'mean' feature #2505 (t = 5.62)\n",
      "Rank 302: 'mean' feature #1381 (t = 5.61)\n",
      "Rank 303: 'mean' feature #1139 (t = 5.61)\n",
      "Rank 304: 'max' feature #3084 (t = 5.61)\n",
      "Rank 305: 'max' feature #4305 (t = 5.60)\n",
      "Rank 306: 'mean' feature #5358 (t = 5.59)\n",
      "Rank 307: 'mean' feature #3130 (t = 5.58)\n",
      "Rank 308: 'max' feature #6422 (t = 5.58)\n",
      "Rank 309: 'max' feature #7978 (t = 5.57)\n",
      "Rank 310: 'mean' feature #6637 (t = 5.57)\n",
      "Rank 311: 'max' feature #3201 (t = 5.56)\n",
      "Rank 312: 'max' feature #5425 (t = 5.56)\n",
      "Rank 313: 'mean' feature #1414 (t = 5.56)\n",
      "Rank 314: 'max' feature #640 (t = 5.56)\n",
      "Rank 315: 'mean' feature #5370 (t = 5.55)\n",
      "Rank 316: 'max' feature #6052 (t = 5.55)\n",
      "Rank 317: 'mean' feature #107 (t = 5.54)\n",
      "Rank 318: 'mean' feature #3494 (t = 5.54)\n",
      "Rank 319: 'max' feature #5997 (t = 5.53)\n",
      "Rank 320: 'max' feature #5162 (t = 5.52)\n",
      "Rank 321: 'max' feature #7495 (t = 5.52)\n",
      "Rank 322: 'mean' feature #4558 (t = 5.52)\n",
      "Rank 323: 'max' feature #6237 (t = 5.51)\n",
      "Rank 324: 'mean' feature #6254 (t = 5.50)\n",
      "Rank 325: 'max' feature #3947 (t = 5.50)\n",
      "Rank 326: 'mean' feature #3746 (t = 5.50)\n",
      "Rank 327: 'mean' feature #3514 (t = 5.49)\n",
      "Rank 328: 'max' feature #3386 (t = 5.49)\n",
      "Rank 329: 'max' feature #7479 (t = 5.49)\n",
      "Rank 330: 'max' feature #7249 (t = 5.48)\n",
      "Rank 331: 'max' feature #2518 (t = 5.48)\n",
      "Rank 332: 'max' feature #6851 (t = 5.47)\n",
      "Rank 333: 'max' feature #3081 (t = 5.47)\n",
      "Rank 334: 'mean' feature #5982 (t = 5.47)\n",
      "Rank 335: 'mean' feature #2152 (t = 5.46)\n",
      "Rank 336: 'max' feature #926 (t = 5.46)\n",
      "Rank 337: 'max' feature #6957 (t = 5.46)\n",
      "Rank 338: 'max' feature #5704 (t = 5.45)\n",
      "Rank 339: 'max' feature #4222 (t = 5.45)\n",
      "Rank 340: 'mean' feature #3321 (t = 5.45)\n",
      "Rank 341: 'mean' feature #1657 (t = 5.45)\n",
      "Rank 342: 'mean' feature #7587 (t = 5.45)\n",
      "Rank 343: 'mean' feature #1158 (t = 5.43)\n",
      "Rank 344: 'max' feature #875 (t = 5.43)\n",
      "Rank 345: 'max' feature #972 (t = 5.42)\n",
      "Rank 346: 'mean' feature #2678 (t = 5.42)\n",
      "Rank 347: 'max' feature #872 (t = 5.42)\n",
      "Rank 348: 'max' feature #3003 (t = 5.41)\n",
      "Rank 349: 'mean' feature #5841 (t = 5.41)\n",
      "Rank 350: 'max' feature #5163 (t = 5.40)\n",
      "Rank 351: 'mean' feature #2414 (t = 5.40)\n",
      "Rank 352: 'max' feature #5308 (t = 5.40)\n",
      "Rank 353: 'mean' feature #1134 (t = 5.39)\n",
      "Rank 354: 'max' feature #4969 (t = 5.39)\n",
      "Rank 355: 'max' feature #6970 (t = 5.39)\n",
      "Rank 356: 'mean' feature #5516 (t = 5.39)\n",
      "Rank 357: 'mean' feature #997 (t = 5.39)\n",
      "Rank 358: 'mean' feature #2790 (t = 5.39)\n",
      "Rank 359: 'mean' feature #5264 (t = 5.38)\n",
      "Rank 360: 'max' feature #1798 (t = 5.38)\n",
      "Rank 361: 'mean' feature #6250 (t = 5.37)\n",
      "Rank 362: 'mean' feature #6806 (t = 5.37)\n",
      "Rank 363: 'mean' feature #1673 (t = 5.37)\n",
      "Rank 364: 'mean' feature #6289 (t = 5.37)\n",
      "Rank 365: 'mean' feature #6947 (t = 5.37)\n",
      "Rank 366: 'max' feature #2001 (t = 5.36)\n",
      "Rank 367: 'max' feature #1818 (t = 5.35)\n",
      "Rank 368: 'mean' feature #2457 (t = 5.35)\n",
      "Rank 369: 'max' feature #1097 (t = 5.35)\n",
      "Rank 370: 'mean' feature #4600 (t = 5.34)\n",
      "Rank 371: 'mean' feature #6610 (t = 5.34)\n",
      "Rank 372: 'mean' feature #3157 (t = 5.34)\n",
      "Rank 373: 'mean' feature #6053 (t = 5.34)\n",
      "Rank 374: 'max' feature #2577 (t = 5.34)\n",
      "Rank 375: 'mean' feature #1922 (t = 5.34)\n",
      "Rank 376: 'max' feature #3984 (t = 5.34)\n",
      "Rank 377: 'max' feature #5850 (t = 5.34)\n",
      "Rank 378: 'max' feature #2267 (t = 5.33)\n",
      "Rank 379: 'max' feature #7132 (t = 5.32)\n",
      "Rank 380: 'max' feature #5975 (t = 5.32)\n",
      "Rank 381: 'max' feature #3043 (t = 5.32)\n",
      "Rank 382: 'max' feature #3774 (t = 5.32)\n",
      "Rank 383: 'max' feature #2911 (t = 5.31)\n",
      "Rank 384: 'max' feature #244 (t = 5.31)\n",
      "Rank 385: 'mean' feature #2439 (t = 5.31)\n",
      "Rank 386: 'max' feature #4620 (t = 5.31)\n",
      "Rank 387: 'mean' feature #6211 (t = 5.31)\n",
      "Rank 388: 'mean' feature #511 (t = 5.31)\n",
      "Rank 389: 'mean' feature #4983 (t = 5.31)\n",
      "Rank 390: 'mean' feature #1374 (t = 5.30)\n",
      "Rank 391: 'max' feature #5196 (t = 5.30)\n",
      "Rank 392: 'max' feature #6953 (t = 5.30)\n",
      "Rank 393: 'max' feature #936 (t = 5.30)\n",
      "Rank 394: 'max' feature #5699 (t = 5.30)\n",
      "Rank 395: 'mean' feature #2182 (t = 5.30)\n",
      "Rank 396: 'max' feature #1486 (t = 5.30)\n",
      "Rank 397: 'max' feature #4707 (t = 5.30)\n",
      "Rank 398: 'mean' feature #5041 (t = 5.29)\n",
      "Rank 399: 'max' feature #5892 (t = 5.28)\n",
      "Rank 400: 'mean' feature #4604 (t = 5.28)\n",
      "Rank 401: 'mean' feature #1406 (t = 5.28)\n",
      "Rank 402: 'max' feature #5341 (t = 5.28)\n",
      "Rank 403: 'mean' feature #8181 (t = 5.28)\n",
      "Rank 404: 'mean' feature #1871 (t = 5.28)\n",
      "Rank 405: 'max' feature #1512 (t = 5.28)\n",
      "Rank 406: 'max' feature #7444 (t = 5.28)\n",
      "Rank 407: 'mean' feature #3078 (t = 5.27)\n",
      "Rank 408: 'mean' feature #4314 (t = 5.27)\n",
      "Rank 409: 'max' feature #4152 (t = 5.27)\n",
      "Rank 410: 'mean' feature #6027 (t = 5.26)\n",
      "Rank 411: 'mean' feature #6290 (t = 5.26)\n",
      "Rank 412: 'max' feature #7042 (t = 5.25)\n",
      "Rank 413: 'mean' feature #2895 (t = 5.25)\n",
      "Rank 414: 'mean' feature #4302 (t = 5.25)\n",
      "Rank 415: 'max' feature #1525 (t = 5.24)\n",
      "Rank 416: 'mean' feature #8171 (t = 5.23)\n",
      "Rank 417: 'mean' feature #2072 (t = 5.23)\n",
      "Rank 418: 'mean' feature #5446 (t = 5.23)\n",
      "Rank 419: 'mean' feature #5874 (t = 5.23)\n",
      "Rank 420: 'max' feature #6396 (t = 5.23)\n",
      "Rank 421: 'max' feature #231 (t = 5.23)\n",
      "Rank 422: 'max' feature #644 (t = 5.23)\n",
      "Rank 423: 'mean' feature #110 (t = 5.22)\n",
      "Rank 424: 'max' feature #6575 (t = 5.22)\n",
      "Rank 425: 'max' feature #7461 (t = 5.22)\n",
      "Rank 426: 'max' feature #5840 (t = 5.22)\n",
      "Rank 427: 'mean' feature #2399 (t = 5.22)\n",
      "Rank 428: 'max' feature #5319 (t = 5.21)\n",
      "Rank 429: 'max' feature #5690 (t = 5.21)\n",
      "Rank 430: 'max' feature #5348 (t = 5.20)\n",
      "Rank 431: 'mean' feature #6368 (t = 5.20)\n",
      "Rank 432: 'mean' feature #2409 (t = 5.20)\n",
      "Rank 433: 'mean' feature #4607 (t = 5.20)\n",
      "Rank 434: 'mean' feature #5675 (t = 5.20)\n",
      "Rank 435: 'mean' feature #2021 (t = 5.19)\n",
      "Rank 436: 'mean' feature #4797 (t = 5.19)\n",
      "Rank 437: 'max' feature #2445 (t = 5.19)\n",
      "Rank 438: 'mean' feature #5937 (t = 5.19)\n",
      "Rank 439: 'mean' feature #420 (t = 5.19)\n",
      "Rank 440: 'mean' feature #6410 (t = 5.19)\n",
      "Rank 441: 'mean' feature #4992 (t = 5.19)\n",
      "Rank 442: 'mean' feature #1269 (t = 5.19)\n",
      "Rank 443: 'mean' feature #939 (t = 5.18)\n",
      "Rank 444: 'mean' feature #7517 (t = 5.18)\n",
      "Rank 445: 'mean' feature #8030 (t = 5.18)\n",
      "Rank 446: 'max' feature #6494 (t = 5.18)\n",
      "Rank 447: 'mean' feature #4019 (t = 5.18)\n",
      "Rank 448: 'mean' feature #4460 (t = 5.18)\n",
      "Rank 449: 'mean' feature #4740 (t = 5.17)\n",
      "Rank 450: 'mean' feature #1773 (t = 5.17)\n",
      "Rank 451: 'mean' feature #4531 (t = 5.17)\n",
      "Rank 452: 'mean' feature #4543 (t = 5.17)\n",
      "Rank 453: 'max' feature #5401 (t = 5.16)\n",
      "Rank 454: 'mean' feature #5401 (t = 5.16)\n",
      "Rank 455: 'mean' feature #2522 (t = 5.16)\n",
      "Rank 456: 'mean' feature #7767 (t = 5.16)\n",
      "Rank 457: 'mean' feature #2294 (t = 5.16)\n",
      "Rank 458: 'mean' feature #1007 (t = 5.16)\n",
      "Rank 459: 'mean' feature #4596 (t = 5.16)\n",
      "Rank 460: 'max' feature #2162 (t = 5.15)\n",
      "Rank 461: 'mean' feature #8121 (t = 5.15)\n",
      "Rank 462: 'max' feature #3995 (t = 5.15)\n",
      "Rank 463: 'mean' feature #3962 (t = 5.14)\n",
      "Rank 464: 'max' feature #5351 (t = 5.14)\n",
      "Rank 465: 'max' feature #5572 (t = 5.14)\n",
      "Rank 466: 'mean' feature #3533 (t = 5.13)\n",
      "Rank 467: 'mean' feature #4329 (t = 5.13)\n",
      "Rank 468: 'max' feature #4850 (t = 5.13)\n",
      "Rank 469: 'mean' feature #3342 (t = 5.12)\n",
      "Rank 470: 'mean' feature #2712 (t = 5.12)\n",
      "Rank 471: 'max' feature #3222 (t = 5.12)\n",
      "Rank 472: 'max' feature #4437 (t = 5.11)\n",
      "Rank 473: 'mean' feature #2305 (t = 5.11)\n",
      "Rank 474: 'mean' feature #4045 (t = 5.11)\n",
      "Rank 475: 'mean' feature #7688 (t = 5.11)\n",
      "Rank 476: 'max' feature #606 (t = 5.10)\n",
      "Rank 477: 'mean' feature #3590 (t = 5.09)\n",
      "Rank 478: 'max' feature #2021 (t = 5.08)\n",
      "Rank 479: 'max' feature #4452 (t = 5.08)\n",
      "Rank 480: 'max' feature #5797 (t = 5.08)\n",
      "Rank 481: 'max' feature #1346 (t = 5.08)\n",
      "Rank 482: 'mean' feature #5027 (t = 5.06)\n",
      "Rank 483: 'mean' feature #3059 (t = 5.06)\n",
      "Rank 484: 'mean' feature #1256 (t = 5.06)\n",
      "Rank 485: 'max' feature #8028 (t = 5.06)\n",
      "Rank 486: 'max' feature #5409 (t = 5.05)\n",
      "Rank 487: 'mean' feature #766 (t = 5.05)\n",
      "Rank 488: 'max' feature #2373 (t = 5.04)\n",
      "Rank 489: 'max' feature #975 (t = 5.04)\n",
      "Rank 490: 'max' feature #1144 (t = 5.04)\n",
      "Rank 491: 'mean' feature #261 (t = 5.04)\n",
      "Rank 492: 'mean' feature #1720 (t = 5.03)\n",
      "Rank 493: 'mean' feature #3025 (t = 5.02)\n",
      "Rank 494: 'max' feature #3466 (t = 5.02)\n",
      "Rank 495: 'max' feature #5334 (t = 5.01)\n",
      "Rank 496: 'max' feature #1150 (t = 5.01)\n",
      "Rank 497: 'max' feature #5969 (t = 5.01)\n",
      "Rank 498: 'max' feature #4128 (t = 5.00)\n",
      "Rank 499: 'max' feature #4307 (t = 5.00)\n",
      "Rank 500: 'max' feature #3393 (t = 4.99)\n",
      "Rank 501: 'mean' feature #5230 (t = 4.99)\n",
      "Rank 502: 'mean' feature #3471 (t = 4.99)\n",
      "Rank 503: 'max' feature #4432 (t = 4.99)\n",
      "Rank 504: 'max' feature #2884 (t = 4.99)\n",
      "Rank 505: 'mean' feature #1852 (t = 4.98)\n",
      "Rank 506: 'max' feature #3205 (t = 4.98)\n",
      "Rank 507: 'mean' feature #4760 (t = 4.98)\n",
      "Rank 508: 'mean' feature #7797 (t = 4.98)\n",
      "Rank 509: 'mean' feature #255 (t = 4.98)\n",
      "Rank 510: 'mean' feature #1645 (t = 4.98)\n",
      "Rank 511: 'max' feature #1310 (t = 4.98)\n",
      "Rank 512: 'max' feature #151 (t = 4.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12273/2266787272.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  t_stats = np.abs((X_pos.mean(0) - X_neg.mean(0)) /\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# meta = meta.assign(label=lambda df: df.SUESCORE.map(lambda s: 1 if s>=0.5 else (0 if s<=-0.5 else np.nan)))\n",
    "# mask = meta.label.notna().values\n",
    "# Xc, y = Xc_aligned[mask], meta.loc[mask, \"label\"].astype(int).values\n",
    "\n",
    "D2 = Xc.shape[1]\n",
    "D = D2 // 2\n",
    "X_pos, X_neg = Xc[y==1], Xc[y==0]\n",
    "t_stats = np.abs((X_pos.mean(0) - X_neg.mean(0)) /\n",
    "                 np.sqrt(X_pos.var(0)/len(X_pos) + X_neg.var(0)/len(X_neg)))\n",
    "\n",
    "ranked_idx = np.argsort(-t_stats)\n",
    "\n",
    "\n",
    "for rank, idx in enumerate(ranked_idx[:512], start=1):\n",
    "    # print(idx)\n",
    "    part = \"mean\" if idx < D else \"max\"\n",
    "    # print(idx)\n",
    "    # print(D)\n",
    "    feat_id = idx if idx < D else idx-D\n",
    "    t_val   = t_stats[idx]\n",
    "    print(f\"Rank {rank:2d}: {part!r} feature #{feat_id} (t = {t_val:.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cd0a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idx = ranked_idx[:2000]\n",
    "X_test = X_test_all_feat[:, top_idx]\n",
    "X_val = X_val_all_feat[:, top_idx]\n",
    "X_top = Xc[:, top_idx]      \n",
    "\n",
    "X_train = X_top\n",
    "y_train = y\n",
    "\n",
    "\n",
    "# X_test = X_test_all_feat[:, :]\n",
    "# X_val = X_val_all_feat[:, :]\n",
    "# X_top = Xc[:, :]      \n",
    "\n",
    "# X_train = X_top\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7db7e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4526, 16384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97c93d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airlay88/surprise_sae/venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       504\n",
      "           1       0.56      0.57      0.56       504\n",
      "\n",
      "    accuracy                           0.56      1008\n",
      "   macro avg       0.56      0.56      0.56      1008\n",
      "weighted avg       0.56      0.56      0.56      1008\n",
      "\n",
      "ROC AUC: 0.5706294091710759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airlay88/surprise_sae/venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5) Train with L1 logistic regression & balanced class weights\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        # class_weight=\"balanced\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6) Evaluate\n",
    "y_pred   = clf.predict(X_test)\n",
    "y_probs  = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_probs))\n",
    "\n",
    "# 7) Inspect which of your top-1000 actually got nonzero weights\n",
    "lr = clf.named_steps[\"logisticregression\"]\n",
    "coefs = lr.coef_.ravel()\n",
    "nz    = np.where(coefs != 0)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dfaa751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best C (inverse reg. strength): 0.01\n",
      "CV ROC AUC: 0.669798985855862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.58      0.58       504\n",
      "           1       0.57      0.56      0.57       504\n",
      "\n",
      "    accuracy                           0.57      1008\n",
      "   macro avg       0.57      0.57      0.57      1008\n",
      "weighted avg       0.57      0.57      0.57      1008\n",
      "\n",
      "Test ROC AUC: 0.5867150100781053\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        # solver=\"saga\",    \n",
    "        solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=7000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e77399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best C (inverse reg. strength): 0.01\n",
      "CV ROC AUC: 0.6844055404425369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60       504\n",
      "           1       0.59      0.58      0.59       504\n",
      "\n",
      "    accuracy                           0.59      1008\n",
      "   macro avg       0.59      0.59      0.59      1008\n",
      "weighted avg       0.59      0.59      0.59      1008\n",
      "\n",
      "Test ROC AUC: 0.5942263479465861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=7000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2516dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        # solver=\"saga\",    \n",
    "        solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=7000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01459b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train loss = 0.4883\n",
      "Epoch  2: train loss = 0.4326\n",
      "Epoch  3: train loss = 0.4096\n",
      "Epoch  4: train loss = 0.3909\n",
      "Epoch  5: train loss = 0.3716\n",
      "Epoch  6: train loss = 0.3501\n",
      "Epoch  7: train loss = 0.3382\n",
      "Epoch  8: train loss = 0.3268\n",
      "Epoch  9: train loss = 0.3035\n",
      "Epoch 10: train loss = 0.2902\n",
      "Epoch 11: train loss = 0.2670\n",
      "Epoch 12: train loss = 0.2486\n",
      "Epoch 13: train loss = 0.2466\n",
      "Epoch 14: train loss = 0.2280\n",
      "Epoch 15: train loss = 0.2109\n",
      "Epoch 16: train loss = 0.1982\n",
      "Epoch 17: train loss = 0.1960\n",
      "Epoch 18: train loss = 0.1869\n",
      "Epoch 19: train loss = 0.1776\n",
      "Epoch 20: train loss = 0.1740\n",
      "Epoch 21: train loss = 0.1530\n",
      "Epoch 22: train loss = 0.1395\n",
      "Epoch 23: train loss = 0.1286\n",
      "Epoch 24: train loss = 0.1288\n",
      "Epoch 25: train loss = 0.1171\n",
      "Epoch 26: train loss = 0.1268\n",
      "Epoch 27: train loss = 0.1193\n",
      "Epoch 28: train loss = 0.1028\n",
      "Epoch 29: train loss = 0.1005\n",
      "Epoch 30: train loss = 0.1140\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.29      0.28       222\n",
      "           1       0.80      0.78      0.79       793\n",
      "\n",
      "    accuracy                           0.67      1015\n",
      "   macro avg       0.53      0.54      0.53      1015\n",
      "weighted avg       0.68      0.67      0.68      1015\n",
      "\n",
      "Test ROC AUC: 0.5737392499687581\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_t = torch.from_numpy(X_train).float().to(device)\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\n",
    "X_test_t  = torch.from_numpy(X_test).float().to(device)\n",
    "y_test_t  = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_top.shape[1]).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 4) Training loop\n",
    "n_epochs = 30\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch:2d}: train loss = {avg_loss:.4f}\")\n",
    "\n",
    "# 5) Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1333966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.79091+0.01652\ttest-auc:0.58206+0.02945\n",
      "[50]\ttrain-auc:1.00000+0.00000\ttest-auc:0.69692+0.02358\n",
      "[100]\ttrain-auc:1.00000+0.00000\ttest-auc:0.70410+0.02146\n",
      "[150]\ttrain-auc:1.00000+0.00000\ttest-auc:0.71121+0.01732\n",
      "[200]\ttrain-auc:1.00000+0.00000\ttest-auc:0.71543+0.01728\n",
      "[206]\ttrain-auc:1.00000+0.00000\ttest-auc:0.71560+0.01736\n",
      "Optimal boosting rounds: 188\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.18      0.22       222\n",
      "           1       0.79      0.89      0.84       793\n",
      "\n",
      "    accuracy                           0.73      1015\n",
      "   macro avg       0.55      0.53      0.53      1015\n",
      "weighted avg       0.69      0.73      0.70      1015\n",
      "\n",
      "Test ROC AUC: 0.5895618190700158\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "\n",
    "scale_pos_weight = float((y_train == 0).sum()) / (y_train == 1).sum()\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\":        \"binary:logistic\",\n",
    "    \"eval_metric\":      \"auc\",\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"tree_method\":      \"hist\",       \n",
    "    \"grow_policy\":      \"lossguide\",  \n",
    "    \"max_depth\":        6,\n",
    "    \"learning_rate\":    0.1,\n",
    "    \"subsample\":        0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\":     42,\n",
    "    \"verbosity\":        1\n",
    "}\n",
    "\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=20,\n",
    "    metrics=\"auc\",\n",
    "    seed=42,\n",
    "    as_pandas=True,\n",
    "    verbose_eval=50\n",
    ")\n",
    "best_rounds = len(cv_results)\n",
    "print(f\"Optimal boosting rounds: {best_rounds}\")\n",
    "\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=best_rounds\n",
    ")\n",
    "\n",
    "\n",
    "y_prob = bst.predict(dtest)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf7986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train_loss = 0.6948, val_loss = 0.6883\n",
      "Epoch  2: train_loss = 0.6825, val_loss = 0.6822\n",
      "Epoch  3: train_loss = 0.6719, val_loss = 0.6773\n",
      "Epoch  4: train_loss = 0.6742, val_loss = 0.6804\n",
      "Epoch  5: train_loss = 0.6629, val_loss = 0.6770\n",
      "Epoch  6: train_loss = 0.6625, val_loss = 0.6746\n",
      "Epoch  7: train_loss = 0.6618, val_loss = 0.6742\n",
      "Epoch  8: train_loss = 0.6602, val_loss = 0.6722\n",
      "Epoch  9: train_loss = 0.6564, val_loss = 0.6716\n",
      "Epoch 10: train_loss = 0.6513, val_loss = 0.6738\n",
      "Epoch 11: train_loss = 0.6584, val_loss = 0.6708\n",
      "Epoch 12: train_loss = 0.6521, val_loss = 0.6695\n",
      "Epoch 13: train_loss = 0.6503, val_loss = 0.6709\n",
      "Epoch 14: train_loss = 0.6491, val_loss = 0.6687\n",
      "Epoch 15: train_loss = 0.6484, val_loss = 0.6727\n",
      "Epoch 16: train_loss = 0.6481, val_loss = 0.6679\n",
      "Epoch 17: train_loss = 0.6476, val_loss = 0.6667\n",
      "Epoch 18: train_loss = 0.6468, val_loss = 0.6673\n",
      "Epoch 19: train_loss = 0.6438, val_loss = 0.6681\n",
      "Epoch 20: train_loss = 0.6462, val_loss = 0.6681\n",
      "Epoch 21: train_loss = 0.6420, val_loss = 0.6666\n",
      "Epoch 22: train_loss = 0.6442, val_loss = 0.6687\n",
      "Epoch 23: train_loss = 0.6393, val_loss = 0.6664\n",
      "Epoch 24: train_loss = 0.6376, val_loss = 0.6665\n",
      "Epoch 25: train_loss = 0.6337, val_loss = 0.6674\n",
      "Epoch 26: train_loss = 0.6353, val_loss = 0.6669\n",
      "Epoch 27: train_loss = 0.6396, val_loss = 0.6676\n",
      "Epoch 28: train_loss = 0.6340, val_loss = 0.6667\n",
      "Epoch 29: train_loss = 0.6361, val_loss = 0.6675\n",
      "Epoch 30: train_loss = 0.6345, val_loss = 0.6654\n",
      "Epoch 31: train_loss = 0.6301, val_loss = 0.6663\n",
      "Epoch 32: train_loss = 0.6282, val_loss = 0.6645\n",
      "Epoch 33: train_loss = 0.6294, val_loss = 0.6646\n",
      "Epoch 34: train_loss = 0.6288, val_loss = 0.6690\n",
      "Epoch 35: train_loss = 0.6246, val_loss = 0.6667\n",
      "Epoch 36: train_loss = 0.6301, val_loss = 0.6637\n",
      "Epoch 37: train_loss = 0.6262, val_loss = 0.6645\n",
      "Epoch 38: train_loss = 0.6284, val_loss = 0.6645\n",
      "Epoch 39: train_loss = 0.6278, val_loss = 0.6646\n",
      "Epoch 40: train_loss = 0.6281, val_loss = 0.6660\n",
      "Epoch 41: train_loss = 0.6223, val_loss = 0.6661\n",
      "Epoch 42: train_loss = 0.6209, val_loss = 0.6660\n",
      "Epoch 43: train_loss = 0.6194, val_loss = 0.6651\n",
      "Epoch 44: train_loss = 0.6253, val_loss = 0.6655\n",
      "Epoch 45: train_loss = 0.6252, val_loss = 0.6651\n",
      "Epoch 46: train_loss = 0.6225, val_loss = 0.6648\n",
      "Epoch 47: train_loss = 0.6152, val_loss = 0.6662\n",
      "Epoch 48: train_loss = 0.6162, val_loss = 0.6636\n",
      "Epoch 49: train_loss = 0.6152, val_loss = 0.6636\n",
      "Epoch 50: train_loss = 0.6148, val_loss = 0.6646\n",
      "Epoch 51: train_loss = 0.6143, val_loss = 0.6674\n",
      "Epoch 52: train_loss = 0.6204, val_loss = 0.6678\n",
      "Epoch 53: train_loss = 0.6164, val_loss = 0.6647\n",
      "Epoch 54: train_loss = 0.6151, val_loss = 0.6659\n",
      "Epoch 55: train_loss = 0.6167, val_loss = 0.6651\n",
      "Epoch 56: train_loss = 0.6104, val_loss = 0.6653\n",
      "Epoch 57: train_loss = 0.6139, val_loss = 0.6679\n",
      "Epoch 58: train_loss = 0.6142, val_loss = 0.6638\n",
      "Epoch 59: train_loss = 0.6064, val_loss = 0.6655\n",
      "Epoch 60: train_loss = 0.6123, val_loss = 0.6636\n",
      "Epoch 61: train_loss = 0.6045, val_loss = 0.6650\n",
      "Epoch 62: train_loss = 0.6061, val_loss = 0.6652\n",
      "Epoch 63: train_loss = 0.6081, val_loss = 0.6644\n",
      "Epoch 64: train_loss = 0.6070, val_loss = 0.6642\n",
      "Epoch 65: train_loss = 0.6042, val_loss = 0.6634\n",
      "Epoch 66: train_loss = 0.6111, val_loss = 0.6647\n",
      "Epoch 67: train_loss = 0.6045, val_loss = 0.6692\n",
      "Epoch 68: train_loss = 0.6026, val_loss = 0.6637\n",
      "Epoch 69: train_loss = 0.6062, val_loss = 0.6642\n",
      "Epoch 70: train_loss = 0.6051, val_loss = 0.6657\n",
      "Epoch 71: train_loss = 0.6053, val_loss = 0.6652\n",
      "Epoch 72: train_loss = 0.6060, val_loss = 0.6634\n",
      "Epoch 73: train_loss = 0.6046, val_loss = 0.6625\n",
      "Epoch 74: train_loss = 0.5997, val_loss = 0.6649\n",
      "Epoch 75: train_loss = 0.5968, val_loss = 0.6623\n",
      "Epoch 76: train_loss = 0.5962, val_loss = 0.6651\n",
      "Epoch 77: train_loss = 0.6013, val_loss = 0.6630\n",
      "Epoch 78: train_loss = 0.6016, val_loss = 0.6655\n",
      "Epoch 79: train_loss = 0.5949, val_loss = 0.6644\n",
      "Epoch 80: train_loss = 0.5916, val_loss = 0.6659\n",
      "Epoch 81: train_loss = 0.5963, val_loss = 0.6614\n",
      "Epoch 82: train_loss = 0.5931, val_loss = 0.6657\n",
      "Epoch 83: train_loss = 0.5865, val_loss = 0.6641\n",
      "Epoch 84: train_loss = 0.5896, val_loss = 0.6656\n",
      "Epoch 85: train_loss = 0.5985, val_loss = 0.6645\n",
      "Epoch 86: train_loss = 0.5910, val_loss = 0.6648\n",
      "Epoch 87: train_loss = 0.5858, val_loss = 0.6656\n",
      "Epoch 88: train_loss = 0.5859, val_loss = 0.6653\n",
      "Epoch 89: train_loss = 0.5838, val_loss = 0.6654\n",
      "Epoch 90: train_loss = 0.5853, val_loss = 0.6658\n",
      "Epoch 91: train_loss = 0.5920, val_loss = 0.6656\n",
      "Epoch 92: train_loss = 0.5877, val_loss = 0.6640\n",
      "Epoch 93: train_loss = 0.5793, val_loss = 0.6663\n",
      "Epoch 94: train_loss = 0.5856, val_loss = 0.6663\n",
      "Epoch 95: train_loss = 0.5816, val_loss = 0.6667\n",
      "Epoch 96: train_loss = 0.5807, val_loss = 0.6663\n",
      "Epoch 97: train_loss = 0.5849, val_loss = 0.6611\n",
      "Epoch 98: train_loss = 0.5840, val_loss = 0.6643\n",
      "Epoch 99: train_loss = 0.5801, val_loss = 0.6623\n",
      "Epoch 100: train_loss = 0.5794, val_loss = 0.6650\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.59       504\n",
      "           1       0.60      0.68      0.64       504\n",
      "\n",
      "    accuracy                           0.62      1008\n",
      "   macro avg       0.62      0.62      0.61      1008\n",
      "weighted avg       0.62      0.62      0.61      1008\n",
      "\n",
      "Test ROC AUC: 0.6510810342655582\n"
     ]
    }
   ],
   "source": [
    "# 3) Convert to tensors and move to device\n",
    "def to_tensor(x, y):\n",
    "    xt = torch.from_numpy(x).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return xt, yt\n",
    "\n",
    "X_tr_t, y_tr_t = to_tensor(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensor(X_val, y_val)\n",
    "X_test_t, y_test_t = to_tensor(X_test, y_test)\n",
    "\n",
    "# 4) DataLoaders\n",
    "batch_size = 8\n",
    "train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 5) Model definition\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 6) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 7) Training with validation\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_tr_loss += loss.item() * xb.size(0)\n",
    "    avg_tr_loss = total_tr_loss / len(train_dl.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_val_loss += loss.item() * xb.size(0)\n",
    "    avg_val_loss = total_val_loss / len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}: train_loss = {avg_tr_loss:.4f}, val_loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "# 8) Final evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "25c03ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train_loss = 0.2867, val_loss = 0.2971\n",
      "Epoch  2: train_loss = 0.2781, val_loss = 0.2948\n",
      "Epoch  3: train_loss = 0.2746, val_loss = 0.2948\n",
      "Epoch  4: train_loss = 0.2709, val_loss = 0.2941\n",
      "Epoch  5: train_loss = 0.2699, val_loss = 0.2941\n",
      "Epoch  6: train_loss = 0.2676, val_loss = 0.2926\n",
      "Epoch  7: train_loss = 0.2661, val_loss = 0.2921\n",
      "Epoch  8: train_loss = 0.2664, val_loss = 0.2928\n",
      "Epoch  9: train_loss = 0.2647, val_loss = 0.2922\n",
      "Epoch 10: train_loss = 0.2639, val_loss = 0.2918\n",
      "Epoch 11: train_loss = 0.2616, val_loss = 0.2929\n",
      "Epoch 12: train_loss = 0.2608, val_loss = 0.2946\n",
      "Epoch 13: train_loss = 0.2593, val_loss = 0.2928\n",
      "Epoch 14: train_loss = 0.2595, val_loss = 0.2940\n",
      "Epoch 15: train_loss = 0.2578, val_loss = 0.2954\n",
      "Epoch 16: train_loss = 0.2572, val_loss = 0.2954\n",
      "Epoch 17: train_loss = 0.2563, val_loss = 0.2979\n",
      "Epoch 18: train_loss = 0.2554, val_loss = 0.2972\n",
      "Epoch 19: train_loss = 0.2561, val_loss = 0.2954\n",
      "Epoch 20: train_loss = 0.2542, val_loss = 0.2954\n",
      "Epoch 21: train_loss = 0.2540, val_loss = 0.2950\n",
      "Epoch 22: train_loss = 0.2528, val_loss = 0.2988\n",
      "Epoch 23: train_loss = 0.2521, val_loss = 0.3003\n",
      "Epoch 24: train_loss = 0.2513, val_loss = 0.2990\n",
      "Epoch 25: train_loss = 0.2515, val_loss = 0.2991\n",
      "Epoch 26: train_loss = 0.2498, val_loss = 0.2995\n",
      "Epoch 27: train_loss = 0.2497, val_loss = 0.2990\n",
      "Epoch 28: train_loss = 0.2494, val_loss = 0.2991\n",
      "Epoch 29: train_loss = 0.2494, val_loss = 0.2982\n",
      "Epoch 30: train_loss = 0.2488, val_loss = 0.3013\n",
      "Epoch 31: train_loss = 0.2478, val_loss = 0.3000\n",
      "Epoch 32: train_loss = 0.2475, val_loss = 0.3004\n",
      "Epoch 33: train_loss = 0.2466, val_loss = 0.3002\n",
      "Epoch 34: train_loss = 0.2454, val_loss = 0.3011\n",
      "Epoch 35: train_loss = 0.2467, val_loss = 0.3005\n",
      "Epoch 36: train_loss = 0.2460, val_loss = 0.3037\n",
      "Epoch 37: train_loss = 0.2464, val_loss = 0.2974\n",
      "Epoch 38: train_loss = 0.2462, val_loss = 0.2992\n",
      "Epoch 39: train_loss = 0.2449, val_loss = 0.2986\n",
      "Epoch 40: train_loss = 0.2440, val_loss = 0.3027\n",
      "Epoch 41: train_loss = 0.2453, val_loss = 0.3044\n",
      "Epoch 42: train_loss = 0.2440, val_loss = 0.3047\n",
      "Epoch 43: train_loss = 0.2433, val_loss = 0.3067\n",
      "Epoch 44: train_loss = 0.2424, val_loss = 0.3040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, yb)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m total_tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/surprise_sae/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/surprise_sae/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/surprise_sae/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:94\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_grads\u001b[39m(\n\u001b[1;32m     89\u001b[0m     outputs: Union[Sequence[torch\u001b[38;5;241m.\u001b[39mTensor], Sequence[graph\u001b[38;5;241m.\u001b[39mGradientEdge]],\n\u001b[1;32m     90\u001b[0m     grads: Sequence[_OptionalTensor],\n\u001b[1;32m     91\u001b[0m     is_grads_batched: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_OptionalTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     93\u001b[0m     new_grads: List[_OptionalTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m out, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     95\u001b[0m         out \u001b[38;5;241m=\u001b[39m cast(Union[torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge], out)\n\u001b[1;32m     96\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3) Convert to tensors and move to device\n",
    "def to_tensor(x, y):\n",
    "    xt = torch.from_numpy(x).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return xt, yt\n",
    "\n",
    "X_tr_t, y_tr_t = to_tensor(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensor(X_val, y_val)\n",
    "X_test_t, y_test_t = to_tensor(X_test, y_test)\n",
    "\n",
    "# 4) DataLoaders\n",
    "batch_size = 8\n",
    "train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "pos = np.sum(y_train == 1)\n",
    "neg = np.sum(y_train == 0)\n",
    "pos_weight = torch.tensor(neg / pos, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# 5) Model definition\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 6) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "# 7) Training with validation\n",
    "n_epochs = 120\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_tr_loss += loss.item() * xb.size(0)\n",
    "    avg_tr_loss = total_tr_loss / len(train_dl.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_val_loss += loss.item() * xb.size(0)\n",
    "    avg_val_loss = total_val_loss / len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}: train_loss = {avg_tr_loss:.4f}, val_loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "# 8) Final evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6aa2acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train Loss: 0.7216  Val Loss: 0.6941\n",
      "Epoch  2  Train Loss: 0.7179  Val Loss: 0.6926\n",
      "Epoch  3  Train Loss: 0.7176  Val Loss: 0.6924\n",
      "Epoch  4  Train Loss: 0.7203  Val Loss: 0.6922\n",
      "Epoch  5  Train Loss: 0.7219  Val Loss: 0.6922\n",
      "Epoch  6  Train Loss: 0.7187  Val Loss: 0.6913\n",
      "Epoch  7  Train Loss: 0.7173  Val Loss: 0.6910\n",
      "Epoch  8  Train Loss: 0.7154  Val Loss: 0.6907\n",
      "Epoch  9  Train Loss: 0.7208  Val Loss: 0.6906\n",
      "Epoch 10  Train Loss: 0.7191  Val Loss: 0.6903\n",
      "Epoch 11  Train Loss: 0.7139  Val Loss: 0.6901\n",
      "Epoch 12  Train Loss: 0.7100  Val Loss: 0.6900\n",
      "Epoch 13  Train Loss: 0.7194  Val Loss: 0.6886\n",
      "Epoch 14  Train Loss: 0.7128  Val Loss: 0.6889\n",
      "Epoch 15  Train Loss: 0.7158  Val Loss: 0.6890\n",
      "Epoch 16  Train Loss: 0.7144  Val Loss: 0.6886\n",
      "Epoch 17  Train Loss: 0.7136  Val Loss: 0.6879\n",
      "Epoch 18  Train Loss: 0.7173  Val Loss: 0.6879\n",
      "Epoch 19  Train Loss: 0.7124  Val Loss: 0.6867\n",
      "Epoch 20  Train Loss: 0.7065  Val Loss: 0.6871\n",
      "Epoch 21  Train Loss: 0.7079  Val Loss: 0.6880\n",
      "Epoch 22  Train Loss: 0.7066  Val Loss: 0.6870\n",
      "Epoch 23  Train Loss: 0.7155  Val Loss: 0.6864\n",
      "Epoch 24  Train Loss: 0.7063  Val Loss: 0.6860\n",
      "Epoch 25  Train Loss: 0.7068  Val Loss: 0.6855\n",
      "Epoch 26  Train Loss: 0.7103  Val Loss: 0.6863\n",
      "Epoch 27  Train Loss: 0.7076  Val Loss: 0.6858\n",
      "Epoch 28  Train Loss: 0.7078  Val Loss: 0.6849\n",
      "Epoch 29  Train Loss: 0.7135  Val Loss: 0.6856\n",
      "Epoch 30  Train Loss: 0.7084  Val Loss: 0.6853\n",
      "Epoch 31  Train Loss: 0.7085  Val Loss: 0.6852\n",
      "Epoch 32  Train Loss: 0.7060  Val Loss: 0.6849\n",
      "Epoch 33  Train Loss: 0.7066  Val Loss: 0.6844\n",
      "Epoch 34  Train Loss: 0.7126  Val Loss: 0.6835\n",
      "Epoch 35  Train Loss: 0.7052  Val Loss: 0.6842\n",
      "Epoch 36  Train Loss: 0.7045  Val Loss: 0.6845\n",
      "Epoch 37  Train Loss: 0.7021  Val Loss: 0.6833\n",
      "Epoch 38  Train Loss: 0.7080  Val Loss: 0.6836\n",
      "Epoch 39  Train Loss: 0.7044  Val Loss: 0.6838\n",
      "Epoch 40  Train Loss: 0.7000  Val Loss: 0.6828\n",
      "Epoch 41  Train Loss: 0.7068  Val Loss: 0.6839\n",
      "Epoch 42  Train Loss: 0.7070  Val Loss: 0.6830\n",
      "Epoch 43  Train Loss: 0.7004  Val Loss: 0.6829\n",
      "Epoch 44  Train Loss: 0.7006  Val Loss: 0.6835\n",
      "Epoch 45  Train Loss: 0.7039  Val Loss: 0.6829\n",
      "Epoch 46  Train Loss: 0.6945  Val Loss: 0.6829\n",
      "Epoch 47  Train Loss: 0.7031  Val Loss: 0.6823\n",
      "Epoch 48  Train Loss: 0.7084  Val Loss: 0.6829\n",
      "Epoch 49  Train Loss: 0.7037  Val Loss: 0.6832\n",
      "Epoch 50  Train Loss: 0.7032  Val Loss: 0.6824\n",
      "Epoch 51  Train Loss: 0.6991  Val Loss: 0.6830\n",
      "Epoch 52  Train Loss: 0.7023  Val Loss: 0.6816\n",
      "Epoch 53  Train Loss: 0.7014  Val Loss: 0.6822\n",
      "Epoch 54  Train Loss: 0.7011  Val Loss: 0.6814\n",
      "Epoch 55  Train Loss: 0.6849  Val Loss: 0.6819\n",
      "Epoch 56  Train Loss: 0.6972  Val Loss: 0.6809\n",
      "Epoch 57  Train Loss: 0.6930  Val Loss: 0.6817\n",
      "Epoch 58  Train Loss: 0.6932  Val Loss: 0.6811\n",
      "Epoch 59  Train Loss: 0.7020  Val Loss: 0.6813\n",
      "Epoch 60  Train Loss: 0.6982  Val Loss: 0.6817\n",
      "Epoch 61  Train Loss: 0.6960  Val Loss: 0.6820\n",
      "Epoch 62  Train Loss: 0.7034  Val Loss: 0.6811\n",
      "Epoch 63  Train Loss: 0.6972  Val Loss: 0.6812\n",
      "Epoch 64  Train Loss: 0.6964  Val Loss: 0.6806\n",
      "Epoch 65  Train Loss: 0.6947  Val Loss: 0.6802\n",
      "Epoch 66  Train Loss: 0.6908  Val Loss: 0.6804\n",
      "Epoch 67  Train Loss: 0.6988  Val Loss: 0.6797\n",
      "Epoch 68  Train Loss: 0.6999  Val Loss: 0.6806\n",
      "Epoch 69  Train Loss: 0.6967  Val Loss: 0.6810\n",
      "Epoch 70  Train Loss: 0.6955  Val Loss: 0.6805\n",
      "Epoch 71  Train Loss: 0.6971  Val Loss: 0.6798\n",
      "Epoch 72  Train Loss: 0.6979  Val Loss: 0.6802\n",
      "Epoch 73  Train Loss: 0.6926  Val Loss: 0.6801\n",
      "Epoch 74  Train Loss: 0.6911  Val Loss: 0.6804\n",
      "Epoch 75  Train Loss: 0.6962  Val Loss: 0.6803\n",
      "Epoch 76  Train Loss: 0.6894  Val Loss: 0.6806\n",
      "Epoch 77  Train Loss: 0.6891  Val Loss: 0.6803\n",
      "Epoch 78  Train Loss: 0.6979  Val Loss: 0.6800\n",
      "Epoch 79  Train Loss: 0.6906  Val Loss: 0.6808\n",
      "Epoch 80  Train Loss: 0.7004  Val Loss: 0.6804\n",
      "Epoch 81  Train Loss: 0.6893  Val Loss: 0.6801\n",
      "Epoch 82  Train Loss: 0.6893  Val Loss: 0.6797\n",
      "Epoch 83  Train Loss: 0.6950  Val Loss: 0.6798\n",
      "Epoch 84  Train Loss: 0.6909  Val Loss: 0.6802\n",
      "Epoch 85  Train Loss: 0.6884  Val Loss: 0.6796\n",
      "Epoch 86  Train Loss: 0.6946  Val Loss: 0.6796\n",
      "Epoch 87  Train Loss: 0.6908  Val Loss: 0.6790\n",
      "Epoch 88  Train Loss: 0.6910  Val Loss: 0.6798\n",
      "Epoch 89  Train Loss: 0.6985  Val Loss: 0.6794\n",
      "Epoch 90  Train Loss: 0.6964  Val Loss: 0.6799\n",
      "Epoch 91  Train Loss: 0.6971  Val Loss: 0.6788\n",
      "Epoch 92  Train Loss: 0.6909  Val Loss: 0.6796\n",
      "Epoch 93  Train Loss: 0.6900  Val Loss: 0.6796\n",
      "Epoch 94  Train Loss: 0.6924  Val Loss: 0.6793\n",
      "Epoch 95  Train Loss: 0.6985  Val Loss: 0.6785\n",
      "Epoch 96  Train Loss: 0.6854  Val Loss: 0.6787\n",
      "Epoch 97  Train Loss: 0.6855  Val Loss: 0.6795\n",
      "Epoch 98  Train Loss: 0.6961  Val Loss: 0.6791\n",
      "Epoch 99  Train Loss: 0.6898  Val Loss: 0.6802\n",
      "Epoch 100  Train Loss: 0.6876  Val Loss: 0.6791\n",
      "Epoch 101  Train Loss: 0.6863  Val Loss: 0.6791\n",
      "Epoch 102  Train Loss: 0.6814  Val Loss: 0.6786\n",
      "Epoch 103  Train Loss: 0.6904  Val Loss: 0.6786\n",
      "Epoch 104  Train Loss: 0.6921  Val Loss: 0.6797\n",
      "Epoch 105  Train Loss: 0.6844  Val Loss: 0.6787\n",
      "Epoch 106  Train Loss: 0.6834  Val Loss: 0.6786\n",
      "Epoch 107  Train Loss: 0.6925  Val Loss: 0.6786\n",
      "Epoch 108  Train Loss: 0.6884  Val Loss: 0.6792\n",
      "Epoch 109  Train Loss: 0.6867  Val Loss: 0.6787\n",
      "Epoch 110  Train Loss: 0.6898  Val Loss: 0.6793\n",
      "Epoch 111  Train Loss: 0.6954  Val Loss: 0.6791\n",
      "Epoch 112  Train Loss: 0.6849  Val Loss: 0.6785\n",
      "Epoch 113  Train Loss: 0.6867  Val Loss: 0.6785\n",
      "Epoch 114  Train Loss: 0.6895  Val Loss: 0.6788\n",
      "Epoch 115  Train Loss: 0.6908  Val Loss: 0.6785\n",
      "Epoch 116  Train Loss: 0.6808  Val Loss: 0.6790\n",
      "Epoch 117  Train Loss: 0.6882  Val Loss: 0.6780\n",
      "Epoch 118  Train Loss: 0.6878  Val Loss: 0.6786\n",
      "Epoch 119  Train Loss: 0.6858  Val Loss: 0.6790\n",
      "Epoch 120  Train Loss: 0.6891  Val Loss: 0.6780\n",
      "Epoch 121  Train Loss: 0.6886  Val Loss: 0.6785\n",
      "Epoch 122  Train Loss: 0.6867  Val Loss: 0.6791\n",
      "Epoch 123  Train Loss: 0.6950  Val Loss: 0.6784\n",
      "Epoch 124  Train Loss: 0.6880  Val Loss: 0.6781\n",
      "Epoch 125  Train Loss: 0.6881  Val Loss: 0.6785\n",
      "Epoch 126  Train Loss: 0.6895  Val Loss: 0.6773\n",
      "Epoch 127  Train Loss: 0.6851  Val Loss: 0.6783\n",
      "Epoch 128  Train Loss: 0.6868  Val Loss: 0.6780\n",
      "Epoch 129  Train Loss: 0.6816  Val Loss: 0.6779\n",
      "Epoch 130  Train Loss: 0.6831  Val Loss: 0.6784\n",
      "Epoch 131  Train Loss: 0.6838  Val Loss: 0.6781\n",
      "Epoch 132  Train Loss: 0.6849  Val Loss: 0.6781\n",
      "Epoch 133  Train Loss: 0.6864  Val Loss: 0.6782\n",
      "Epoch 134  Train Loss: 0.6874  Val Loss: 0.6783\n",
      "Epoch 135  Train Loss: 0.6921  Val Loss: 0.6785\n",
      "Epoch 136  Train Loss: 0.6906  Val Loss: 0.6778\n",
      "Epoch 137  Train Loss: 0.6856  Val Loss: 0.6786\n",
      "Epoch 138  Train Loss: 0.6842  Val Loss: 0.6783\n",
      "Epoch 139  Train Loss: 0.6862  Val Loss: 0.6784\n",
      "Epoch 140  Train Loss: 0.6833  Val Loss: 0.6790\n",
      "Epoch 141  Train Loss: 0.6879  Val Loss: 0.6781\n",
      "Epoch 142  Train Loss: 0.6875  Val Loss: 0.6779\n",
      "Epoch 143  Train Loss: 0.6895  Val Loss: 0.6781\n",
      "Epoch 144  Train Loss: 0.6957  Val Loss: 0.6779\n",
      "Epoch 145  Train Loss: 0.6872  Val Loss: 0.6778\n",
      "Epoch 146  Train Loss: 0.6874  Val Loss: 0.6784\n",
      "Epoch 147  Train Loss: 0.6797  Val Loss: 0.6777\n",
      "Epoch 148  Train Loss: 0.6827  Val Loss: 0.6780\n",
      "Epoch 149  Train Loss: 0.6878  Val Loss: 0.6783\n",
      "Epoch 150  Train Loss: 0.6828  Val Loss: 0.6779\n",
      "Epoch 151  Train Loss: 0.6828  Val Loss: 0.6787\n",
      "Epoch 152  Train Loss: 0.6819  Val Loss: 0.6780\n",
      "Epoch 153  Train Loss: 0.6871  Val Loss: 0.6769\n",
      "Epoch 154  Train Loss: 0.6857  Val Loss: 0.6783\n",
      "Epoch 155  Train Loss: 0.6893  Val Loss: 0.6778\n",
      "Epoch 156  Train Loss: 0.6790  Val Loss: 0.6780\n",
      "Epoch 157  Train Loss: 0.6860  Val Loss: 0.6782\n",
      "Epoch 158  Train Loss: 0.6828  Val Loss: 0.6778\n",
      "Epoch 159  Train Loss: 0.6826  Val Loss: 0.6775\n",
      "Epoch 160  Train Loss: 0.6838  Val Loss: 0.6777\n",
      "Epoch 161  Train Loss: 0.6794  Val Loss: 0.6778\n",
      "Epoch 162  Train Loss: 0.6840  Val Loss: 0.6783\n",
      "Epoch 163  Train Loss: 0.6855  Val Loss: 0.6778\n",
      "Epoch 164  Train Loss: 0.6860  Val Loss: 0.6778\n",
      "Epoch 165  Train Loss: 0.6859  Val Loss: 0.6782\n",
      "Epoch 166  Train Loss: 0.6820  Val Loss: 0.6781\n",
      "Epoch 167  Train Loss: 0.6877  Val Loss: 0.6778\n",
      "Epoch 168  Train Loss: 0.6829  Val Loss: 0.6776\n",
      "Epoch 169  Train Loss: 0.6885  Val Loss: 0.6780\n",
      "Epoch 170  Train Loss: 0.6785  Val Loss: 0.6776\n",
      "Epoch 171  Train Loss: 0.6841  Val Loss: 0.6779\n",
      "Epoch 172  Train Loss: 0.6781  Val Loss: 0.6777\n",
      "Epoch 173  Train Loss: 0.6816  Val Loss: 0.6777\n",
      "Epoch 174  Train Loss: 0.6808  Val Loss: 0.6780\n",
      "Epoch 175  Train Loss: 0.6756  Val Loss: 0.6776\n",
      "Epoch 176  Train Loss: 0.6836  Val Loss: 0.6778\n",
      "Epoch 177  Train Loss: 0.6798  Val Loss: 0.6775\n",
      "Epoch 178  Train Loss: 0.6808  Val Loss: 0.6779\n",
      "Epoch 179  Train Loss: 0.6788  Val Loss: 0.6772\n",
      "Epoch 180  Train Loss: 0.6822  Val Loss: 0.6780\n",
      "Epoch 181  Train Loss: 0.6729  Val Loss: 0.6780\n",
      "Epoch 182  Train Loss: 0.6808  Val Loss: 0.6786\n",
      "Epoch 183  Train Loss: 0.6760  Val Loss: 0.6780\n",
      "Epoch 184  Train Loss: 0.6794  Val Loss: 0.6773\n",
      "Epoch 185  Train Loss: 0.6757  Val Loss: 0.6777\n",
      "Epoch 186  Train Loss: 0.6728  Val Loss: 0.6776\n",
      "Epoch 187  Train Loss: 0.6854  Val Loss: 0.6777\n",
      "Epoch 188  Train Loss: 0.6836  Val Loss: 0.6775\n",
      "Epoch 189  Train Loss: 0.6747  Val Loss: 0.6775\n",
      "Epoch 190  Train Loss: 0.6807  Val Loss: 0.6775\n",
      "Epoch 191  Train Loss: 0.6757  Val Loss: 0.6769\n",
      "Epoch 192  Train Loss: 0.6789  Val Loss: 0.6778\n",
      "Epoch 193  Train Loss: 0.6769  Val Loss: 0.6768\n",
      "Epoch 194  Train Loss: 0.6738  Val Loss: 0.6772\n",
      "Epoch 195  Train Loss: 0.6825  Val Loss: 0.6769\n",
      "Epoch 196  Train Loss: 0.6807  Val Loss: 0.6776\n",
      "Epoch 197  Train Loss: 0.6762  Val Loss: 0.6778\n",
      "Epoch 198  Train Loss: 0.6794  Val Loss: 0.6777\n",
      "Epoch 199  Train Loss: 0.6792  Val Loss: 0.6778\n",
      "Epoch 200  Train Loss: 0.6835  Val Loss: 0.6772\n",
      "Epoch 201  Train Loss: 0.6787  Val Loss: 0.6765\n",
      "Epoch 202  Train Loss: 0.6780  Val Loss: 0.6771\n",
      "Epoch 203  Train Loss: 0.6834  Val Loss: 0.6776\n",
      "Epoch 204  Train Loss: 0.6804  Val Loss: 0.6775\n",
      "Epoch 205  Train Loss: 0.6837  Val Loss: 0.6776\n",
      "Epoch 206  Train Loss: 0.6749  Val Loss: 0.6772\n",
      "Epoch 207  Train Loss: 0.6804  Val Loss: 0.6773\n",
      "Epoch 208  Train Loss: 0.6761  Val Loss: 0.6775\n",
      "Epoch 209  Train Loss: 0.6773  Val Loss: 0.6781\n",
      "Epoch 210  Train Loss: 0.6856  Val Loss: 0.6770\n",
      "Epoch 211  Train Loss: 0.6808  Val Loss: 0.6781\n",
      "Epoch 212  Train Loss: 0.6844  Val Loss: 0.6767\n",
      "Epoch 213  Train Loss: 0.6769  Val Loss: 0.6784\n",
      "Epoch 214  Train Loss: 0.6760  Val Loss: 0.6768\n",
      "Epoch 215  Train Loss: 0.6852  Val Loss: 0.6772\n",
      "Epoch 216  Train Loss: 0.6730  Val Loss: 0.6769\n",
      "Epoch 217  Train Loss: 0.6840  Val Loss: 0.6770\n",
      "Epoch 218  Train Loss: 0.6764  Val Loss: 0.6776\n",
      "Epoch 219  Train Loss: 0.6744  Val Loss: 0.6777\n",
      "Epoch 220  Train Loss: 0.6805  Val Loss: 0.6773\n",
      "Epoch 221  Train Loss: 0.6775  Val Loss: 0.6770\n",
      "Epoch 222  Train Loss: 0.6771  Val Loss: 0.6765\n",
      "Epoch 223  Train Loss: 0.6798  Val Loss: 0.6777\n",
      "Epoch 224  Train Loss: 0.6736  Val Loss: 0.6768\n",
      "Epoch 225  Train Loss: 0.6729  Val Loss: 0.6770\n",
      "Epoch 226  Train Loss: 0.6765  Val Loss: 0.6772\n",
      "Epoch 227  Train Loss: 0.6803  Val Loss: 0.6764\n",
      "Epoch 228  Train Loss: 0.6830  Val Loss: 0.6776\n",
      "Epoch 229  Train Loss: 0.6714  Val Loss: 0.6775\n",
      "Epoch 230  Train Loss: 0.6777  Val Loss: 0.6775\n",
      "Epoch 231  Train Loss: 0.6782  Val Loss: 0.6773\n",
      "Epoch 232  Train Loss: 0.6799  Val Loss: 0.6773\n",
      "Epoch 233  Train Loss: 0.6794  Val Loss: 0.6777\n",
      "Epoch 234  Train Loss: 0.6841  Val Loss: 0.6766\n",
      "Epoch 235  Train Loss: 0.6799  Val Loss: 0.6763\n",
      "Epoch 236  Train Loss: 0.6746  Val Loss: 0.6766\n",
      "Epoch 237  Train Loss: 0.6774  Val Loss: 0.6775\n",
      "Epoch 238  Train Loss: 0.6717  Val Loss: 0.6773\n",
      "Epoch 239  Train Loss: 0.6758  Val Loss: 0.6772\n",
      "Epoch 240  Train Loss: 0.6770  Val Loss: 0.6768\n",
      "Epoch 241  Train Loss: 0.6831  Val Loss: 0.6774\n",
      "Epoch 242  Train Loss: 0.6812  Val Loss: 0.6767\n",
      "Epoch 243  Train Loss: 0.6766  Val Loss: 0.6766\n",
      "Epoch 244  Train Loss: 0.6736  Val Loss: 0.6766\n",
      "Epoch 245  Train Loss: 0.6802  Val Loss: 0.6774\n",
      "Epoch 246  Train Loss: 0.6739  Val Loss: 0.6777\n",
      "Epoch 247  Train Loss: 0.6728  Val Loss: 0.6772\n",
      "Epoch 248  Train Loss: 0.6672  Val Loss: 0.6773\n",
      "Epoch 249  Train Loss: 0.6773  Val Loss: 0.6770\n",
      "Epoch 250  Train Loss: 0.6708  Val Loss: 0.6766\n",
      "Epoch 251  Train Loss: 0.6774  Val Loss: 0.6774\n",
      "Epoch 252  Train Loss: 0.6782  Val Loss: 0.6776\n",
      "Epoch 253  Train Loss: 0.6718  Val Loss: 0.6772\n",
      "Epoch 254  Train Loss: 0.6759  Val Loss: 0.6774\n",
      "Epoch 255  Train Loss: 0.6753  Val Loss: 0.6772\n",
      "Epoch 256  Train Loss: 0.6827  Val Loss: 0.6776\n",
      "Epoch 257  Train Loss: 0.6783  Val Loss: 0.6776\n",
      "Epoch 258  Train Loss: 0.6772  Val Loss: 0.6767\n",
      "Epoch 259  Train Loss: 0.6725  Val Loss: 0.6771\n",
      "Epoch 260  Train Loss: 0.6752  Val Loss: 0.6772\n",
      "Epoch 261  Train Loss: 0.6731  Val Loss: 0.6776\n",
      "Epoch 262  Train Loss: 0.6755  Val Loss: 0.6776\n",
      "Epoch 263  Train Loss: 0.6739  Val Loss: 0.6768\n",
      "Epoch 264  Train Loss: 0.6747  Val Loss: 0.6774\n",
      "Epoch 265  Train Loss: 0.6722  Val Loss: 0.6776\n",
      "Epoch 266  Train Loss: 0.6767  Val Loss: 0.6779\n",
      "Epoch 267  Train Loss: 0.6753  Val Loss: 0.6763\n",
      "Epoch 268  Train Loss: 0.6789  Val Loss: 0.6775\n",
      "Epoch 269  Train Loss: 0.6777  Val Loss: 0.6769\n",
      "Epoch 270  Train Loss: 0.6776  Val Loss: 0.6767\n",
      "Epoch 271  Train Loss: 0.6835  Val Loss: 0.6769\n",
      "Epoch 272  Train Loss: 0.6764  Val Loss: 0.6763\n",
      "Epoch 273  Train Loss: 0.6749  Val Loss: 0.6767\n",
      "Epoch 274  Train Loss: 0.6724  Val Loss: 0.6766\n",
      "Epoch 275  Train Loss: 0.6766  Val Loss: 0.6770\n",
      "Epoch 276  Train Loss: 0.6769  Val Loss: 0.6776\n",
      "Epoch 277  Train Loss: 0.6718  Val Loss: 0.6765\n",
      "Epoch 278  Train Loss: 0.6749  Val Loss: 0.6775\n",
      "Epoch 279  Train Loss: 0.6729  Val Loss: 0.6772\n",
      "Epoch 280  Train Loss: 0.6769  Val Loss: 0.6770\n",
      "Epoch 281  Train Loss: 0.6749  Val Loss: 0.6774\n",
      "Epoch 282  Train Loss: 0.6705  Val Loss: 0.6758\n",
      "Epoch 283  Train Loss: 0.6752  Val Loss: 0.6776\n",
      "Epoch 284  Train Loss: 0.6707  Val Loss: 0.6774\n",
      "Epoch 285  Train Loss: 0.6732  Val Loss: 0.6771\n",
      "Epoch 286  Train Loss: 0.6755  Val Loss: 0.6770\n",
      "Epoch 287  Train Loss: 0.6684  Val Loss: 0.6766\n",
      "Epoch 288  Train Loss: 0.6754  Val Loss: 0.6767\n",
      "Epoch 289  Train Loss: 0.6701  Val Loss: 0.6768\n",
      "Epoch 290  Train Loss: 0.6713  Val Loss: 0.6767\n",
      "Epoch 291  Train Loss: 0.6701  Val Loss: 0.6767\n",
      "Epoch 292  Train Loss: 0.6717  Val Loss: 0.6757\n",
      "Epoch 293  Train Loss: 0.6763  Val Loss: 0.6777\n",
      "Epoch 294  Train Loss: 0.6687  Val Loss: 0.6767\n",
      "Epoch 295  Train Loss: 0.6790  Val Loss: 0.6768\n",
      "Epoch 296  Train Loss: 0.6722  Val Loss: 0.6771\n",
      "Epoch 297  Train Loss: 0.6726  Val Loss: 0.6769\n",
      "Epoch 298  Train Loss: 0.6815  Val Loss: 0.6769\n",
      "Epoch 299  Train Loss: 0.6770  Val Loss: 0.6771\n",
      "Epoch 300  Train Loss: 0.6759  Val Loss: 0.6764\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.56      0.56       504\n",
      "         1.0       0.56      0.55      0.55       504\n",
      "\n",
      "    accuracy                           0.55      1008\n",
      "   macro avg       0.55      0.55      0.55      1008\n",
      "weighted avg       0.55      0.55      0.55      1008\n",
      "\n",
      "Test ROC AUC: 0.5950176366843034\n"
     ]
    }
   ],
   "source": [
    "# 3 layer mlp\n",
    "\n",
    "def to_tensor_dataset(X, y):\n",
    "    Xt = torch.from_numpy(X).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return TensorDataset(Xt, yt)\n",
    "\n",
    "train_ds = to_tensor_dataset(X_train, y_train)\n",
    "val_ds   = to_tensor_dataset(X_val, y_val)\n",
    "test_ds  = to_tensor_dataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3) Define the 3-layer MLP\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ThreeLayerMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 4) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5) Training & Validation Loop\n",
    "n_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # -- Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # -- Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 6) Load best model and test evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_probs = []\n",
    "y_true  = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        probs = model(xb)\n",
    "        y_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "        y_true .extend(yb.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_pred = (np.array(y_probs) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_true, y_probs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
