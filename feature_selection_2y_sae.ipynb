{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85047a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c99e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6701b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(npz_paths: str, data_type: str):\n",
    "    if data_type not in [\"X_mean\", \"X_max\", \"X_concat\"]:\n",
    "        raise Exception(\"data type in valid\")\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for npz_path in npz_paths:\n",
    "    \n",
    "        base = os.path.splitext(os.path.basename(npz_path))[0]      \n",
    "        csv_path = os.path.join(\n",
    "            os.path.dirname(npz_path),\n",
    "            base + \"_meta.csv\"                                       \n",
    "        )\n",
    "\n",
    "\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        X = data[data_type]      # (N_docs, 2*D)\n",
    "        # X_concat = data[\"X_mean\"]\n",
    "        tids = data[\"transcriptids\"]    \n",
    "\n",
    "\n",
    "        meta = pd.read_csv(csv_path)\n",
    "\n",
    "        meta_unique = (\n",
    "            meta[[\"transcriptid\", \"SUESCORE\", \"label\"]]\n",
    "            .drop_duplicates(subset=\"transcriptid\", keep=\"first\")\n",
    "            .set_index(\"transcriptid\")\n",
    "        )\n",
    "\n",
    "        mask_ids = np.isin(tids, meta_unique.index)\n",
    "        X_filt = X[mask_ids]\n",
    "        tids_filt = np.array(tids)[mask_ids]\n",
    "\n",
    "\n",
    "        lab_df = meta.assign(\n",
    "            label=lambda df: df.SUESCORE.map(\n",
    "                lambda s: 1 if s >= 0.5 else (0 if s <= -0.5 else np.nan)\n",
    "            )\n",
    "        )\n",
    "        mask_label = lab_df.label.notna().values\n",
    "        # apply the same mask in the same order as the CSV, so we use .loc on lab_df\n",
    "        # but first filter lab_df to only those transcriptids in tids_filt\n",
    "        Xc, y = X_filt[mask_label], meta.loc[mask_label, \"label\"].astype(int).values\n",
    "        \n",
    "        # now align X and y\n",
    "        # X_final = X_filt[lab_sub.label.notna()]\n",
    "        # y_final = lab_sub.label.astype(int).values\n",
    "\n",
    "        # collect\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(y)\n",
    "\n",
    "    # 2. concatenate all files together\n",
    "    Xc = np.vstack(X_list)   # shape: (sum_i N_i, 2*D)\n",
    "    y  = np.concatenate(y_list)  # shape: (sum_i N_i,)\n",
    "\n",
    "    print(\"Combined Xc shape:\", Xc.shape)\n",
    "    print(\"Combined y shape: \", y.shape)\n",
    "\n",
    "    return Xc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "439345d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_balance(Xc_unbalanced, y_unbalanced):\n",
    "    # forced resampling\n",
    "    idx0 = np.where(y_unbalanced == 0)[0]\n",
    "    idx1 = np.where(y_unbalanced == 1)[0]\n",
    "\n",
    "    n = min(len(idx0), len(idx1))\n",
    "\n",
    "    sel0 = np.random.choice(idx0, size=n, replace=False)\n",
    "    sel1 = np.random.choice(idx1, size=n, replace=False)\n",
    "\n",
    "    sel = np.concatenate([sel0, sel1])\n",
    "    np.random.shuffle(sel)\n",
    "\n",
    "    # slice out your balanced subset\n",
    "    Xc_out = Xc_unbalanced[sel]\n",
    "    y_out = y_unbalanced[sel]\n",
    "\n",
    "    print(\"Balanced X shape:\", Xc_out.shape)\n",
    "    print(\"Balanced y counts:\", np.bincount(y_out))\n",
    "    return Xc_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7d34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz_paths = [\n",
    "    \"./data/doc_features/transcript_componenttext_2013_1_features.npz\",    \n",
    "    \"./data/doc_features/transcript_componenttext_2013_2_features.npz\",\n",
    "\n",
    "]\n",
    "\n",
    "val_npz_paths = [\n",
    "    \"./data/doc_features/transcript_componenttext_2014_1_features.npz\",\n",
    "]\n",
    "\n",
    "# test_npz_paths = [\n",
    "\n",
    "#     \"./data/doc_features/transcript_componenttext_2014_1_features.npz\",\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6823001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Xc shape: (2449, 16384)\n",
      "Combined y shape:  (2449,)\n",
      "Combined Xc shape: (1403, 16384)\n",
      "Combined y shape:  (1403,)\n"
     ]
    }
   ],
   "source": [
    "Xc, y = load_data(train_npz_paths, \"X_max\")\n",
    "X_val_all_feat, y_val = load_data(val_npz_paths, \"X_max\")\n",
    "# X_test_all_feat, y_test = load_data(test_npz_paths, \"X_mean\")\n",
    "# Split X_val_all_feat, y_val into two equal parts:\n",
    "X_val_all_feat, X_test_all_feat, y_val, y_test = train_test_split(\n",
    "    X_val_all_feat, \n",
    "    y_val, \n",
    "    test_size=0.5,       # puts half into X_test/y_test\n",
    "    random_state=42,     # for reproducibility\n",
    "    stratify=y_val       # if you want to preserve class proportions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d55b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced X shape: (1136, 16384)\n",
      "Balanced y counts: [568 568]\n",
      "Balanced X shape: (302, 16384)\n",
      "Balanced y counts: [151 151]\n",
      "Balanced X shape: (302, 16384)\n",
      "Balanced y counts: [151 151]\n"
     ]
    }
   ],
   "source": [
    "# optional downsampling for balancing data\n",
    "Xc, y = downsample_balance(Xc, y)\n",
    "X_val_all_feat, y_val = downsample_balance(X_val_all_feat, y_val)\n",
    "X_test_all_feat, y_test = downsample_balance(X_test_all_feat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e826e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  1: 'mean' feature #3604 (t = 6.77)\n",
      "Rank  2: 'mean' feature #7516 (t = 6.73)\n",
      "Rank  3: 'mean' feature #4600 (t = 6.60)\n",
      "Rank  4: 'mean' feature #2415 (t = 6.55)\n",
      "Rank  5: 'max' feature #2368 (t = 6.52)\n",
      "Rank  6: 'max' feature #6893 (t = 6.50)\n",
      "Rank  7: 'mean' feature #1541 (t = 6.50)\n",
      "Rank  8: 'max' feature #4107 (t = 6.45)\n",
      "Rank  9: 'max' feature #2397 (t = 6.11)\n",
      "Rank 10: 'mean' feature #2428 (t = 6.10)\n",
      "Rank 11: 'max' feature #7429 (t = 6.09)\n",
      "Rank 12: 'max' feature #221 (t = 6.05)\n",
      "Rank 13: 'max' feature #4673 (t = 6.04)\n",
      "Rank 14: 'max' feature #1301 (t = 5.96)\n",
      "Rank 15: 'max' feature #7489 (t = 5.93)\n",
      "Rank 16: 'max' feature #6700 (t = 5.86)\n",
      "Rank 17: 'max' feature #1310 (t = 5.82)\n",
      "Rank 18: 'mean' feature #2845 (t = 5.80)\n",
      "Rank 19: 'mean' feature #3456 (t = 5.78)\n",
      "Rank 20: 'max' feature #5820 (t = 5.74)\n",
      "Rank 21: 'mean' feature #5279 (t = 5.69)\n",
      "Rank 22: 'mean' feature #7459 (t = 5.67)\n",
      "Rank 23: 'mean' feature #6770 (t = 5.66)\n",
      "Rank 24: 'mean' feature #1712 (t = 5.59)\n",
      "Rank 25: 'max' feature #5848 (t = 5.51)\n",
      "Rank 26: 'max' feature #2528 (t = 5.51)\n",
      "Rank 27: 'mean' feature #207 (t = 5.49)\n",
      "Rank 28: 'max' feature #1818 (t = 5.45)\n",
      "Rank 29: 'max' feature #6993 (t = 5.44)\n",
      "Rank 30: 'mean' feature #1547 (t = 5.43)\n",
      "Rank 31: 'max' feature #5818 (t = 5.41)\n",
      "Rank 32: 'max' feature #1346 (t = 5.41)\n",
      "Rank 33: 'max' feature #3828 (t = 5.39)\n",
      "Rank 34: 'max' feature #6157 (t = 5.38)\n",
      "Rank 35: 'mean' feature #5078 (t = 5.32)\n",
      "Rank 36: 'max' feature #7570 (t = 5.32)\n",
      "Rank 37: 'max' feature #4759 (t = 5.26)\n",
      "Rank 38: 'max' feature #1848 (t = 5.22)\n",
      "Rank 39: 'max' feature #4471 (t = 5.21)\n",
      "Rank 40: 'max' feature #2323 (t = 5.21)\n",
      "Rank 41: 'max' feature #6973 (t = 5.20)\n",
      "Rank 42: 'mean' feature #7357 (t = 5.17)\n",
      "Rank 43: 'mean' feature #1381 (t = 5.14)\n",
      "Rank 44: 'max' feature #903 (t = 5.10)\n",
      "Rank 45: 'mean' feature #1700 (t = 5.08)\n",
      "Rank 46: 'mean' feature #6283 (t = 5.08)\n",
      "Rank 47: 'max' feature #6178 (t = 5.07)\n",
      "Rank 48: 'max' feature #3360 (t = 5.05)\n",
      "Rank 49: 'mean' feature #7899 (t = 5.03)\n",
      "Rank 50: 'mean' feature #1174 (t = 5.02)\n",
      "Rank 51: 'mean' feature #6139 (t = 5.01)\n",
      "Rank 52: 'mean' feature #5076 (t = 5.01)\n",
      "Rank 53: 'max' feature #5363 (t = 5.00)\n",
      "Rank 54: 'mean' feature #5095 (t = 5.00)\n",
      "Rank 55: 'max' feature #1053 (t = 4.99)\n",
      "Rank 56: 'max' feature #6430 (t = 4.97)\n",
      "Rank 57: 'mean' feature #4152 (t = 4.95)\n",
      "Rank 58: 'max' feature #1201 (t = 4.95)\n",
      "Rank 59: 'max' feature #2268 (t = 4.92)\n",
      "Rank 60: 'mean' feature #4502 (t = 4.87)\n",
      "Rank 61: 'max' feature #4027 (t = 4.84)\n",
      "Rank 62: 'max' feature #2527 (t = 4.84)\n",
      "Rank 63: 'mean' feature #7386 (t = 4.83)\n",
      "Rank 64: 'max' feature #4508 (t = 4.82)\n",
      "Rank 65: 'max' feature #76 (t = 4.80)\n",
      "Rank 66: 'mean' feature #3154 (t = 4.80)\n",
      "Rank 67: 'mean' feature #4983 (t = 4.76)\n",
      "Rank 68: 'mean' feature #4838 (t = 4.73)\n",
      "Rank 69: 'max' feature #7819 (t = 4.72)\n",
      "Rank 70: 'max' feature #475 (t = 4.71)\n",
      "Rank 71: 'max' feature #3438 (t = 4.66)\n",
      "Rank 72: 'mean' feature #3609 (t = 4.66)\n",
      "Rank 73: 'mean' feature #3585 (t = 4.65)\n",
      "Rank 74: 'max' feature #8087 (t = 4.65)\n",
      "Rank 75: 'max' feature #4717 (t = 4.64)\n",
      "Rank 76: 'max' feature #5129 (t = 4.64)\n",
      "Rank 77: 'mean' feature #5375 (t = 4.63)\n",
      "Rank 78: 'mean' feature #2505 (t = 4.62)\n",
      "Rank 79: 'max' feature #188 (t = 4.61)\n",
      "Rank 80: 'max' feature #987 (t = 4.57)\n",
      "Rank 81: 'mean' feature #86 (t = 4.54)\n",
      "Rank 82: 'mean' feature #4878 (t = 4.53)\n",
      "Rank 83: 'mean' feature #615 (t = 4.53)\n",
      "Rank 84: 'max' feature #6335 (t = 4.52)\n",
      "Rank 85: 'max' feature #141 (t = 4.50)\n",
      "Rank 86: 'mean' feature #635 (t = 4.48)\n",
      "Rank 87: 'mean' feature #5520 (t = 4.46)\n",
      "Rank 88: 'mean' feature #637 (t = 4.46)\n",
      "Rank 89: 'mean' feature #4660 (t = 4.45)\n",
      "Rank 90: 'mean' feature #5817 (t = 4.44)\n",
      "Rank 91: 'mean' feature #7456 (t = 4.41)\n",
      "Rank 92: 'max' feature #6752 (t = 4.41)\n",
      "Rank 93: 'max' feature #5690 (t = 4.41)\n",
      "Rank 94: 'mean' feature #6122 (t = 4.40)\n",
      "Rank 95: 'max' feature #2305 (t = 4.39)\n",
      "Rank 96: 'max' feature #326 (t = 4.37)\n",
      "Rank 97: 'mean' feature #4192 (t = 4.37)\n",
      "Rank 98: 'mean' feature #4470 (t = 4.36)\n",
      "Rank 99: 'mean' feature #1224 (t = 4.36)\n",
      "Rank 100: 'max' feature #1756 (t = 4.35)\n",
      "Rank 101: 'max' feature #2365 (t = 4.34)\n",
      "Rank 102: 'max' feature #2453 (t = 4.33)\n",
      "Rank 103: 'max' feature #6824 (t = 4.33)\n",
      "Rank 104: 'mean' feature #523 (t = 4.32)\n",
      "Rank 105: 'mean' feature #5240 (t = 4.31)\n",
      "Rank 106: 'max' feature #2505 (t = 4.31)\n",
      "Rank 107: 'max' feature #7206 (t = 4.31)\n",
      "Rank 108: 'max' feature #294 (t = 4.28)\n",
      "Rank 109: 'max' feature #3651 (t = 4.28)\n",
      "Rank 110: 'max' feature #2483 (t = 4.26)\n",
      "Rank 111: 'mean' feature #7589 (t = 4.25)\n",
      "Rank 112: 'max' feature #4753 (t = 4.25)\n",
      "Rank 113: 'max' feature #957 (t = 4.24)\n",
      "Rank 114: 'mean' feature #3774 (t = 4.23)\n",
      "Rank 115: 'mean' feature #4177 (t = 4.23)\n",
      "Rank 116: 'mean' feature #5409 (t = 4.22)\n",
      "Rank 117: 'mean' feature #7418 (t = 4.22)\n",
      "Rank 118: 'max' feature #351 (t = 4.21)\n",
      "Rank 119: 'mean' feature #1090 (t = 4.21)\n",
      "Rank 120: 'mean' feature #1382 (t = 4.21)\n",
      "Rank 121: 'mean' feature #3242 (t = 4.21)\n",
      "Rank 122: 'mean' feature #3494 (t = 4.21)\n",
      "Rank 123: 'max' feature #4122 (t = 4.20)\n",
      "Rank 124: 'mean' feature #6864 (t = 4.20)\n",
      "Rank 125: 'max' feature #7096 (t = 4.18)\n",
      "Rank 126: 'mean' feature #1462 (t = 4.18)\n",
      "Rank 127: 'max' feature #7455 (t = 4.17)\n",
      "Rank 128: 'mean' feature #255 (t = 4.17)\n",
      "Rank 129: 'max' feature #2347 (t = 4.17)\n",
      "Rank 130: 'max' feature #792 (t = 4.17)\n",
      "Rank 131: 'max' feature #2596 (t = 4.16)\n",
      "Rank 132: 'max' feature #2162 (t = 4.15)\n",
      "Rank 133: 'mean' feature #1899 (t = 4.15)\n",
      "Rank 134: 'max' feature #1841 (t = 4.14)\n",
      "Rank 135: 'mean' feature #102 (t = 4.14)\n",
      "Rank 136: 'mean' feature #6000 (t = 4.14)\n",
      "Rank 137: 'max' feature #5353 (t = 4.14)\n",
      "Rank 138: 'mean' feature #6802 (t = 4.13)\n",
      "Rank 139: 'mean' feature #6703 (t = 4.13)\n",
      "Rank 140: 'max' feature #4435 (t = 4.13)\n",
      "Rank 141: 'max' feature #7691 (t = 4.12)\n",
      "Rank 142: 'max' feature #145 (t = 4.11)\n",
      "Rank 143: 'max' feature #4455 (t = 4.11)\n",
      "Rank 144: 'max' feature #6715 (t = 4.11)\n",
      "Rank 145: 'max' feature #7039 (t = 4.11)\n",
      "Rank 146: 'max' feature #971 (t = 4.11)\n",
      "Rank 147: 'max' feature #4819 (t = 4.10)\n",
      "Rank 148: 'max' feature #3787 (t = 4.10)\n",
      "Rank 149: 'mean' feature #110 (t = 4.10)\n",
      "Rank 150: 'mean' feature #7172 (t = 4.10)\n",
      "Rank 151: 'max' feature #5755 (t = 4.10)\n",
      "Rank 152: 'mean' feature #4283 (t = 4.09)\n",
      "Rank 153: 'max' feature #3067 (t = 4.08)\n",
      "Rank 154: 'max' feature #5166 (t = 4.07)\n",
      "Rank 155: 'mean' feature #6344 (t = 4.07)\n",
      "Rank 156: 'mean' feature #1145 (t = 4.07)\n",
      "Rank 157: 'max' feature #6953 (t = 4.07)\n",
      "Rank 158: 'max' feature #5146 (t = 4.06)\n",
      "Rank 159: 'max' feature #2438 (t = 4.05)\n",
      "Rank 160: 'max' feature #6652 (t = 4.05)\n",
      "Rank 161: 'mean' feature #4151 (t = 4.05)\n",
      "Rank 162: 'max' feature #3246 (t = 4.05)\n",
      "Rank 163: 'mean' feature #1813 (t = 4.05)\n",
      "Rank 164: 'max' feature #4004 (t = 4.05)\n",
      "Rank 165: 'mean' feature #3352 (t = 4.04)\n",
      "Rank 166: 'max' feature #2790 (t = 4.04)\n",
      "Rank 167: 'mean' feature #1153 (t = 4.03)\n",
      "Rank 168: 'max' feature #4746 (t = 4.03)\n",
      "Rank 169: 'mean' feature #4408 (t = 4.03)\n",
      "Rank 170: 'max' feature #4292 (t = 4.02)\n",
      "Rank 171: 'mean' feature #1770 (t = 4.02)\n",
      "Rank 172: 'mean' feature #6523 (t = 4.02)\n",
      "Rank 173: 'mean' feature #4113 (t = 4.02)\n",
      "Rank 174: 'mean' feature #3688 (t = 4.01)\n",
      "Rank 175: 'mean' feature #2133 (t = 4.01)\n",
      "Rank 176: 'mean' feature #1797 (t = 4.00)\n",
      "Rank 177: 'mean' feature #5445 (t = 4.00)\n",
      "Rank 178: 'mean' feature #1198 (t = 4.00)\n",
      "Rank 179: 'max' feature #540 (t = 3.99)\n",
      "Rank 180: 'mean' feature #939 (t = 3.99)\n",
      "Rank 181: 'max' feature #7987 (t = 3.99)\n",
      "Rank 182: 'mean' feature #7654 (t = 3.99)\n",
      "Rank 183: 'mean' feature #3590 (t = 3.99)\n",
      "Rank 184: 'max' feature #6351 (t = 3.98)\n",
      "Rank 185: 'mean' feature #6273 (t = 3.98)\n",
      "Rank 186: 'max' feature #698 (t = 3.97)\n",
      "Rank 187: 'max' feature #2445 (t = 3.97)\n",
      "Rank 188: 'mean' feature #4789 (t = 3.96)\n",
      "Rank 189: 'max' feature #6238 (t = 3.96)\n",
      "Rank 190: 'max' feature #3378 (t = 3.96)\n",
      "Rank 191: 'mean' feature #1746 (t = 3.96)\n",
      "Rank 192: 'mean' feature #4314 (t = 3.96)\n",
      "Rank 193: 'max' feature #6506 (t = 3.95)\n",
      "Rank 194: 'max' feature #5049 (t = 3.95)\n",
      "Rank 195: 'mean' feature #1519 (t = 3.93)\n",
      "Rank 196: 'max' feature #4384 (t = 3.93)\n",
      "Rank 197: 'mean' feature #4236 (t = 3.93)\n",
      "Rank 198: 'max' feature #1722 (t = 3.93)\n",
      "Rank 199: 'max' feature #5160 (t = 3.93)\n",
      "Rank 200: 'max' feature #4170 (t = 3.92)\n",
      "Rank 201: 'mean' feature #3074 (t = 3.92)\n",
      "Rank 202: 'mean' feature #5653 (t = 3.92)\n",
      "Rank 203: 'max' feature #7040 (t = 3.91)\n",
      "Rank 204: 'mean' feature #7276 (t = 3.91)\n",
      "Rank 205: 'max' feature #5903 (t = 3.91)\n",
      "Rank 206: 'mean' feature #2397 (t = 3.90)\n",
      "Rank 207: 'max' feature #4273 (t = 3.90)\n",
      "Rank 208: 'max' feature #631 (t = 3.90)\n",
      "Rank 209: 'mean' feature #3471 (t = 3.89)\n",
      "Rank 210: 'mean' feature #1074 (t = 3.89)\n",
      "Rank 211: 'mean' feature #2625 (t = 3.89)\n",
      "Rank 212: 'mean' feature #1443 (t = 3.88)\n",
      "Rank 213: 'mean' feature #1881 (t = 3.88)\n",
      "Rank 214: 'max' feature #4308 (t = 3.88)\n",
      "Rank 215: 'mean' feature #3878 (t = 3.88)\n",
      "Rank 216: 'max' feature #4305 (t = 3.87)\n",
      "Rank 217: 'mean' feature #261 (t = 3.87)\n",
      "Rank 218: 'max' feature #4828 (t = 3.87)\n",
      "Rank 219: 'mean' feature #1405 (t = 3.87)\n",
      "Rank 220: 'mean' feature #6806 (t = 3.86)\n",
      "Rank 221: 'max' feature #3941 (t = 3.86)\n",
      "Rank 222: 'mean' feature #945 (t = 3.86)\n",
      "Rank 223: 'mean' feature #7502 (t = 3.85)\n",
      "Rank 224: 'max' feature #3357 (t = 3.85)\n",
      "Rank 225: 'max' feature #8086 (t = 3.84)\n",
      "Rank 226: 'mean' feature #6259 (t = 3.84)\n",
      "Rank 227: 'mean' feature #7452 (t = 3.84)\n",
      "Rank 228: 'max' feature #441 (t = 3.84)\n",
      "Rank 229: 'max' feature #6471 (t = 3.84)\n",
      "Rank 230: 'max' feature #1245 (t = 3.84)\n",
      "Rank 231: 'max' feature #7546 (t = 3.84)\n",
      "Rank 232: 'max' feature #2487 (t = 3.83)\n",
      "Rank 233: 'max' feature #7431 (t = 3.83)\n",
      "Rank 234: 'max' feature #4707 (t = 3.83)\n",
      "Rank 235: 'max' feature #7225 (t = 3.83)\n",
      "Rank 236: 'max' feature #2464 (t = 3.83)\n",
      "Rank 237: 'max' feature #4362 (t = 3.83)\n",
      "Rank 238: 'mean' feature #5266 (t = 3.83)\n",
      "Rank 239: 'mean' feature #6753 (t = 3.82)\n",
      "Rank 240: 'max' feature #6061 (t = 3.81)\n",
      "Rank 241: 'max' feature #7274 (t = 3.80)\n",
      "Rank 242: 'max' feature #606 (t = 3.80)\n",
      "Rank 243: 'max' feature #5142 (t = 3.79)\n",
      "Rank 244: 'max' feature #1983 (t = 3.79)\n",
      "Rank 245: 'mean' feature #5365 (t = 3.79)\n",
      "Rank 246: 'max' feature #6134 (t = 3.78)\n",
      "Rank 247: 'max' feature #7356 (t = 3.78)\n",
      "Rank 248: 'mean' feature #1603 (t = 3.78)\n",
      "Rank 249: 'max' feature #4189 (t = 3.77)\n",
      "Rank 250: 'max' feature #2911 (t = 3.76)\n",
      "Rank 251: 'max' feature #5591 (t = 3.76)\n",
      "Rank 252: 'mean' feature #3845 (t = 3.76)\n",
      "Rank 253: 'mean' feature #6509 (t = 3.76)\n",
      "Rank 254: 'mean' feature #1460 (t = 3.76)\n",
      "Rank 255: 'max' feature #3866 (t = 3.75)\n",
      "Rank 256: 'max' feature #3487 (t = 3.75)\n",
      "Rank 257: 'mean' feature #3820 (t = 3.75)\n",
      "Rank 258: 'max' feature #4750 (t = 3.75)\n",
      "Rank 259: 'max' feature #272 (t = 3.74)\n",
      "Rank 260: 'mean' feature #2170 (t = 3.73)\n",
      "Rank 261: 'max' feature #1782 (t = 3.72)\n",
      "Rank 262: 'max' feature #3939 (t = 3.72)\n",
      "Rank 263: 'max' feature #7665 (t = 3.72)\n",
      "Rank 264: 'mean' feature #3550 (t = 3.72)\n",
      "Rank 265: 'max' feature #5982 (t = 3.72)\n",
      "Rank 266: 'mean' feature #190 (t = 3.71)\n",
      "Rank 267: 'mean' feature #2176 (t = 3.71)\n",
      "Rank 268: 'mean' feature #5103 (t = 3.71)\n",
      "Rank 269: 'max' feature #6588 (t = 3.71)\n",
      "Rank 270: 'mean' feature #493 (t = 3.71)\n",
      "Rank 271: 'max' feature #3087 (t = 3.71)\n",
      "Rank 272: 'mean' feature #1272 (t = 3.71)\n",
      "Rank 273: 'mean' feature #4999 (t = 3.71)\n",
      "Rank 274: 'mean' feature #3272 (t = 3.70)\n",
      "Rank 275: 'mean' feature #655 (t = 3.70)\n",
      "Rank 276: 'max' feature #7131 (t = 3.70)\n",
      "Rank 277: 'max' feature #7829 (t = 3.70)\n",
      "Rank 278: 'max' feature #6363 (t = 3.70)\n",
      "Rank 279: 'mean' feature #2936 (t = 3.69)\n",
      "Rank 280: 'max' feature #6403 (t = 3.69)\n",
      "Rank 281: 'mean' feature #4746 (t = 3.69)\n",
      "Rank 282: 'mean' feature #720 (t = 3.68)\n",
      "Rank 283: 'mean' feature #2297 (t = 3.67)\n",
      "Rank 284: 'mean' feature #2285 (t = 3.67)\n",
      "Rank 285: 'mean' feature #2264 (t = 3.67)\n",
      "Rank 286: 'max' feature #1025 (t = 3.67)\n",
      "Rank 287: 'mean' feature #7121 (t = 3.67)\n",
      "Rank 288: 'mean' feature #473 (t = 3.66)\n",
      "Rank 289: 'max' feature #522 (t = 3.66)\n",
      "Rank 290: 'mean' feature #2059 (t = 3.66)\n",
      "Rank 291: 'max' feature #7604 (t = 3.65)\n",
      "Rank 292: 'max' feature #37 (t = 3.65)\n",
      "Rank 293: 'max' feature #7047 (t = 3.65)\n",
      "Rank 294: 'mean' feature #7455 (t = 3.63)\n",
      "Rank 295: 'mean' feature #4544 (t = 3.63)\n",
      "Rank 296: 'max' feature #6962 (t = 3.63)\n",
      "Rank 297: 'max' feature #7813 (t = 3.63)\n",
      "Rank 298: 'max' feature #4334 (t = 3.63)\n",
      "Rank 299: 'max' feature #195 (t = 3.63)\n",
      "Rank 300: 'max' feature #7897 (t = 3.62)\n",
      "Rank 301: 'mean' feature #7791 (t = 3.62)\n",
      "Rank 302: 'max' feature #2802 (t = 3.62)\n",
      "Rank 303: 'max' feature #6274 (t = 3.61)\n",
      "Rank 304: 'max' feature #3581 (t = 3.61)\n",
      "Rank 305: 'mean' feature #1254 (t = 3.61)\n",
      "Rank 306: 'mean' feature #2925 (t = 3.61)\n",
      "Rank 307: 'max' feature #1467 (t = 3.61)\n",
      "Rank 308: 'max' feature #3417 (t = 3.60)\n",
      "Rank 309: 'max' feature #7924 (t = 3.60)\n",
      "Rank 310: 'max' feature #1383 (t = 3.60)\n",
      "Rank 311: 'mean' feature #1352 (t = 3.59)\n",
      "Rank 312: 'max' feature #2663 (t = 3.59)\n",
      "Rank 313: 'max' feature #3316 (t = 3.59)\n",
      "Rank 314: 'max' feature #7440 (t = 3.59)\n",
      "Rank 315: 'mean' feature #4920 (t = 3.59)\n",
      "Rank 316: 'mean' feature #3990 (t = 3.59)\n",
      "Rank 317: 'max' feature #4327 (t = 3.59)\n",
      "Rank 318: 'max' feature #920 (t = 3.59)\n",
      "Rank 319: 'max' feature #3161 (t = 3.59)\n",
      "Rank 320: 'mean' feature #7804 (t = 3.58)\n",
      "Rank 321: 'mean' feature #2490 (t = 3.58)\n",
      "Rank 322: 'mean' feature #3531 (t = 3.58)\n",
      "Rank 323: 'max' feature #2577 (t = 3.58)\n",
      "Rank 324: 'max' feature #5888 (t = 3.57)\n",
      "Rank 325: 'mean' feature #3339 (t = 3.57)\n",
      "Rank 326: 'mean' feature #1051 (t = 3.57)\n",
      "Rank 327: 'mean' feature #6212 (t = 3.57)\n",
      "Rank 328: 'mean' feature #5317 (t = 3.56)\n",
      "Rank 329: 'mean' feature #2503 (t = 3.56)\n",
      "Rank 330: 'mean' feature #4183 (t = 3.55)\n",
      "Rank 331: 'max' feature #1644 (t = 3.55)\n",
      "Rank 332: 'max' feature #7130 (t = 3.55)\n",
      "Rank 333: 'mean' feature #7316 (t = 3.55)\n",
      "Rank 334: 'max' feature #4445 (t = 3.54)\n",
      "Rank 335: 'max' feature #2074 (t = 3.54)\n",
      "Rank 336: 'max' feature #7385 (t = 3.54)\n",
      "Rank 337: 'mean' feature #2366 (t = 3.54)\n",
      "Rank 338: 'max' feature #7782 (t = 3.54)\n",
      "Rank 339: 'max' feature #4366 (t = 3.54)\n",
      "Rank 340: 'mean' feature #7930 (t = 3.54)\n",
      "Rank 341: 'max' feature #7059 (t = 3.54)\n",
      "Rank 342: 'mean' feature #2488 (t = 3.53)\n",
      "Rank 343: 'max' feature #5784 (t = 3.53)\n",
      "Rank 344: 'max' feature #4490 (t = 3.53)\n",
      "Rank 345: 'max' feature #4116 (t = 3.53)\n",
      "Rank 346: 'max' feature #5834 (t = 3.53)\n",
      "Rank 347: 'mean' feature #4832 (t = 3.53)\n",
      "Rank 348: 'max' feature #5420 (t = 3.53)\n",
      "Rank 349: 'max' feature #2458 (t = 3.53)\n",
      "Rank 350: 'max' feature #4873 (t = 3.52)\n",
      "Rank 351: 'max' feature #4900 (t = 3.52)\n",
      "Rank 352: 'max' feature #1633 (t = 3.52)\n",
      "Rank 353: 'mean' feature #1637 (t = 3.52)\n",
      "Rank 354: 'max' feature #1762 (t = 3.52)\n",
      "Rank 355: 'mean' feature #753 (t = 3.52)\n",
      "Rank 356: 'mean' feature #1446 (t = 3.52)\n",
      "Rank 357: 'mean' feature #1525 (t = 3.51)\n",
      "Rank 358: 'mean' feature #3758 (t = 3.51)\n",
      "Rank 359: 'max' feature #4732 (t = 3.50)\n",
      "Rank 360: 'mean' feature #4547 (t = 3.50)\n",
      "Rank 361: 'max' feature #5907 (t = 3.50)\n",
      "Rank 362: 'max' feature #2867 (t = 3.50)\n",
      "Rank 363: 'mean' feature #5659 (t = 3.50)\n",
      "Rank 364: 'mean' feature #2207 (t = 3.50)\n",
      "Rank 365: 'mean' feature #1111 (t = 3.50)\n",
      "Rank 366: 'mean' feature #2903 (t = 3.50)\n",
      "Rank 367: 'mean' feature #4988 (t = 3.49)\n",
      "Rank 368: 'mean' feature #7859 (t = 3.48)\n",
      "Rank 369: 'mean' feature #4013 (t = 3.48)\n",
      "Rank 370: 'max' feature #2198 (t = 3.48)\n",
      "Rank 371: 'mean' feature #1481 (t = 3.48)\n",
      "Rank 372: 'max' feature #6659 (t = 3.47)\n",
      "Rank 373: 'max' feature #6681 (t = 3.47)\n",
      "Rank 374: 'mean' feature #32 (t = 3.47)\n",
      "Rank 375: 'max' feature #1681 (t = 3.47)\n",
      "Rank 376: 'max' feature #5171 (t = 3.47)\n",
      "Rank 377: 'mean' feature #5256 (t = 3.47)\n",
      "Rank 378: 'mean' feature #1302 (t = 3.47)\n",
      "Rank 379: 'mean' feature #6902 (t = 3.47)\n",
      "Rank 380: 'mean' feature #4918 (t = 3.46)\n",
      "Rank 381: 'mean' feature #11 (t = 3.45)\n",
      "Rank 382: 'mean' feature #5142 (t = 3.45)\n",
      "Rank 383: 'max' feature #3222 (t = 3.45)\n",
      "Rank 384: 'mean' feature #5267 (t = 3.45)\n",
      "Rank 385: 'max' feature #1810 (t = 3.44)\n",
      "Rank 386: 'mean' feature #1336 (t = 3.44)\n",
      "Rank 387: 'max' feature #734 (t = 3.44)\n",
      "Rank 388: 'max' feature #2548 (t = 3.44)\n",
      "Rank 389: 'max' feature #3150 (t = 3.43)\n",
      "Rank 390: 'mean' feature #3484 (t = 3.43)\n",
      "Rank 391: 'mean' feature #7711 (t = 3.43)\n",
      "Rank 392: 'mean' feature #6240 (t = 3.43)\n",
      "Rank 393: 'mean' feature #6281 (t = 3.43)\n",
      "Rank 394: 'mean' feature #7046 (t = 3.43)\n",
      "Rank 395: 'max' feature #1980 (t = 3.43)\n",
      "Rank 396: 'mean' feature #5276 (t = 3.42)\n",
      "Rank 397: 'max' feature #3062 (t = 3.42)\n",
      "Rank 398: 'mean' feature #7853 (t = 3.42)\n",
      "Rank 399: 'mean' feature #631 (t = 3.42)\n",
      "Rank 400: 'mean' feature #931 (t = 3.42)\n",
      "Rank 401: 'max' feature #5425 (t = 3.42)\n",
      "Rank 402: 'max' feature #6444 (t = 3.42)\n",
      "Rank 403: 'mean' feature #1328 (t = 3.42)\n",
      "Rank 404: 'mean' feature #4588 (t = 3.42)\n",
      "Rank 405: 'mean' feature #961 (t = 3.41)\n",
      "Rank 406: 'max' feature #5170 (t = 3.41)\n",
      "Rank 407: 'max' feature #4158 (t = 3.41)\n",
      "Rank 408: 'max' feature #742 (t = 3.41)\n",
      "Rank 409: 'mean' feature #5521 (t = 3.40)\n",
      "Rank 410: 'max' feature #731 (t = 3.40)\n",
      "Rank 411: 'max' feature #6007 (t = 3.40)\n",
      "Rank 412: 'max' feature #1738 (t = 3.39)\n",
      "Rank 413: 'mean' feature #1946 (t = 3.39)\n",
      "Rank 414: 'mean' feature #6615 (t = 3.39)\n",
      "Rank 415: 'mean' feature #6787 (t = 3.39)\n",
      "Rank 416: 'max' feature #2518 (t = 3.39)\n",
      "Rank 417: 'max' feature #1784 (t = 3.38)\n",
      "Rank 418: 'mean' feature #7067 (t = 3.38)\n",
      "Rank 419: 'mean' feature #657 (t = 3.38)\n",
      "Rank 420: 'mean' feature #1340 (t = 3.37)\n",
      "Rank 421: 'mean' feature #8155 (t = 3.37)\n",
      "Rank 422: 'mean' feature #928 (t = 3.37)\n",
      "Rank 423: 'mean' feature #5358 (t = 3.37)\n",
      "Rank 424: 'mean' feature #6211 (t = 3.37)\n",
      "Rank 425: 'mean' feature #88 (t = 3.37)\n",
      "Rank 426: 'mean' feature #6694 (t = 3.36)\n",
      "Rank 427: 'max' feature #3823 (t = 3.36)\n",
      "Rank 428: 'max' feature #3290 (t = 3.35)\n",
      "Rank 429: 'max' feature #6813 (t = 3.35)\n",
      "Rank 430: 'max' feature #2400 (t = 3.35)\n",
      "Rank 431: 'max' feature #4081 (t = 3.34)\n",
      "Rank 432: 'max' feature #7084 (t = 3.34)\n",
      "Rank 433: 'max' feature #3301 (t = 3.34)\n",
      "Rank 434: 'mean' feature #6769 (t = 3.34)\n",
      "Rank 435: 'mean' feature #1064 (t = 3.34)\n",
      "Rank 436: 'mean' feature #8184 (t = 3.33)\n",
      "Rank 437: 'mean' feature #1733 (t = 3.33)\n",
      "Rank 438: 'max' feature #1921 (t = 3.33)\n",
      "Rank 439: 'mean' feature #5845 (t = 3.33)\n",
      "Rank 440: 'mean' feature #6609 (t = 3.33)\n",
      "Rank 441: 'max' feature #1540 (t = 3.32)\n",
      "Rank 442: 'max' feature #4832 (t = 3.32)\n",
      "Rank 443: 'mean' feature #2740 (t = 3.32)\n",
      "Rank 444: 'max' feature #6550 (t = 3.32)\n",
      "Rank 445: 'mean' feature #1156 (t = 3.32)\n",
      "Rank 446: 'mean' feature #1315 (t = 3.32)\n",
      "Rank 447: 'mean' feature #1413 (t = 3.32)\n",
      "Rank 448: 'mean' feature #5405 (t = 3.31)\n",
      "Rank 449: 'max' feature #1927 (t = 3.31)\n",
      "Rank 450: 'mean' feature #4817 (t = 3.31)\n",
      "Rank 451: 'mean' feature #4105 (t = 3.31)\n",
      "Rank 452: 'max' feature #6553 (t = 3.31)\n",
      "Rank 453: 'mean' feature #5523 (t = 3.30)\n",
      "Rank 454: 'mean' feature #5776 (t = 3.30)\n",
      "Rank 455: 'max' feature #3821 (t = 3.30)\n",
      "Rank 456: 'mean' feature #6017 (t = 3.30)\n",
      "Rank 457: 'max' feature #1403 (t = 3.29)\n",
      "Rank 458: 'max' feature #3547 (t = 3.28)\n",
      "Rank 459: 'max' feature #248 (t = 3.28)\n",
      "Rank 460: 'mean' feature #4371 (t = 3.28)\n",
      "Rank 461: 'max' feature #83 (t = 3.28)\n",
      "Rank 462: 'mean' feature #3934 (t = 3.28)\n",
      "Rank 463: 'max' feature #3445 (t = 3.28)\n",
      "Rank 464: 'max' feature #4128 (t = 3.28)\n",
      "Rank 465: 'max' feature #3132 (t = 3.27)\n",
      "Rank 466: 'mean' feature #781 (t = 3.27)\n",
      "Rank 467: 'mean' feature #4453 (t = 3.27)\n",
      "Rank 468: 'max' feature #6245 (t = 3.26)\n",
      "Rank 469: 'mean' feature #6618 (t = 3.26)\n",
      "Rank 470: 'max' feature #6110 (t = 3.26)\n",
      "Rank 471: 'mean' feature #1353 (t = 3.26)\n",
      "Rank 472: 'max' feature #2784 (t = 3.26)\n",
      "Rank 473: 'max' feature #738 (t = 3.25)\n",
      "Rank 474: 'mean' feature #3337 (t = 3.25)\n",
      "Rank 475: 'mean' feature #6470 (t = 3.25)\n",
      "Rank 476: 'mean' feature #8078 (t = 3.25)\n",
      "Rank 477: 'max' feature #223 (t = 3.25)\n",
      "Rank 478: 'max' feature #1495 (t = 3.25)\n",
      "Rank 479: 'max' feature #1601 (t = 3.25)\n",
      "Rank 480: 'mean' feature #7735 (t = 3.25)\n",
      "Rank 481: 'max' feature #2857 (t = 3.25)\n",
      "Rank 482: 'max' feature #136 (t = 3.24)\n",
      "Rank 483: 'max' feature #1452 (t = 3.24)\n",
      "Rank 484: 'mean' feature #4317 (t = 3.24)\n",
      "Rank 485: 'mean' feature #5498 (t = 3.24)\n",
      "Rank 486: 'max' feature #4242 (t = 3.24)\n",
      "Rank 487: 'mean' feature #3719 (t = 3.24)\n",
      "Rank 488: 'mean' feature #5083 (t = 3.24)\n",
      "Rank 489: 'max' feature #425 (t = 3.24)\n",
      "Rank 490: 'max' feature #3451 (t = 3.24)\n",
      "Rank 491: 'max' feature #2635 (t = 3.23)\n",
      "Rank 492: 'max' feature #5032 (t = 3.23)\n",
      "Rank 493: 'max' feature #7009 (t = 3.23)\n",
      "Rank 494: 'mean' feature #2077 (t = 3.23)\n",
      "Rank 495: 'max' feature #6716 (t = 3.22)\n",
      "Rank 496: 'mean' feature #5612 (t = 3.22)\n",
      "Rank 497: 'mean' feature #2092 (t = 3.22)\n",
      "Rank 498: 'mean' feature #2051 (t = 3.22)\n",
      "Rank 499: 'mean' feature #5595 (t = 3.22)\n",
      "Rank 500: 'max' feature #5066 (t = 3.22)\n",
      "Rank 501: 'mean' feature #3644 (t = 3.22)\n",
      "Rank 502: 'mean' feature #3857 (t = 3.22)\n",
      "Rank 503: 'mean' feature #7327 (t = 3.22)\n",
      "Rank 504: 'mean' feature #6976 (t = 3.22)\n",
      "Rank 505: 'mean' feature #7473 (t = 3.21)\n",
      "Rank 506: 'mean' feature #2152 (t = 3.21)\n",
      "Rank 507: 'max' feature #232 (t = 3.21)\n",
      "Rank 508: 'max' feature #5357 (t = 3.21)\n",
      "Rank 509: 'mean' feature #1632 (t = 3.21)\n",
      "Rank 510: 'max' feature #3208 (t = 3.21)\n",
      "Rank 511: 'mean' feature #5013 (t = 3.21)\n",
      "Rank 512: 'mean' feature #4631 (t = 3.20)\n",
      "Rank 513: 'max' feature #7100 (t = 3.20)\n",
      "Rank 514: 'mean' feature #4364 (t = 3.20)\n",
      "Rank 515: 'max' feature #7380 (t = 3.20)\n",
      "Rank 516: 'mean' feature #6884 (t = 3.20)\n",
      "Rank 517: 'max' feature #7005 (t = 3.19)\n",
      "Rank 518: 'mean' feature #2182 (t = 3.19)\n",
      "Rank 519: 'mean' feature #6352 (t = 3.19)\n",
      "Rank 520: 'max' feature #3829 (t = 3.18)\n",
      "Rank 521: 'mean' feature #868 (t = 3.18)\n",
      "Rank 522: 'mean' feature #4605 (t = 3.18)\n",
      "Rank 523: 'mean' feature #5343 (t = 3.18)\n",
      "Rank 524: 'max' feature #4117 (t = 3.17)\n",
      "Rank 525: 'max' feature #3339 (t = 3.17)\n",
      "Rank 526: 'mean' feature #598 (t = 3.17)\n",
      "Rank 527: 'mean' feature #6448 (t = 3.17)\n",
      "Rank 528: 'max' feature #2426 (t = 3.17)\n",
      "Rank 529: 'mean' feature #6811 (t = 3.17)\n",
      "Rank 530: 'max' feature #3207 (t = 3.17)\n",
      "Rank 531: 'max' feature #7786 (t = 3.17)\n",
      "Rank 532: 'max' feature #6349 (t = 3.17)\n",
      "Rank 533: 'mean' feature #3118 (t = 3.17)\n",
      "Rank 534: 'max' feature #5454 (t = 3.17)\n",
      "Rank 535: 'mean' feature #4392 (t = 3.17)\n",
      "Rank 536: 'max' feature #2623 (t = 3.17)\n",
      "Rank 537: 'mean' feature #5273 (t = 3.16)\n",
      "Rank 538: 'max' feature #7243 (t = 3.16)\n",
      "Rank 539: 'max' feature #4372 (t = 3.16)\n",
      "Rank 540: 'max' feature #3416 (t = 3.16)\n",
      "Rank 541: 'mean' feature #692 (t = 3.16)\n",
      "Rank 542: 'mean' feature #6337 (t = 3.16)\n",
      "Rank 543: 'max' feature #6727 (t = 3.16)\n",
      "Rank 544: 'max' feature #6614 (t = 3.16)\n",
      "Rank 545: 'mean' feature #5082 (t = 3.16)\n",
      "Rank 546: 'mean' feature #505 (t = 3.15)\n",
      "Rank 547: 'max' feature #5816 (t = 3.15)\n",
      "Rank 548: 'mean' feature #5404 (t = 3.15)\n",
      "Rank 549: 'mean' feature #50 (t = 3.15)\n",
      "Rank 550: 'max' feature #220 (t = 3.15)\n",
      "Rank 551: 'max' feature #5535 (t = 3.15)\n",
      "Rank 552: 'max' feature #4610 (t = 3.15)\n",
      "Rank 553: 'max' feature #271 (t = 3.14)\n",
      "Rank 554: 'max' feature #4629 (t = 3.14)\n",
      "Rank 555: 'mean' feature #6537 (t = 3.14)\n",
      "Rank 556: 'mean' feature #2041 (t = 3.14)\n",
      "Rank 557: 'mean' feature #2661 (t = 3.14)\n",
      "Rank 558: 'max' feature #33 (t = 3.13)\n",
      "Rank 559: 'max' feature #737 (t = 3.13)\n",
      "Rank 560: 'max' feature #7820 (t = 3.13)\n",
      "Rank 561: 'max' feature #2813 (t = 3.13)\n",
      "Rank 562: 'mean' feature #3500 (t = 3.12)\n",
      "Rank 563: 'max' feature #7728 (t = 3.12)\n",
      "Rank 564: 'mean' feature #4372 (t = 3.12)\n",
      "Rank 565: 'mean' feature #6290 (t = 3.12)\n",
      "Rank 566: 'mean' feature #1578 (t = 3.12)\n",
      "Rank 567: 'max' feature #3117 (t = 3.12)\n",
      "Rank 568: 'max' feature #658 (t = 3.12)\n",
      "Rank 569: 'mean' feature #1298 (t = 3.12)\n",
      "Rank 570: 'max' feature #5110 (t = 3.12)\n",
      "Rank 571: 'mean' feature #5059 (t = 3.12)\n",
      "Rank 572: 'mean' feature #4709 (t = 3.11)\n",
      "Rank 573: 'mean' feature #294 (t = 3.11)\n",
      "Rank 574: 'max' feature #6620 (t = 3.11)\n",
      "Rank 575: 'mean' feature #7732 (t = 3.11)\n",
      "Rank 576: 'mean' feature #8172 (t = 3.11)\n",
      "Rank 577: 'mean' feature #399 (t = 3.11)\n",
      "Rank 578: 'mean' feature #4222 (t = 3.10)\n",
      "Rank 579: 'mean' feature #6717 (t = 3.10)\n",
      "Rank 580: 'max' feature #4400 (t = 3.10)\n",
      "Rank 581: 'max' feature #4808 (t = 3.10)\n",
      "Rank 582: 'max' feature #4594 (t = 3.10)\n",
      "Rank 583: 'max' feature #2230 (t = 3.10)\n",
      "Rank 584: 'mean' feature #1911 (t = 3.10)\n",
      "Rank 585: 'max' feature #4768 (t = 3.10)\n",
      "Rank 586: 'mean' feature #6092 (t = 3.10)\n",
      "Rank 587: 'mean' feature #1023 (t = 3.10)\n",
      "Rank 588: 'mean' feature #981 (t = 3.10)\n",
      "Rank 589: 'max' feature #7400 (t = 3.10)\n",
      "Rank 590: 'max' feature #6067 (t = 3.09)\n",
      "Rank 591: 'mean' feature #6453 (t = 3.09)\n",
      "Rank 592: 'mean' feature #6620 (t = 3.09)\n",
      "Rank 593: 'mean' feature #1599 (t = 3.09)\n",
      "Rank 594: 'max' feature #2877 (t = 3.09)\n",
      "Rank 595: 'mean' feature #1764 (t = 3.09)\n",
      "Rank 596: 'mean' feature #3260 (t = 3.09)\n",
      "Rank 597: 'max' feature #3920 (t = 3.09)\n",
      "Rank 598: 'mean' feature #1082 (t = 3.09)\n",
      "Rank 599: 'max' feature #358 (t = 3.08)\n",
      "Rank 600: 'mean' feature #1819 (t = 3.08)\n",
      "Rank 601: 'max' feature #608 (t = 3.08)\n",
      "Rank 602: 'mean' feature #667 (t = 3.08)\n",
      "Rank 603: 'mean' feature #4007 (t = 3.08)\n",
      "Rank 604: 'max' feature #4854 (t = 3.07)\n",
      "Rank 605: 'max' feature #3311 (t = 3.07)\n",
      "Rank 606: 'mean' feature #581 (t = 3.07)\n",
      "Rank 607: 'max' feature #2700 (t = 3.07)\n",
      "Rank 608: 'mean' feature #3557 (t = 3.07)\n",
      "Rank 609: 'mean' feature #6070 (t = 3.07)\n",
      "Rank 610: 'mean' feature #7394 (t = 3.07)\n",
      "Rank 611: 'max' feature #175 (t = 3.07)\n",
      "Rank 612: 'max' feature #7925 (t = 3.07)\n",
      "Rank 613: 'mean' feature #257 (t = 3.06)\n",
      "Rank 614: 'max' feature #5815 (t = 3.06)\n",
      "Rank 615: 'max' feature #7511 (t = 3.06)\n",
      "Rank 616: 'mean' feature #6058 (t = 3.06)\n",
      "Rank 617: 'mean' feature #7775 (t = 3.06)\n",
      "Rank 618: 'max' feature #4620 (t = 3.06)\n",
      "Rank 619: 'max' feature #5305 (t = 3.06)\n",
      "Rank 620: 'mean' feature #6654 (t = 3.05)\n",
      "Rank 621: 'max' feature #108 (t = 3.05)\n",
      "Rank 622: 'max' feature #6222 (t = 3.05)\n",
      "Rank 623: 'mean' feature #5434 (t = 3.05)\n",
      "Rank 624: 'max' feature #172 (t = 3.05)\n",
      "Rank 625: 'mean' feature #7251 (t = 3.05)\n",
      "Rank 626: 'mean' feature #6225 (t = 3.04)\n",
      "Rank 627: 'mean' feature #6970 (t = 3.04)\n",
      "Rank 628: 'max' feature #2353 (t = 3.04)\n",
      "Rank 629: 'mean' feature #7096 (t = 3.04)\n",
      "Rank 630: 'max' feature #652 (t = 3.04)\n",
      "Rank 631: 'max' feature #7204 (t = 3.04)\n",
      "Rank 632: 'max' feature #5148 (t = 3.04)\n",
      "Rank 633: 'mean' feature #6570 (t = 3.04)\n",
      "Rank 634: 'max' feature #4338 (t = 3.04)\n",
      "Rank 635: 'max' feature #1013 (t = 3.04)\n",
      "Rank 636: 'max' feature #5181 (t = 3.04)\n",
      "Rank 637: 'max' feature #2622 (t = 3.03)\n",
      "Rank 638: 'max' feature #6603 (t = 3.03)\n",
      "Rank 639: 'mean' feature #4573 (t = 3.03)\n",
      "Rank 640: 'mean' feature #4206 (t = 3.02)\n",
      "Rank 641: 'max' feature #6694 (t = 3.02)\n",
      "Rank 642: 'mean' feature #1261 (t = 3.02)\n",
      "Rank 643: 'max' feature #3496 (t = 3.02)\n",
      "Rank 644: 'mean' feature #133 (t = 3.01)\n",
      "Rank 645: 'max' feature #1858 (t = 3.01)\n",
      "Rank 646: 'mean' feature #3170 (t = 3.01)\n",
      "Rank 647: 'mean' feature #1142 (t = 3.01)\n",
      "Rank 648: 'max' feature #2839 (t = 3.01)\n",
      "Rank 649: 'max' feature #406 (t = 3.01)\n",
      "Rank 650: 'mean' feature #4099 (t = 3.01)\n",
      "Rank 651: 'mean' feature #6991 (t = 3.01)\n",
      "Rank 652: 'max' feature #6984 (t = 3.00)\n",
      "Rank 653: 'max' feature #8043 (t = 3.00)\n",
      "Rank 654: 'max' feature #5925 (t = 3.00)\n",
      "Rank 655: 'max' feature #1285 (t = 3.00)\n",
      "Rank 656: 'max' feature #3838 (t = 3.00)\n",
      "Rank 657: 'mean' feature #2195 (t = 3.00)\n",
      "Rank 658: 'max' feature #1897 (t = 3.00)\n",
      "Rank 659: 'max' feature #1801 (t = 3.00)\n",
      "Rank 660: 'mean' feature #4095 (t = 3.00)\n",
      "Rank 661: 'max' feature #7303 (t = 2.99)\n",
      "Rank 662: 'mean' feature #951 (t = 2.99)\n",
      "Rank 663: 'max' feature #5840 (t = 2.99)\n",
      "Rank 664: 'mean' feature #5576 (t = 2.99)\n",
      "Rank 665: 'mean' feature #1518 (t = 2.99)\n",
      "Rank 666: 'max' feature #4106 (t = 2.99)\n",
      "Rank 667: 'mean' feature #1279 (t = 2.98)\n",
      "Rank 668: 'mean' feature #3145 (t = 2.98)\n",
      "Rank 669: 'mean' feature #2442 (t = 2.98)\n",
      "Rank 670: 'max' feature #1733 (t = 2.98)\n",
      "Rank 671: 'max' feature #4666 (t = 2.98)\n",
      "Rank 672: 'max' feature #5634 (t = 2.98)\n",
      "Rank 673: 'max' feature #6933 (t = 2.97)\n",
      "Rank 674: 'mean' feature #6314 (t = 2.97)\n",
      "Rank 675: 'mean' feature #4273 (t = 2.97)\n",
      "Rank 676: 'mean' feature #3933 (t = 2.97)\n",
      "Rank 677: 'mean' feature #5024 (t = 2.97)\n",
      "Rank 678: 'mean' feature #5499 (t = 2.97)\n",
      "Rank 679: 'mean' feature #2672 (t = 2.97)\n",
      "Rank 680: 'max' feature #1379 (t = 2.97)\n",
      "Rank 681: 'max' feature #6944 (t = 2.96)\n",
      "Rank 682: 'max' feature #4890 (t = 2.96)\n",
      "Rank 683: 'mean' feature #1659 (t = 2.96)\n",
      "Rank 684: 'mean' feature #7 (t = 2.96)\n",
      "Rank 685: 'max' feature #6634 (t = 2.96)\n",
      "Rank 686: 'mean' feature #5562 (t = 2.96)\n",
      "Rank 687: 'mean' feature #2527 (t = 2.96)\n",
      "Rank 688: 'mean' feature #6228 (t = 2.96)\n",
      "Rank 689: 'max' feature #1893 (t = 2.96)\n",
      "Rank 690: 'mean' feature #3379 (t = 2.95)\n",
      "Rank 691: 'mean' feature #6605 (t = 2.95)\n",
      "Rank 692: 'mean' feature #6325 (t = 2.95)\n",
      "Rank 693: 'max' feature #1130 (t = 2.95)\n",
      "Rank 694: 'max' feature #5352 (t = 2.95)\n",
      "Rank 695: 'mean' feature #7191 (t = 2.95)\n",
      "Rank 696: 'mean' feature #5489 (t = 2.95)\n",
      "Rank 697: 'mean' feature #7744 (t = 2.95)\n",
      "Rank 698: 'mean' feature #112 (t = 2.95)\n",
      "Rank 699: 'mean' feature #1173 (t = 2.95)\n",
      "Rank 700: 'mean' feature #4977 (t = 2.95)\n",
      "Rank 701: 'max' feature #936 (t = 2.95)\n",
      "Rank 702: 'mean' feature #6893 (t = 2.95)\n",
      "Rank 703: 'max' feature #5356 (t = 2.94)\n",
      "Rank 704: 'max' feature #7320 (t = 2.94)\n",
      "Rank 705: 'max' feature #3133 (t = 2.94)\n",
      "Rank 706: 'max' feature #6048 (t = 2.94)\n",
      "Rank 707: 'max' feature #3064 (t = 2.94)\n",
      "Rank 708: 'mean' feature #1903 (t = 2.93)\n",
      "Rank 709: 'max' feature #1945 (t = 2.93)\n",
      "Rank 710: 'max' feature #3541 (t = 2.93)\n",
      "Rank 711: 'max' feature #3269 (t = 2.93)\n",
      "Rank 712: 'max' feature #900 (t = 2.93)\n",
      "Rank 713: 'mean' feature #245 (t = 2.93)\n",
      "Rank 714: 'mean' feature #3786 (t = 2.93)\n",
      "Rank 715: 'mean' feature #6954 (t = 2.93)\n",
      "Rank 716: 'mean' feature #2445 (t = 2.93)\n",
      "Rank 717: 'mean' feature #2021 (t = 2.93)\n",
      "Rank 718: 'mean' feature #5145 (t = 2.92)\n",
      "Rank 719: 'mean' feature #8092 (t = 2.92)\n",
      "Rank 720: 'max' feature #5310 (t = 2.92)\n",
      "Rank 721: 'max' feature #6458 (t = 2.92)\n",
      "Rank 722: 'mean' feature #3700 (t = 2.92)\n",
      "Rank 723: 'max' feature #1184 (t = 2.92)\n",
      "Rank 724: 'max' feature #4607 (t = 2.92)\n",
      "Rank 725: 'mean' feature #5433 (t = 2.91)\n",
      "Rank 726: 'max' feature #667 (t = 2.91)\n",
      "Rank 727: 'max' feature #2517 (t = 2.91)\n",
      "Rank 728: 'max' feature #3085 (t = 2.91)\n",
      "Rank 729: 'mean' feature #5553 (t = 2.91)\n",
      "Rank 730: 'mean' feature #3285 (t = 2.91)\n",
      "Rank 731: 'max' feature #5071 (t = 2.91)\n",
      "Rank 732: 'max' feature #2417 (t = 2.91)\n",
      "Rank 733: 'max' feature #5401 (t = 2.91)\n",
      "Rank 734: 'max' feature #6628 (t = 2.90)\n",
      "Rank 735: 'mean' feature #4730 (t = 2.90)\n",
      "Rank 736: 'mean' feature #2265 (t = 2.90)\n",
      "Rank 737: 'max' feature #5060 (t = 2.90)\n",
      "Rank 738: 'max' feature #2859 (t = 2.90)\n",
      "Rank 739: 'mean' feature #2215 (t = 2.90)\n",
      "Rank 740: 'mean' feature #2943 (t = 2.90)\n",
      "Rank 741: 'mean' feature #3234 (t = 2.90)\n",
      "Rank 742: 'mean' feature #2100 (t = 2.89)\n",
      "Rank 743: 'max' feature #6560 (t = 2.89)\n",
      "Rank 744: 'mean' feature #1124 (t = 2.89)\n",
      "Rank 745: 'max' feature #4227 (t = 2.89)\n",
      "Rank 746: 'mean' feature #1664 (t = 2.89)\n",
      "Rank 747: 'mean' feature #4546 (t = 2.89)\n",
      "Rank 748: 'max' feature #2500 (t = 2.89)\n",
      "Rank 749: 'mean' feature #2760 (t = 2.89)\n",
      "Rank 750: 'mean' feature #3762 (t = 2.89)\n",
      "Rank 751: 'max' feature #5556 (t = 2.89)\n",
      "Rank 752: 'mean' feature #5006 (t = 2.89)\n",
      "Rank 753: 'mean' feature #4948 (t = 2.89)\n",
      "Rank 754: 'mean' feature #1704 (t = 2.89)\n",
      "Rank 755: 'max' feature #1273 (t = 2.89)\n",
      "Rank 756: 'max' feature #7555 (t = 2.88)\n",
      "Rank 757: 'mean' feature #4187 (t = 2.88)\n",
      "Rank 758: 'max' feature #3026 (t = 2.88)\n",
      "Rank 759: 'max' feature #4585 (t = 2.88)\n",
      "Rank 760: 'mean' feature #1334 (t = 2.88)\n",
      "Rank 761: 'mean' feature #184 (t = 2.88)\n",
      "Rank 762: 'max' feature #5878 (t = 2.88)\n",
      "Rank 763: 'mean' feature #8164 (t = 2.88)\n",
      "Rank 764: 'mean' feature #1803 (t = 2.87)\n",
      "Rank 765: 'max' feature #1258 (t = 2.87)\n",
      "Rank 766: 'mean' feature #1189 (t = 2.87)\n",
      "Rank 767: 'mean' feature #2010 (t = 2.87)\n",
      "Rank 768: 'mean' feature #4641 (t = 2.87)\n",
      "Rank 769: 'max' feature #2255 (t = 2.87)\n",
      "Rank 770: 'max' feature #2865 (t = 2.87)\n",
      "Rank 771: 'mean' feature #5218 (t = 2.87)\n",
      "Rank 772: 'max' feature #5178 (t = 2.87)\n",
      "Rank 773: 'max' feature #3616 (t = 2.86)\n",
      "Rank 774: 'max' feature #52 (t = 2.86)\n",
      "Rank 775: 'mean' feature #5859 (t = 2.86)\n",
      "Rank 776: 'max' feature #6627 (t = 2.86)\n",
      "Rank 777: 'max' feature #5997 (t = 2.86)\n",
      "Rank 778: 'max' feature #3234 (t = 2.86)\n",
      "Rank 779: 'mean' feature #7006 (t = 2.86)\n",
      "Rank 780: 'mean' feature #6720 (t = 2.85)\n",
      "Rank 781: 'max' feature #982 (t = 2.85)\n",
      "Rank 782: 'max' feature #4977 (t = 2.85)\n",
      "Rank 783: 'mean' feature #7593 (t = 2.85)\n",
      "Rank 784: 'mean' feature #3575 (t = 2.85)\n",
      "Rank 785: 'mean' feature #3417 (t = 2.85)\n",
      "Rank 786: 'mean' feature #1390 (t = 2.85)\n",
      "Rank 787: 'mean' feature #5539 (t = 2.85)\n",
      "Rank 788: 'max' feature #799 (t = 2.85)\n",
      "Rank 789: 'max' feature #2702 (t = 2.85)\n",
      "Rank 790: 'max' feature #4000 (t = 2.85)\n",
      "Rank 791: 'mean' feature #4384 (t = 2.85)\n",
      "Rank 792: 'mean' feature #2583 (t = 2.84)\n",
      "Rank 793: 'mean' feature #1411 (t = 2.84)\n",
      "Rank 794: 'max' feature #1391 (t = 2.84)\n",
      "Rank 795: 'max' feature #6364 (t = 2.84)\n",
      "Rank 796: 'max' feature #4985 (t = 2.84)\n",
      "Rank 797: 'max' feature #2741 (t = 2.84)\n",
      "Rank 798: 'mean' feature #4511 (t = 2.84)\n",
      "Rank 799: 'mean' feature #5517 (t = 2.84)\n",
      "Rank 800: 'max' feature #3048 (t = 2.84)\n",
      "Rank 801: 'max' feature #2507 (t = 2.84)\n",
      "Rank 802: 'mean' feature #284 (t = 2.84)\n",
      "Rank 803: 'max' feature #5229 (t = 2.83)\n",
      "Rank 804: 'max' feature #159 (t = 2.83)\n",
      "Rank 805: 'mean' feature #5354 (t = 2.83)\n",
      "Rank 806: 'mean' feature #1417 (t = 2.83)\n",
      "Rank 807: 'mean' feature #4452 (t = 2.83)\n",
      "Rank 808: 'max' feature #167 (t = 2.83)\n",
      "Rank 809: 'max' feature #6490 (t = 2.83)\n",
      "Rank 810: 'max' feature #7631 (t = 2.83)\n",
      "Rank 811: 'max' feature #5438 (t = 2.83)\n",
      "Rank 812: 'mean' feature #7691 (t = 2.83)\n",
      "Rank 813: 'max' feature #3911 (t = 2.83)\n",
      "Rank 814: 'max' feature #6194 (t = 2.83)\n",
      "Rank 815: 'mean' feature #2393 (t = 2.82)\n",
      "Rank 816: 'mean' feature #6254 (t = 2.82)\n",
      "Rank 817: 'mean' feature #2969 (t = 2.82)\n",
      "Rank 818: 'mean' feature #6650 (t = 2.82)\n",
      "Rank 819: 'max' feature #4441 (t = 2.82)\n",
      "Rank 820: 'max' feature #7332 (t = 2.82)\n",
      "Rank 821: 'mean' feature #1530 (t = 2.82)\n",
      "Rank 822: 'mean' feature #3375 (t = 2.82)\n",
      "Rank 823: 'max' feature #1298 (t = 2.82)\n",
      "Rank 824: 'max' feature #2591 (t = 2.82)\n",
      "Rank 825: 'mean' feature #1859 (t = 2.82)\n",
      "Rank 826: 'mean' feature #1649 (t = 2.82)\n",
      "Rank 827: 'mean' feature #4558 (t = 2.81)\n",
      "Rank 828: 'mean' feature #5652 (t = 2.81)\n",
      "Rank 829: 'max' feature #6381 (t = 2.81)\n",
      "Rank 830: 'max' feature #3856 (t = 2.81)\n",
      "Rank 831: 'max' feature #7544 (t = 2.81)\n",
      "Rank 832: 'mean' feature #1915 (t = 2.81)\n",
      "Rank 833: 'max' feature #2919 (t = 2.81)\n",
      "Rank 834: 'mean' feature #6571 (t = 2.81)\n",
      "Rank 835: 'max' feature #2909 (t = 2.81)\n",
      "Rank 836: 'max' feature #212 (t = 2.81)\n",
      "Rank 837: 'mean' feature #1278 (t = 2.80)\n",
      "Rank 838: 'mean' feature #2752 (t = 2.80)\n",
      "Rank 839: 'mean' feature #3834 (t = 2.80)\n",
      "Rank 840: 'max' feature #417 (t = 2.80)\n",
      "Rank 841: 'max' feature #4361 (t = 2.80)\n",
      "Rank 842: 'max' feature #3442 (t = 2.80)\n",
      "Rank 843: 'mean' feature #5310 (t = 2.80)\n",
      "Rank 844: 'mean' feature #809 (t = 2.80)\n",
      "Rank 845: 'mean' feature #1653 (t = 2.80)\n",
      "Rank 846: 'max' feature #354 (t = 2.79)\n",
      "Rank 847: 'mean' feature #4307 (t = 2.79)\n",
      "Rank 848: 'mean' feature #1 (t = 2.79)\n",
      "Rank 849: 'mean' feature #5041 (t = 2.79)\n",
      "Rank 850: 'max' feature #3081 (t = 2.79)\n",
      "Rank 851: 'mean' feature #7710 (t = 2.79)\n",
      "Rank 852: 'max' feature #3441 (t = 2.79)\n",
      "Rank 853: 'max' feature #3873 (t = 2.79)\n",
      "Rank 854: 'max' feature #5781 (t = 2.79)\n",
      "Rank 855: 'mean' feature #3213 (t = 2.78)\n",
      "Rank 856: 'mean' feature #5454 (t = 2.78)\n",
      "Rank 857: 'max' feature #4812 (t = 2.78)\n",
      "Rank 858: 'max' feature #4388 (t = 2.78)\n",
      "Rank 859: 'mean' feature #3070 (t = 2.78)\n",
      "Rank 860: 'mean' feature #4608 (t = 2.78)\n",
      "Rank 861: 'mean' feature #4560 (t = 2.78)\n",
      "Rank 862: 'max' feature #7340 (t = 2.77)\n",
      "Rank 863: 'mean' feature #5245 (t = 2.77)\n",
      "Rank 864: 'mean' feature #3788 (t = 2.77)\n",
      "Rank 865: 'max' feature #6887 (t = 2.77)\n",
      "Rank 866: 'mean' feature #2539 (t = 2.77)\n",
      "Rank 867: 'max' feature #4852 (t = 2.77)\n",
      "Rank 868: 'max' feature #3876 (t = 2.77)\n",
      "Rank 869: 'max' feature #4415 (t = 2.77)\n",
      "Rank 870: 'mean' feature #482 (t = 2.77)\n",
      "Rank 871: 'max' feature #6270 (t = 2.77)\n",
      "Rank 872: 'mean' feature #4019 (t = 2.77)\n",
      "Rank 873: 'mean' feature #827 (t = 2.77)\n",
      "Rank 874: 'max' feature #6619 (t = 2.77)\n",
      "Rank 875: 'max' feature #5276 (t = 2.77)\n",
      "Rank 876: 'max' feature #2986 (t = 2.76)\n",
      "Rank 877: 'mean' feature #1604 (t = 2.76)\n",
      "Rank 878: 'mean' feature #4035 (t = 2.76)\n",
      "Rank 879: 'max' feature #1119 (t = 2.76)\n",
      "Rank 880: 'max' feature #4090 (t = 2.76)\n",
      "Rank 881: 'mean' feature #4651 (t = 2.76)\n",
      "Rank 882: 'max' feature #4731 (t = 2.75)\n",
      "Rank 883: 'mean' feature #6220 (t = 2.75)\n",
      "Rank 884: 'max' feature #3609 (t = 2.75)\n",
      "Rank 885: 'max' feature #2034 (t = 2.75)\n",
      "Rank 886: 'max' feature #5716 (t = 2.75)\n",
      "Rank 887: 'max' feature #5279 (t = 2.75)\n",
      "Rank 888: 'mean' feature #7822 (t = 2.75)\n",
      "Rank 889: 'mean' feature #1296 (t = 2.75)\n",
      "Rank 890: 'max' feature #3272 (t = 2.75)\n",
      "Rank 891: 'mean' feature #1144 (t = 2.75)\n",
      "Rank 892: 'max' feature #2987 (t = 2.75)\n",
      "Rank 893: 'mean' feature #916 (t = 2.74)\n",
      "Rank 894: 'max' feature #1925 (t = 2.74)\n",
      "Rank 895: 'max' feature #5023 (t = 2.74)\n",
      "Rank 896: 'max' feature #6399 (t = 2.74)\n",
      "Rank 897: 'mean' feature #1251 (t = 2.74)\n",
      "Rank 898: 'max' feature #7066 (t = 2.74)\n",
      "Rank 899: 'mean' feature #3330 (t = 2.74)\n",
      "Rank 900: 'mean' feature #2617 (t = 2.74)\n",
      "Rank 901: 'mean' feature #6175 (t = 2.74)\n",
      "Rank 902: 'mean' feature #5064 (t = 2.73)\n",
      "Rank 903: 'max' feature #7901 (t = 2.73)\n",
      "Rank 904: 'mean' feature #1705 (t = 2.73)\n",
      "Rank 905: 'mean' feature #4423 (t = 2.73)\n",
      "Rank 906: 'mean' feature #5144 (t = 2.73)\n",
      "Rank 907: 'mean' feature #7620 (t = 2.73)\n",
      "Rank 908: 'max' feature #4838 (t = 2.73)\n",
      "Rank 909: 'mean' feature #3681 (t = 2.73)\n",
      "Rank 910: 'max' feature #5173 (t = 2.73)\n",
      "Rank 911: 'mean' feature #4760 (t = 2.73)\n",
      "Rank 912: 'max' feature #118 (t = 2.73)\n",
      "Rank 913: 'mean' feature #2789 (t = 2.73)\n",
      "Rank 914: 'mean' feature #1679 (t = 2.72)\n",
      "Rank 915: 'mean' feature #4926 (t = 2.72)\n",
      "Rank 916: 'mean' feature #229 (t = 2.72)\n",
      "Rank 917: 'max' feature #825 (t = 2.72)\n",
      "Rank 918: 'max' feature #6639 (t = 2.72)\n",
      "Rank 919: 'mean' feature #6397 (t = 2.72)\n",
      "Rank 920: 'max' feature #7261 (t = 2.72)\n",
      "Rank 921: 'mean' feature #6198 (t = 2.72)\n",
      "Rank 922: 'mean' feature #6690 (t = 2.72)\n",
      "Rank 923: 'mean' feature #4482 (t = 2.72)\n",
      "Rank 924: 'max' feature #3933 (t = 2.71)\n",
      "Rank 925: 'max' feature #2643 (t = 2.71)\n",
      "Rank 926: 'mean' feature #2587 (t = 2.71)\n",
      "Rank 927: 'mean' feature #940 (t = 2.71)\n",
      "Rank 928: 'max' feature #5478 (t = 2.71)\n",
      "Rank 929: 'mean' feature #8102 (t = 2.71)\n",
      "Rank 930: 'mean' feature #1542 (t = 2.71)\n",
      "Rank 931: 'max' feature #5933 (t = 2.71)\n",
      "Rank 932: 'mean' feature #5883 (t = 2.71)\n",
      "Rank 933: 'mean' feature #2484 (t = 2.71)\n",
      "Rank 934: 'mean' feature #1967 (t = 2.71)\n",
      "Rank 935: 'mean' feature #7504 (t = 2.71)\n",
      "Rank 936: 'max' feature #7278 (t = 2.71)\n",
      "Rank 937: 'max' feature #1567 (t = 2.71)\n",
      "Rank 938: 'mean' feature #8097 (t = 2.71)\n",
      "Rank 939: 'mean' feature #1260 (t = 2.71)\n",
      "Rank 940: 'max' feature #653 (t = 2.71)\n",
      "Rank 941: 'max' feature #3585 (t = 2.70)\n",
      "Rank 942: 'max' feature #2696 (t = 2.70)\n",
      "Rank 943: 'max' feature #1078 (t = 2.70)\n",
      "Rank 944: 'mean' feature #1692 (t = 2.70)\n",
      "Rank 945: 'max' feature #7497 (t = 2.70)\n",
      "Rank 946: 'mean' feature #6250 (t = 2.70)\n",
      "Rank 947: 'mean' feature #1862 (t = 2.69)\n",
      "Rank 948: 'mean' feature #2081 (t = 2.69)\n",
      "Rank 949: 'max' feature #4042 (t = 2.69)\n",
      "Rank 950: 'max' feature #2140 (t = 2.69)\n",
      "Rank 951: 'mean' feature #2389 (t = 2.69)\n",
      "Rank 952: 'max' feature #3641 (t = 2.69)\n",
      "Rank 953: 'max' feature #6474 (t = 2.69)\n",
      "Rank 954: 'mean' feature #3287 (t = 2.69)\n",
      "Rank 955: 'max' feature #4805 (t = 2.69)\n",
      "Rank 956: 'mean' feature #1952 (t = 2.69)\n",
      "Rank 957: 'max' feature #1038 (t = 2.69)\n",
      "Rank 958: 'mean' feature #5097 (t = 2.69)\n",
      "Rank 959: 'mean' feature #1112 (t = 2.69)\n",
      "Rank 960: 'max' feature #3943 (t = 2.68)\n",
      "Rank 961: 'mean' feature #1929 (t = 2.68)\n",
      "Rank 962: 'max' feature #6623 (t = 2.68)\n",
      "Rank 963: 'max' feature #7475 (t = 2.68)\n",
      "Rank 964: 'max' feature #4222 (t = 2.68)\n",
      "Rank 965: 'max' feature #3929 (t = 2.68)\n",
      "Rank 966: 'max' feature #7785 (t = 2.68)\n",
      "Rank 967: 'mean' feature #5247 (t = 2.68)\n",
      "Rank 968: 'mean' feature #2325 (t = 2.68)\n",
      "Rank 969: 'max' feature #4427 (t = 2.68)\n",
      "Rank 970: 'mean' feature #723 (t = 2.68)\n",
      "Rank 971: 'max' feature #926 (t = 2.67)\n",
      "Rank 972: 'mean' feature #1974 (t = 2.67)\n",
      "Rank 973: 'mean' feature #2998 (t = 2.67)\n",
      "Rank 974: 'max' feature #5004 (t = 2.67)\n",
      "Rank 975: 'max' feature #5024 (t = 2.67)\n",
      "Rank 976: 'max' feature #5861 (t = 2.67)\n",
      "Rank 977: 'mean' feature #4505 (t = 2.66)\n",
      "Rank 978: 'max' feature #8013 (t = 2.66)\n",
      "Rank 979: 'mean' feature #5687 (t = 2.66)\n",
      "Rank 980: 'mean' feature #4540 (t = 2.66)\n",
      "Rank 981: 'max' feature #4036 (t = 2.65)\n",
      "Rank 982: 'mean' feature #2574 (t = 2.65)\n",
      "Rank 983: 'mean' feature #5493 (t = 2.65)\n",
      "Rank 984: 'mean' feature #1412 (t = 2.65)\n",
      "Rank 985: 'max' feature #3953 (t = 2.65)\n",
      "Rank 986: 'max' feature #4502 (t = 2.65)\n",
      "Rank 987: 'mean' feature #3438 (t = 2.65)\n",
      "Rank 988: 'mean' feature #6504 (t = 2.65)\n",
      "Rank 989: 'mean' feature #4707 (t = 2.65)\n",
      "Rank 990: 'mean' feature #216 (t = 2.65)\n",
      "Rank 991: 'mean' feature #5002 (t = 2.65)\n",
      "Rank 992: 'mean' feature #323 (t = 2.65)\n",
      "Rank 993: 'mean' feature #67 (t = 2.65)\n",
      "Rank 994: 'max' feature #2940 (t = 2.65)\n",
      "Rank 995: 'mean' feature #4844 (t = 2.65)\n",
      "Rank 996: 'max' feature #7929 (t = 2.65)\n",
      "Rank 997: 'max' feature #1641 (t = 2.64)\n",
      "Rank 998: 'max' feature #4130 (t = 2.64)\n",
      "Rank 999: 'mean' feature #187 (t = 2.64)\n",
      "Rank 1000: 'mean' feature #5124 (t = 2.64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33543/4004282485.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  t_stats = np.abs((X_pos.mean(0) - X_neg.mean(0)) /\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# meta = meta.assign(label=lambda df: df.SUESCORE.map(lambda s: 1 if s>=0.5 else (0 if s<=-0.5 else np.nan)))\n",
    "# mask = meta.label.notna().values\n",
    "# Xc, y = Xc_aligned[mask], meta.loc[mask, \"label\"].astype(int).values\n",
    "\n",
    "D2 = Xc.shape[1]\n",
    "D = D2 // 2\n",
    "X_pos, X_neg = Xc[y==1], Xc[y==0]\n",
    "t_stats = np.abs((X_pos.mean(0) - X_neg.mean(0)) /\n",
    "                 np.sqrt(X_pos.var(0)/len(X_pos) + X_neg.var(0)/len(X_neg)))\n",
    "\n",
    "ranked_idx = np.argsort(-t_stats)\n",
    "\n",
    "\n",
    "for rank, idx in enumerate(ranked_idx[:1000], start=1):\n",
    "    # print(idx)\n",
    "    part = \"mean\" if idx < D else \"max\"\n",
    "    # print(idx)\n",
    "    # print(D)\n",
    "    feat_id = idx if idx < D else idx-D\n",
    "    t_val   = t_stats[idx]\n",
    "    print(f\"Rank {rank:2d}: {part!r} feature #{feat_id} (t = {t_val:.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd0a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_idx = ranked_idx[:1000]\n",
    "# X_test = X_test_all_feat[:, top_idx]\n",
    "# X_val = X_val_all_feat[:, top_idx]\n",
    "# X_top = Xc[:, top_idx]      \n",
    "\n",
    "# X_train = X_top\n",
    "# y_train = y\n",
    "\n",
    "\n",
    "X_test = X_test_all_feat[:, :]\n",
    "X_val = X_val_all_feat[:, :]\n",
    "X_top = Xc[:, :]      \n",
    "\n",
    "X_train = X_top\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7db7e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2449, 16384)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97c93d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airlay88/surprise_sae/venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       504\n",
      "           1       0.56      0.57      0.56       504\n",
      "\n",
      "    accuracy                           0.56      1008\n",
      "   macro avg       0.56      0.56      0.56      1008\n",
      "weighted avg       0.56      0.56      0.56      1008\n",
      "\n",
      "ROC AUC: 0.5706294091710759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airlay88/surprise_sae/venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5) Train with L1 logistic regression & balanced class weights\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        # class_weight=\"balanced\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6) Evaluate\n",
    "y_pred   = clf.predict(X_test)\n",
    "y_probs  = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_probs))\n",
    "\n",
    "# 7) Inspect which of your top-1000 actually got nonzero weights\n",
    "lr = clf.named_steps[\"logisticregression\"]\n",
    "coefs = lr.coef_.ravel()\n",
    "nz    = np.where(coefs != 0)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dfaa751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best C (inverse reg. strength): 0.01\n",
      "CV ROC AUC: 0.791094758876122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.20      0.25       151\n",
      "           1       0.80      0.89      0.84       551\n",
      "\n",
      "    accuracy                           0.74       702\n",
      "   macro avg       0.56      0.54      0.54       702\n",
      "weighted avg       0.70      0.74      0.71       702\n",
      "\n",
      "Test ROC AUC: 0.5483828319371161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",    \n",
    "        # solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=8000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e77399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        # solver=\"saga\",    \n",
    "        solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=7000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3c21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2516dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        # solver=\"saga\",    \n",
    "        solver=\"liblinear\",    \n",
    "        # class_weight=\"balanced\",\n",
    "        max_iter=7000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best C (inverse reg. strength):\", search.best_params_[\"logisticregression__C\"])\n",
    "print(\"CV ROC AUC:\", search.best_score_)\n",
    "\n",
    "\n",
    "best_clf = search.best_estimator_\n",
    "y_pred_probs = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred       = best_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01459b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train loss = 0.7071\n",
      "Epoch  2: train loss = 0.6726\n",
      "Epoch  3: train loss = 0.6493\n",
      "Epoch  4: train loss = 0.6274\n",
      "Epoch  5: train loss = 0.6057\n",
      "Epoch  6: train loss = 0.5875\n",
      "Epoch  7: train loss = 0.5723\n",
      "Epoch  8: train loss = 0.5577\n",
      "Epoch  9: train loss = 0.5514\n",
      "Epoch 10: train loss = 0.5440\n",
      "Epoch 11: train loss = 0.5296\n",
      "Epoch 12: train loss = 0.5217\n",
      "Epoch 13: train loss = 0.5093\n",
      "Epoch 14: train loss = 0.5048\n",
      "Epoch 15: train loss = 0.4923\n",
      "Epoch 16: train loss = 0.4870\n",
      "Epoch 17: train loss = 0.4762\n",
      "Epoch 18: train loss = 0.4719\n",
      "Epoch 19: train loss = 0.4676\n",
      "Epoch 20: train loss = 0.4646\n",
      "Epoch 21: train loss = 0.4553\n",
      "Epoch 22: train loss = 0.4460\n",
      "Epoch 23: train loss = 0.4459\n",
      "Epoch 24: train loss = 0.4378\n",
      "Epoch 25: train loss = 0.4408\n",
      "Epoch 26: train loss = 0.4319\n",
      "Epoch 27: train loss = 0.4293\n",
      "Epoch 28: train loss = 0.4262\n",
      "Epoch 29: train loss = 0.4236\n",
      "Epoch 30: train loss = 0.4166\n",
      "Epoch 31: train loss = 0.4160\n",
      "Epoch 32: train loss = 0.4065\n",
      "Epoch 33: train loss = 0.4002\n",
      "Epoch 34: train loss = 0.4047\n",
      "Epoch 35: train loss = 0.4079\n",
      "Epoch 36: train loss = 0.3984\n",
      "Epoch 37: train loss = 0.3934\n",
      "Epoch 38: train loss = 0.3941\n",
      "Epoch 39: train loss = 0.3886\n",
      "Epoch 40: train loss = 0.3943\n",
      "Epoch 41: train loss = 0.3881\n",
      "Epoch 42: train loss = 0.3841\n",
      "Epoch 43: train loss = 0.3798\n",
      "Epoch 44: train loss = 0.3764\n",
      "Epoch 45: train loss = 0.3724\n",
      "Epoch 46: train loss = 0.3759\n",
      "Epoch 47: train loss = 0.3745\n",
      "Epoch 48: train loss = 0.3680\n",
      "Epoch 49: train loss = 0.3634\n",
      "Epoch 50: train loss = 0.3609\n",
      "Epoch 51: train loss = 0.3605\n",
      "Epoch 52: train loss = 0.3594\n",
      "Epoch 53: train loss = 0.3542\n",
      "Epoch 54: train loss = 0.3551\n",
      "Epoch 55: train loss = 0.3514\n",
      "Epoch 56: train loss = 0.3432\n",
      "Epoch 57: train loss = 0.3463\n",
      "Epoch 58: train loss = 0.3383\n",
      "Epoch 59: train loss = 0.3384\n",
      "Epoch 60: train loss = 0.3419\n",
      "Epoch 61: train loss = 0.3348\n",
      "Epoch 62: train loss = 0.3304\n",
      "Epoch 63: train loss = 0.3388\n",
      "Epoch 64: train loss = 0.3248\n",
      "Epoch 65: train loss = 0.3231\n",
      "Epoch 66: train loss = 0.3219\n",
      "Epoch 67: train loss = 0.3255\n",
      "Epoch 68: train loss = 0.3269\n",
      "Epoch 69: train loss = 0.3150\n",
      "Epoch 70: train loss = 0.3173\n",
      "Epoch 71: train loss = 0.3122\n",
      "Epoch 72: train loss = 0.3100\n",
      "Epoch 73: train loss = 0.3091\n",
      "Epoch 74: train loss = 0.3071\n",
      "Epoch 75: train loss = 0.3084\n",
      "Epoch 76: train loss = 0.2991\n",
      "Epoch 77: train loss = 0.2972\n",
      "Epoch 78: train loss = 0.2981\n",
      "Epoch 79: train loss = 0.2960\n",
      "Epoch 80: train loss = 0.2946\n",
      "Epoch 81: train loss = 0.2957\n",
      "Epoch 82: train loss = 0.2859\n",
      "Epoch 83: train loss = 0.2897\n",
      "Epoch 84: train loss = 0.2856\n",
      "Epoch 85: train loss = 0.2837\n",
      "Epoch 86: train loss = 0.2843\n",
      "Epoch 87: train loss = 0.2794\n",
      "Epoch 88: train loss = 0.2755\n",
      "Epoch 89: train loss = 0.2762\n",
      "Epoch 90: train loss = 0.2691\n",
      "Epoch 91: train loss = 0.2716\n",
      "Epoch 92: train loss = 0.2668\n",
      "Epoch 93: train loss = 0.2662\n",
      "Epoch 94: train loss = 0.2696\n",
      "Epoch 95: train loss = 0.2576\n",
      "Epoch 96: train loss = 0.2599\n",
      "Epoch 97: train loss = 0.2610\n",
      "Epoch 98: train loss = 0.2591\n",
      "Epoch 99: train loss = 0.2515\n",
      "Epoch 100: train loss = 0.2569\n",
      "Epoch 101: train loss = 0.2485\n",
      "Epoch 102: train loss = 0.2490\n",
      "Epoch 103: train loss = 0.2449\n",
      "Epoch 104: train loss = 0.2467\n",
      "Epoch 105: train loss = 0.2418\n",
      "Epoch 106: train loss = 0.2481\n",
      "Epoch 107: train loss = 0.2379\n",
      "Epoch 108: train loss = 0.2400\n",
      "Epoch 109: train loss = 0.2409\n",
      "Epoch 110: train loss = 0.2408\n",
      "Epoch 111: train loss = 0.2326\n",
      "Epoch 112: train loss = 0.2308\n",
      "Epoch 113: train loss = 0.2314\n",
      "Epoch 114: train loss = 0.2309\n",
      "Epoch 115: train loss = 0.2270\n",
      "Epoch 116: train loss = 0.2301\n",
      "Epoch 117: train loss = 0.2267\n",
      "Epoch 118: train loss = 0.2218\n",
      "Epoch 119: train loss = 0.2280\n",
      "Epoch 120: train loss = 0.2227\n",
      "Epoch 121: train loss = 0.2196\n",
      "Epoch 122: train loss = 0.2146\n",
      "Epoch 123: train loss = 0.2151\n",
      "Epoch 124: train loss = 0.2143\n",
      "Epoch 125: train loss = 0.2163\n",
      "Epoch 126: train loss = 0.2057\n",
      "Epoch 127: train loss = 0.2100\n",
      "Epoch 128: train loss = 0.2053\n",
      "Epoch 129: train loss = 0.2029\n",
      "Epoch 130: train loss = 0.2052\n",
      "Epoch 131: train loss = 0.2018\n",
      "Epoch 132: train loss = 0.2002\n",
      "Epoch 133: train loss = 0.2012\n",
      "Epoch 134: train loss = 0.1943\n",
      "Epoch 135: train loss = 0.1998\n",
      "Epoch 136: train loss = 0.1920\n",
      "Epoch 137: train loss = 0.1891\n",
      "Epoch 138: train loss = 0.1951\n",
      "Epoch 139: train loss = 0.1888\n",
      "Epoch 140: train loss = 0.1878\n",
      "Epoch 141: train loss = 0.1884\n",
      "Epoch 142: train loss = 0.1874\n",
      "Epoch 143: train loss = 0.1821\n",
      "Epoch 144: train loss = 0.1827\n",
      "Epoch 145: train loss = 0.1858\n",
      "Epoch 146: train loss = 0.1816\n",
      "Epoch 147: train loss = 0.1815\n",
      "Epoch 148: train loss = 0.1796\n",
      "Epoch 149: train loss = 0.1717\n",
      "Epoch 150: train loss = 0.1689\n",
      "Epoch 151: train loss = 0.1718\n",
      "Epoch 152: train loss = 0.1705\n",
      "Epoch 153: train loss = 0.1719\n",
      "Epoch 154: train loss = 0.1666\n",
      "Epoch 155: train loss = 0.1631\n",
      "Epoch 156: train loss = 0.1657\n",
      "Epoch 157: train loss = 0.1655\n",
      "Epoch 158: train loss = 0.1619\n",
      "Epoch 159: train loss = 0.1644\n",
      "Epoch 160: train loss = 0.1554\n",
      "Epoch 161: train loss = 0.1598\n",
      "Epoch 162: train loss = 0.1607\n",
      "Epoch 163: train loss = 0.1594\n",
      "Epoch 164: train loss = 0.1577\n",
      "Epoch 165: train loss = 0.1499\n",
      "Epoch 166: train loss = 0.1498\n",
      "Epoch 167: train loss = 0.1472\n",
      "Epoch 168: train loss = 0.1453\n",
      "Epoch 169: train loss = 0.1507\n",
      "Epoch 170: train loss = 0.1469\n",
      "Epoch 171: train loss = 0.1434\n",
      "Epoch 172: train loss = 0.1460\n",
      "Epoch 173: train loss = 0.1453\n",
      "Epoch 174: train loss = 0.1425\n",
      "Epoch 175: train loss = 0.1406\n",
      "Epoch 176: train loss = 0.1448\n",
      "Epoch 177: train loss = 0.1376\n",
      "Epoch 178: train loss = 0.1359\n",
      "Epoch 179: train loss = 0.1441\n",
      "Epoch 180: train loss = 0.1396\n",
      "Epoch 181: train loss = 0.1421\n",
      "Epoch 182: train loss = 0.1379\n",
      "Epoch 183: train loss = 0.1416\n",
      "Epoch 184: train loss = 0.1323\n",
      "Epoch 185: train loss = 0.1319\n",
      "Epoch 186: train loss = 0.1296\n",
      "Epoch 187: train loss = 0.1313\n",
      "Epoch 188: train loss = 0.1309\n",
      "Epoch 189: train loss = 0.1304\n",
      "Epoch 190: train loss = 0.1287\n",
      "Epoch 191: train loss = 0.1297\n",
      "Epoch 192: train loss = 0.1254\n",
      "Epoch 193: train loss = 0.1242\n",
      "Epoch 194: train loss = 0.1211\n",
      "Epoch 195: train loss = 0.1298\n",
      "Epoch 196: train loss = 0.1199\n",
      "Epoch 197: train loss = 0.1196\n",
      "Epoch 198: train loss = 0.1135\n",
      "Epoch 199: train loss = 0.1242\n",
      "Epoch 200: train loss = 0.1188\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.16      0.21       151\n",
      "           1       0.80      0.90      0.85       551\n",
      "\n",
      "    accuracy                           0.74       702\n",
      "   macro avg       0.55      0.53      0.53       702\n",
      "weighted avg       0.69      0.74      0.71       702\n",
      "\n",
      "Test ROC AUC: 0.5513755844280718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_t = torch.from_numpy(X_train).float().to(device)\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\n",
    "X_test_t  = torch.from_numpy(X_test).float().to(device)\n",
    "y_test_t  = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_top.shape[1]).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 4) Training loop\n",
    "n_epochs = \n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(train_dl.dataset)\n",
    "    print(f\"Epoch {epoch:2d}: train loss = {avg_loss:.4f}\")\n",
    "\n",
    "# 5) Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1333966a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "\n",
    "scale_pos_weight = float((y_train == 0).sum()) / (y_train == 1).sum()\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\":        \"binary:logistic\",\n",
    "    \"eval_metric\":      \"auc\",\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"tree_method\":      \"hist\",       \n",
    "    \"grow_policy\":      \"lossguide\",  \n",
    "    \"max_depth\":        6,\n",
    "    \"learning_rate\":    0.1,\n",
    "    \"subsample\":        0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\":     42,\n",
    "    \"verbosity\":        1\n",
    "}\n",
    "\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=20,\n",
    "    metrics=\"auc\",\n",
    "    seed=42,\n",
    "    as_pandas=True,\n",
    "    verbose_eval=50\n",
    ")\n",
    "best_rounds = len(cv_results)\n",
    "print(f\"Optimal boosting rounds: {best_rounds}\")\n",
    "\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=best_rounds\n",
    ")\n",
    "\n",
    "\n",
    "y_prob = bst.predict(dtest)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eaf7986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train_loss = 0.5346, val_loss = 0.5233\n",
      "Epoch  2: train_loss = 0.4585, val_loss = 0.5468\n",
      "Epoch  3: train_loss = 0.4176, val_loss = 0.5566\n",
      "Epoch  4: train_loss = 0.3821, val_loss = 0.5594\n",
      "Epoch  5: train_loss = 0.3600, val_loss = 0.5835\n",
      "Epoch  6: train_loss = 0.3274, val_loss = 0.6083\n",
      "Epoch  7: train_loss = 0.2983, val_loss = 0.6271\n",
      "Epoch  8: train_loss = 0.2765, val_loss = 0.6300\n",
      "Epoch  9: train_loss = 0.2605, val_loss = 0.6660\n",
      "Epoch 10: train_loss = 0.2380, val_loss = 0.6718\n",
      "Epoch 11: train_loss = 0.2135, val_loss = 0.6833\n",
      "Epoch 12: train_loss = 0.2041, val_loss = 0.7182\n",
      "Epoch 13: train_loss = 0.1856, val_loss = 0.8112\n",
      "Epoch 14: train_loss = 0.1735, val_loss = 0.7527\n",
      "Epoch 15: train_loss = 0.1669, val_loss = 0.7794\n",
      "Epoch 16: train_loss = 0.1474, val_loss = 0.7835\n",
      "Epoch 17: train_loss = 0.1322, val_loss = 0.8707\n",
      "Epoch 18: train_loss = 0.1305, val_loss = 0.8369\n",
      "Epoch 19: train_loss = 0.1145, val_loss = 0.8303\n",
      "Epoch 20: train_loss = 0.1174, val_loss = 0.8522\n",
      "Epoch 21: train_loss = 0.0966, val_loss = 0.9116\n",
      "Epoch 22: train_loss = 0.0928, val_loss = 0.8917\n",
      "Epoch 23: train_loss = 0.0959, val_loss = 0.9233\n",
      "Epoch 24: train_loss = 0.0811, val_loss = 0.9086\n",
      "Epoch 25: train_loss = 0.0793, val_loss = 1.1182\n",
      "Epoch 26: train_loss = 0.0844, val_loss = 0.9632\n",
      "Epoch 27: train_loss = 0.0712, val_loss = 1.1308\n",
      "Epoch 28: train_loss = 0.0671, val_loss = 0.9962\n",
      "Epoch 29: train_loss = 0.0758, val_loss = 1.1632\n",
      "Epoch 30: train_loss = 0.0736, val_loss = 1.1365\n",
      "Epoch 31: train_loss = 0.0563, val_loss = 1.0391\n",
      "Epoch 32: train_loss = 0.0748, val_loss = 1.3161\n",
      "Epoch 33: train_loss = 0.0564, val_loss = 1.3063\n",
      "Epoch 34: train_loss = 0.0570, val_loss = 1.3284\n",
      "Epoch 35: train_loss = 0.0561, val_loss = 1.3239\n",
      "Epoch 36: train_loss = 0.0532, val_loss = 1.3200\n",
      "Epoch 37: train_loss = 0.0449, val_loss = 1.6106\n",
      "Epoch 38: train_loss = 0.0501, val_loss = 1.3180\n",
      "Epoch 39: train_loss = 0.0426, val_loss = 1.4618\n",
      "Epoch 40: train_loss = 0.0430, val_loss = 1.2449\n",
      "Epoch 41: train_loss = 0.0393, val_loss = 1.4040\n",
      "Epoch 42: train_loss = 0.0403, val_loss = 1.5141\n",
      "Epoch 43: train_loss = 0.0425, val_loss = 1.4022\n",
      "Epoch 44: train_loss = 0.0411, val_loss = 1.5471\n",
      "Epoch 45: train_loss = 0.0496, val_loss = 1.7025\n",
      "Epoch 46: train_loss = 0.0414, val_loss = 1.5838\n",
      "Epoch 47: train_loss = 0.0387, val_loss = 1.6615\n",
      "Epoch 48: train_loss = 0.0363, val_loss = 1.4503\n",
      "Epoch 49: train_loss = 0.0525, val_loss = 1.5693\n",
      "Epoch 50: train_loss = 0.0302, val_loss = 1.7604\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.19      0.25       151\n",
      "           1       0.80      0.90      0.85       551\n",
      "\n",
      "    accuracy                           0.75       702\n",
      "   macro avg       0.57      0.55      0.55       702\n",
      "weighted avg       0.70      0.75      0.72       702\n",
      "\n",
      "Test ROC AUC: 0.5551676061585799\n"
     ]
    }
   ],
   "source": [
    "# 3) Convert to tensors and move to device\n",
    "def to_tensor(x, y):\n",
    "    xt = torch.from_numpy(x).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return xt, yt\n",
    "\n",
    "X_tr_t, y_tr_t = to_tensor(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensor(X_val, y_val)\n",
    "X_test_t, y_test_t = to_tensor(X_test, y_test)\n",
    "\n",
    "# 4) DataLoaders\n",
    "batch_size = 8\n",
    "train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 5) Model definition\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 6) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 7) Training with validation\n",
    "n_epochs = 50\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_tr_loss += loss.item() * xb.size(0)\n",
    "    avg_tr_loss = total_tr_loss / len(train_dl.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_val_loss += loss.item() * xb.size(0)\n",
    "    avg_val_loss = total_val_loss / len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}: train_loss = {avg_tr_loss:.4f}, val_loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "# 8) Final evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "25c03ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: train_loss = 0.2867, val_loss = 0.2971\n",
      "Epoch  2: train_loss = 0.2781, val_loss = 0.2948\n",
      "Epoch  3: train_loss = 0.2746, val_loss = 0.2948\n",
      "Epoch  4: train_loss = 0.2709, val_loss = 0.2941\n",
      "Epoch  5: train_loss = 0.2699, val_loss = 0.2941\n",
      "Epoch  6: train_loss = 0.2676, val_loss = 0.2926\n",
      "Epoch  7: train_loss = 0.2661, val_loss = 0.2921\n",
      "Epoch  8: train_loss = 0.2664, val_loss = 0.2928\n",
      "Epoch  9: train_loss = 0.2647, val_loss = 0.2922\n",
      "Epoch 10: train_loss = 0.2639, val_loss = 0.2918\n",
      "Epoch 11: train_loss = 0.2616, val_loss = 0.2929\n",
      "Epoch 12: train_loss = 0.2608, val_loss = 0.2946\n",
      "Epoch 13: train_loss = 0.2593, val_loss = 0.2928\n",
      "Epoch 14: train_loss = 0.2595, val_loss = 0.2940\n",
      "Epoch 15: train_loss = 0.2578, val_loss = 0.2954\n",
      "Epoch 16: train_loss = 0.2572, val_loss = 0.2954\n",
      "Epoch 17: train_loss = 0.2563, val_loss = 0.2979\n",
      "Epoch 18: train_loss = 0.2554, val_loss = 0.2972\n",
      "Epoch 19: train_loss = 0.2561, val_loss = 0.2954\n",
      "Epoch 20: train_loss = 0.2542, val_loss = 0.2954\n",
      "Epoch 21: train_loss = 0.2540, val_loss = 0.2950\n",
      "Epoch 22: train_loss = 0.2528, val_loss = 0.2988\n",
      "Epoch 23: train_loss = 0.2521, val_loss = 0.3003\n",
      "Epoch 24: train_loss = 0.2513, val_loss = 0.2990\n",
      "Epoch 25: train_loss = 0.2515, val_loss = 0.2991\n",
      "Epoch 26: train_loss = 0.2498, val_loss = 0.2995\n",
      "Epoch 27: train_loss = 0.2497, val_loss = 0.2990\n",
      "Epoch 28: train_loss = 0.2494, val_loss = 0.2991\n",
      "Epoch 29: train_loss = 0.2494, val_loss = 0.2982\n",
      "Epoch 30: train_loss = 0.2488, val_loss = 0.3013\n",
      "Epoch 31: train_loss = 0.2478, val_loss = 0.3000\n",
      "Epoch 32: train_loss = 0.2475, val_loss = 0.3004\n",
      "Epoch 33: train_loss = 0.2466, val_loss = 0.3002\n",
      "Epoch 34: train_loss = 0.2454, val_loss = 0.3011\n",
      "Epoch 35: train_loss = 0.2467, val_loss = 0.3005\n",
      "Epoch 36: train_loss = 0.2460, val_loss = 0.3037\n",
      "Epoch 37: train_loss = 0.2464, val_loss = 0.2974\n",
      "Epoch 38: train_loss = 0.2462, val_loss = 0.2992\n",
      "Epoch 39: train_loss = 0.2449, val_loss = 0.2986\n",
      "Epoch 40: train_loss = 0.2440, val_loss = 0.3027\n",
      "Epoch 41: train_loss = 0.2453, val_loss = 0.3044\n",
      "Epoch 42: train_loss = 0.2440, val_loss = 0.3047\n",
      "Epoch 43: train_loss = 0.2433, val_loss = 0.3067\n",
      "Epoch 44: train_loss = 0.2424, val_loss = 0.3040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, yb)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m total_tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/surprise_sae/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/surprise_sae/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/surprise_sae/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:94\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_grads\u001b[39m(\n\u001b[1;32m     89\u001b[0m     outputs: Union[Sequence[torch\u001b[38;5;241m.\u001b[39mTensor], Sequence[graph\u001b[38;5;241m.\u001b[39mGradientEdge]],\n\u001b[1;32m     90\u001b[0m     grads: Sequence[_OptionalTensor],\n\u001b[1;32m     91\u001b[0m     is_grads_batched: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_OptionalTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     93\u001b[0m     new_grads: List[_OptionalTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m out, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     95\u001b[0m         out \u001b[38;5;241m=\u001b[39m cast(Union[torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge], out)\n\u001b[1;32m     96\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3) Convert to tensors and move to device\n",
    "def to_tensor(x, y):\n",
    "    xt = torch.from_numpy(x).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return xt, yt\n",
    "\n",
    "X_tr_t, y_tr_t = to_tensor(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensor(X_val, y_val)\n",
    "X_test_t, y_test_t = to_tensor(X_test, y_test)\n",
    "\n",
    "# 4) DataLoaders\n",
    "batch_size = 8\n",
    "train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "pos = np.sum(y_train == 1)\n",
    "neg = np.sum(y_train == 0)\n",
    "pos_weight = torch.tensor(neg / pos, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# 5) Model definition\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ShallowMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 6) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "# 7) Training with validation\n",
    "n_epochs = 120\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_tr_loss += loss.item() * xb.size(0)\n",
    "    avg_tr_loss = total_tr_loss / len(train_dl.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_val_loss += loss.item() * xb.size(0)\n",
    "    avg_val_loss = total_val_loss / len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}: train_loss = {avg_tr_loss:.4f}, val_loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "# 8) Final evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6aa2acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train Loss: 0.6615  Val Loss: 0.6759\n",
      "Epoch  2  Train Loss: 0.6653  Val Loss: 0.6752\n",
      "Epoch  3  Train Loss: 0.6513  Val Loss: 0.6758\n",
      "Epoch  4  Train Loss: 0.6523  Val Loss: 0.6753\n",
      "Epoch  5  Train Loss: 0.6579  Val Loss: 0.6786\n",
      "Epoch  6  Train Loss: 0.6435  Val Loss: 0.6728\n",
      "Epoch  7  Train Loss: 0.6529  Val Loss: 0.6724\n",
      "Epoch  8  Train Loss: 0.6460  Val Loss: 0.6756\n",
      "Epoch  9  Train Loss: 0.6446  Val Loss: 0.6652\n",
      "Epoch 10  Train Loss: 0.6501  Val Loss: 0.6692\n",
      "Epoch 11  Train Loss: 0.6375  Val Loss: 0.6672\n",
      "Epoch 12  Train Loss: 0.6455  Val Loss: 0.6650\n",
      "Epoch 13  Train Loss: 0.6357  Val Loss: 0.6679\n",
      "Epoch 14  Train Loss: 0.6393  Val Loss: 0.6670\n",
      "Epoch 15  Train Loss: 0.6389  Val Loss: 0.6633\n",
      "Epoch 16  Train Loss: 0.6352  Val Loss: 0.6626\n",
      "Epoch 17  Train Loss: 0.6397  Val Loss: 0.6672\n",
      "Epoch 18  Train Loss: 0.6317  Val Loss: 0.6595\n",
      "Epoch 19  Train Loss: 0.6189  Val Loss: 0.6623\n",
      "Epoch 20  Train Loss: 0.6198  Val Loss: 0.6657\n",
      "Epoch 21  Train Loss: 0.6286  Val Loss: 0.6639\n",
      "Epoch 22  Train Loss: 0.6182  Val Loss: 0.6619\n",
      "Epoch 23  Train Loss: 0.6273  Val Loss: 0.6648\n",
      "Epoch 24  Train Loss: 0.6273  Val Loss: 0.6602\n",
      "Epoch 25  Train Loss: 0.6193  Val Loss: 0.6578\n",
      "Epoch 26  Train Loss: 0.6289  Val Loss: 0.6598\n",
      "Epoch 27  Train Loss: 0.6138  Val Loss: 0.6570\n",
      "Epoch 28  Train Loss: 0.6257  Val Loss: 0.6559\n",
      "Epoch 29  Train Loss: 0.6200  Val Loss: 0.6588\n",
      "Epoch 30  Train Loss: 0.6154  Val Loss: 0.6558\n",
      "Epoch 31  Train Loss: 0.6132  Val Loss: 0.6578\n",
      "Epoch 32  Train Loss: 0.6138  Val Loss: 0.6524\n",
      "Epoch 33  Train Loss: 0.6065  Val Loss: 0.6585\n",
      "Epoch 34  Train Loss: 0.6042  Val Loss: 0.6518\n",
      "Epoch 35  Train Loss: 0.6107  Val Loss: 0.6517\n",
      "Epoch 36  Train Loss: 0.6115  Val Loss: 0.6506\n",
      "Epoch 37  Train Loss: 0.6035  Val Loss: 0.6499\n",
      "Epoch 38  Train Loss: 0.6008  Val Loss: 0.6496\n",
      "Epoch 39  Train Loss: 0.5992  Val Loss: 0.6481\n",
      "Epoch 40  Train Loss: 0.5965  Val Loss: 0.6555\n",
      "Epoch 41  Train Loss: 0.5953  Val Loss: 0.6482\n",
      "Epoch 42  Train Loss: 0.5965  Val Loss: 0.6530\n",
      "Epoch 43  Train Loss: 0.5919  Val Loss: 0.6452\n",
      "Epoch 44  Train Loss: 0.5981  Val Loss: 0.6483\n",
      "Epoch 45  Train Loss: 0.5982  Val Loss: 0.6429\n",
      "Epoch 46  Train Loss: 0.5860  Val Loss: 0.6459\n",
      "Epoch 47  Train Loss: 0.5866  Val Loss: 0.6463\n",
      "Epoch 48  Train Loss: 0.5912  Val Loss: 0.6450\n",
      "Epoch 49  Train Loss: 0.5813  Val Loss: 0.6436\n",
      "Epoch 50  Train Loss: 0.5883  Val Loss: 0.6426\n",
      "Epoch 51  Train Loss: 0.5842  Val Loss: 0.6442\n",
      "Epoch 52  Train Loss: 0.5850  Val Loss: 0.6434\n",
      "Epoch 53  Train Loss: 0.5840  Val Loss: 0.6435\n",
      "Epoch 54  Train Loss: 0.5822  Val Loss: 0.6413\n",
      "Epoch 55  Train Loss: 0.5707  Val Loss: 0.6350\n",
      "Epoch 56  Train Loss: 0.5762  Val Loss: 0.6390\n",
      "Epoch 57  Train Loss: 0.5781  Val Loss: 0.6374\n",
      "Epoch 58  Train Loss: 0.5800  Val Loss: 0.6390\n",
      "Epoch 59  Train Loss: 0.5710  Val Loss: 0.6424\n",
      "Epoch 60  Train Loss: 0.5698  Val Loss: 0.6386\n",
      "Epoch 61  Train Loss: 0.5742  Val Loss: 0.6432\n",
      "Epoch 62  Train Loss: 0.5698  Val Loss: 0.6391\n",
      "Epoch 63  Train Loss: 0.5733  Val Loss: 0.6422\n",
      "Epoch 64  Train Loss: 0.5708  Val Loss: 0.6379\n",
      "Epoch 65  Train Loss: 0.5702  Val Loss: 0.6325\n",
      "Epoch 66  Train Loss: 0.5711  Val Loss: 0.6347\n",
      "Epoch 67  Train Loss: 0.5695  Val Loss: 0.6325\n",
      "Epoch 68  Train Loss: 0.5621  Val Loss: 0.6391\n",
      "Epoch 69  Train Loss: 0.5665  Val Loss: 0.6359\n",
      "Epoch 70  Train Loss: 0.5638  Val Loss: 0.6315\n",
      "Epoch 71  Train Loss: 0.5573  Val Loss: 0.6301\n",
      "Epoch 72  Train Loss: 0.5549  Val Loss: 0.6398\n",
      "Epoch 73  Train Loss: 0.5561  Val Loss: 0.6332\n",
      "Epoch 74  Train Loss: 0.5575  Val Loss: 0.6339\n",
      "Epoch 75  Train Loss: 0.5544  Val Loss: 0.6307\n",
      "Epoch 76  Train Loss: 0.5552  Val Loss: 0.6264\n",
      "Epoch 77  Train Loss: 0.5490  Val Loss: 0.6285\n",
      "Epoch 78  Train Loss: 0.5526  Val Loss: 0.6322\n",
      "Epoch 79  Train Loss: 0.5505  Val Loss: 0.6317\n",
      "Epoch 80  Train Loss: 0.5444  Val Loss: 0.6290\n",
      "Epoch 81  Train Loss: 0.5528  Val Loss: 0.6255\n",
      "Epoch 82  Train Loss: 0.5466  Val Loss: 0.6272\n",
      "Epoch 83  Train Loss: 0.5460  Val Loss: 0.6242\n",
      "Epoch 84  Train Loss: 0.5446  Val Loss: 0.6231\n",
      "Epoch 85  Train Loss: 0.5500  Val Loss: 0.6257\n",
      "Epoch 86  Train Loss: 0.5463  Val Loss: 0.6248\n",
      "Epoch 87  Train Loss: 0.5442  Val Loss: 0.6228\n",
      "Epoch 88  Train Loss: 0.5379  Val Loss: 0.6292\n",
      "Epoch 89  Train Loss: 0.5408  Val Loss: 0.6252\n",
      "Epoch 90  Train Loss: 0.5436  Val Loss: 0.6279\n",
      "Epoch 91  Train Loss: 0.5350  Val Loss: 0.6270\n",
      "Epoch 92  Train Loss: 0.5418  Val Loss: 0.6243\n",
      "Epoch 93  Train Loss: 0.5364  Val Loss: 0.6304\n",
      "Epoch 94  Train Loss: 0.5387  Val Loss: 0.6229\n",
      "Epoch 95  Train Loss: 0.5299  Val Loss: 0.6287\n",
      "Epoch 96  Train Loss: 0.5269  Val Loss: 0.6222\n",
      "Epoch 97  Train Loss: 0.5403  Val Loss: 0.6175\n",
      "Epoch 98  Train Loss: 0.5386  Val Loss: 0.6231\n",
      "Epoch 99  Train Loss: 0.5302  Val Loss: 0.6220\n",
      "Epoch 100  Train Loss: 0.5243  Val Loss: 0.6289\n",
      "Epoch 101  Train Loss: 0.5246  Val Loss: 0.6209\n",
      "Epoch 102  Train Loss: 0.5245  Val Loss: 0.6239\n",
      "Epoch 103  Train Loss: 0.5315  Val Loss: 0.6203\n",
      "Epoch 104  Train Loss: 0.5310  Val Loss: 0.6133\n",
      "Epoch 105  Train Loss: 0.5273  Val Loss: 0.6208\n",
      "Epoch 106  Train Loss: 0.5183  Val Loss: 0.6209\n",
      "Epoch 107  Train Loss: 0.5189  Val Loss: 0.6209\n",
      "Epoch 108  Train Loss: 0.5236  Val Loss: 0.6191\n",
      "Epoch 109  Train Loss: 0.5167  Val Loss: 0.6157\n",
      "Epoch 110  Train Loss: 0.5184  Val Loss: 0.6261\n",
      "Epoch 111  Train Loss: 0.5174  Val Loss: 0.6172\n",
      "Epoch 112  Train Loss: 0.5096  Val Loss: 0.6134\n",
      "Epoch 113  Train Loss: 0.5084  Val Loss: 0.6157\n",
      "Epoch 114  Train Loss: 0.5098  Val Loss: 0.6159\n",
      "Epoch 115  Train Loss: 0.5162  Val Loss: 0.6149\n",
      "Epoch 116  Train Loss: 0.5140  Val Loss: 0.6160\n",
      "Epoch 117  Train Loss: 0.5212  Val Loss: 0.6114\n",
      "Epoch 118  Train Loss: 0.5122  Val Loss: 0.6135\n",
      "Epoch 119  Train Loss: 0.5001  Val Loss: 0.6111\n",
      "Epoch 120  Train Loss: 0.5173  Val Loss: 0.6097\n",
      "Epoch 121  Train Loss: 0.5076  Val Loss: 0.6179\n",
      "Epoch 122  Train Loss: 0.5061  Val Loss: 0.6155\n",
      "Epoch 123  Train Loss: 0.5156  Val Loss: 0.6142\n",
      "Epoch 124  Train Loss: 0.5046  Val Loss: 0.6249\n",
      "Epoch 125  Train Loss: 0.5060  Val Loss: 0.6114\n",
      "Epoch 126  Train Loss: 0.5010  Val Loss: 0.6153\n",
      "Epoch 127  Train Loss: 0.5038  Val Loss: 0.6098\n",
      "Epoch 128  Train Loss: 0.5061  Val Loss: 0.6084\n",
      "Epoch 129  Train Loss: 0.4981  Val Loss: 0.6081\n",
      "Epoch 130  Train Loss: 0.4994  Val Loss: 0.6167\n",
      "Epoch 131  Train Loss: 0.5011  Val Loss: 0.6118\n",
      "Epoch 132  Train Loss: 0.4992  Val Loss: 0.6142\n",
      "Epoch 133  Train Loss: 0.4958  Val Loss: 0.6098\n",
      "Epoch 134  Train Loss: 0.5025  Val Loss: 0.6158\n",
      "Epoch 135  Train Loss: 0.4990  Val Loss: 0.6127\n",
      "Epoch 136  Train Loss: 0.4946  Val Loss: 0.6179\n",
      "Epoch 137  Train Loss: 0.4862  Val Loss: 0.6175\n",
      "Epoch 138  Train Loss: 0.4870  Val Loss: 0.6114\n",
      "Epoch 139  Train Loss: 0.4832  Val Loss: 0.6124\n",
      "Epoch 140  Train Loss: 0.4950  Val Loss: 0.6088\n",
      "Epoch 141  Train Loss: 0.4866  Val Loss: 0.6127\n",
      "Epoch 142  Train Loss: 0.4859  Val Loss: 0.6141\n",
      "Epoch 143  Train Loss: 0.4865  Val Loss: 0.6093\n",
      "Epoch 144  Train Loss: 0.4775  Val Loss: 0.6067\n",
      "Epoch 145  Train Loss: 0.4832  Val Loss: 0.6095\n",
      "Epoch 146  Train Loss: 0.4833  Val Loss: 0.6090\n",
      "Epoch 147  Train Loss: 0.4808  Val Loss: 0.6180\n",
      "Epoch 148  Train Loss: 0.4824  Val Loss: 0.6106\n",
      "Epoch 149  Train Loss: 0.4750  Val Loss: 0.6099\n",
      "Epoch 150  Train Loss: 0.4815  Val Loss: 0.6154\n",
      "Epoch 151  Train Loss: 0.4796  Val Loss: 0.6173\n",
      "Epoch 152  Train Loss: 0.4796  Val Loss: 0.6095\n",
      "Epoch 153  Train Loss: 0.4682  Val Loss: 0.6133\n",
      "Epoch 154  Train Loss: 0.4708  Val Loss: 0.6082\n",
      "Epoch 155  Train Loss: 0.4708  Val Loss: 0.6053\n",
      "Epoch 156  Train Loss: 0.4704  Val Loss: 0.6084\n",
      "Epoch 157  Train Loss: 0.4712  Val Loss: 0.6099\n",
      "Epoch 158  Train Loss: 0.4713  Val Loss: 0.6120\n",
      "Epoch 159  Train Loss: 0.4696  Val Loss: 0.6095\n",
      "Epoch 160  Train Loss: 0.4697  Val Loss: 0.6132\n",
      "Epoch 161  Train Loss: 0.4774  Val Loss: 0.6139\n",
      "Epoch 162  Train Loss: 0.4729  Val Loss: 0.6104\n",
      "Epoch 163  Train Loss: 0.4631  Val Loss: 0.6088\n",
      "Epoch 164  Train Loss: 0.4615  Val Loss: 0.6103\n",
      "Epoch 165  Train Loss: 0.4629  Val Loss: 0.6045\n",
      "Epoch 166  Train Loss: 0.4685  Val Loss: 0.6063\n",
      "Epoch 167  Train Loss: 0.4589  Val Loss: 0.6041\n",
      "Epoch 168  Train Loss: 0.4560  Val Loss: 0.6139\n",
      "Epoch 169  Train Loss: 0.4588  Val Loss: 0.6067\n",
      "Epoch 170  Train Loss: 0.4576  Val Loss: 0.6032\n",
      "Epoch 171  Train Loss: 0.4541  Val Loss: 0.6118\n",
      "Epoch 172  Train Loss: 0.4517  Val Loss: 0.6113\n",
      "Epoch 173  Train Loss: 0.4479  Val Loss: 0.6099\n",
      "Epoch 174  Train Loss: 0.4470  Val Loss: 0.6095\n",
      "Epoch 175  Train Loss: 0.4510  Val Loss: 0.6129\n",
      "Epoch 176  Train Loss: 0.4510  Val Loss: 0.6079\n",
      "Epoch 177  Train Loss: 0.4480  Val Loss: 0.6117\n",
      "Epoch 178  Train Loss: 0.4424  Val Loss: 0.6027\n",
      "Epoch 179  Train Loss: 0.4495  Val Loss: 0.6133\n",
      "Epoch 180  Train Loss: 0.4476  Val Loss: 0.6105\n",
      "Epoch 181  Train Loss: 0.4406  Val Loss: 0.6056\n",
      "Epoch 182  Train Loss: 0.4443  Val Loss: 0.6099\n",
      "Epoch 183  Train Loss: 0.4434  Val Loss: 0.5978\n",
      "Epoch 184  Train Loss: 0.4439  Val Loss: 0.6084\n",
      "Epoch 185  Train Loss: 0.4353  Val Loss: 0.6096\n",
      "Epoch 186  Train Loss: 0.4314  Val Loss: 0.6176\n",
      "Epoch 187  Train Loss: 0.4301  Val Loss: 0.6055\n",
      "Epoch 188  Train Loss: 0.4315  Val Loss: 0.6008\n",
      "Epoch 189  Train Loss: 0.4338  Val Loss: 0.6157\n",
      "Epoch 190  Train Loss: 0.4271  Val Loss: 0.6061\n",
      "Epoch 191  Train Loss: 0.4297  Val Loss: 0.6024\n",
      "Epoch 192  Train Loss: 0.4282  Val Loss: 0.6063\n",
      "Epoch 193  Train Loss: 0.4312  Val Loss: 0.6106\n",
      "Epoch 194  Train Loss: 0.4220  Val Loss: 0.6037\n",
      "Epoch 195  Train Loss: 0.4216  Val Loss: 0.6102\n",
      "Epoch 196  Train Loss: 0.4244  Val Loss: 0.6101\n",
      "Epoch 197  Train Loss: 0.4234  Val Loss: 0.6109\n",
      "Epoch 198  Train Loss: 0.4256  Val Loss: 0.6160\n",
      "Epoch 199  Train Loss: 0.4139  Val Loss: 0.6067\n",
      "Epoch 200  Train Loss: 0.4238  Val Loss: 0.6053\n",
      "Epoch 201  Train Loss: 0.4239  Val Loss: 0.6033\n",
      "Epoch 202  Train Loss: 0.4129  Val Loss: 0.6097\n",
      "Epoch 203  Train Loss: 0.4189  Val Loss: 0.6066\n",
      "Epoch 204  Train Loss: 0.4079  Val Loss: 0.6134\n",
      "Epoch 205  Train Loss: 0.4200  Val Loss: 0.6080\n",
      "Epoch 206  Train Loss: 0.4144  Val Loss: 0.6090\n",
      "Epoch 207  Train Loss: 0.4072  Val Loss: 0.6161\n",
      "Epoch 208  Train Loss: 0.4094  Val Loss: 0.6115\n",
      "Epoch 209  Train Loss: 0.4126  Val Loss: 0.6059\n",
      "Epoch 210  Train Loss: 0.4024  Val Loss: 0.6151\n",
      "Epoch 211  Train Loss: 0.4019  Val Loss: 0.6100\n",
      "Epoch 212  Train Loss: 0.3996  Val Loss: 0.6022\n",
      "Epoch 213  Train Loss: 0.4013  Val Loss: 0.6083\n",
      "Epoch 214  Train Loss: 0.4036  Val Loss: 0.6023\n",
      "Epoch 215  Train Loss: 0.3974  Val Loss: 0.6068\n",
      "Epoch 216  Train Loss: 0.4052  Val Loss: 0.6035\n",
      "Epoch 217  Train Loss: 0.3951  Val Loss: 0.6043\n",
      "Epoch 218  Train Loss: 0.3966  Val Loss: 0.6141\n",
      "Epoch 219  Train Loss: 0.3966  Val Loss: 0.6100\n",
      "Epoch 220  Train Loss: 0.3935  Val Loss: 0.6134\n",
      "Epoch 221  Train Loss: 0.3809  Val Loss: 0.6160\n",
      "Epoch 222  Train Loss: 0.3839  Val Loss: 0.6007\n",
      "Epoch 223  Train Loss: 0.3881  Val Loss: 0.6060\n",
      "Epoch 224  Train Loss: 0.3868  Val Loss: 0.6079\n",
      "Epoch 225  Train Loss: 0.3965  Val Loss: 0.6064\n",
      "Epoch 226  Train Loss: 0.3808  Val Loss: 0.6153\n",
      "Epoch 227  Train Loss: 0.3834  Val Loss: 0.6162\n",
      "Epoch 228  Train Loss: 0.3756  Val Loss: 0.6170\n",
      "Epoch 229  Train Loss: 0.3823  Val Loss: 0.6039\n",
      "Epoch 230  Train Loss: 0.3786  Val Loss: 0.6077\n",
      "Epoch 231  Train Loss: 0.3703  Val Loss: 0.6182\n",
      "Epoch 232  Train Loss: 0.3744  Val Loss: 0.6173\n",
      "Epoch 233  Train Loss: 0.3743  Val Loss: 0.6155\n",
      "Epoch 234  Train Loss: 0.3685  Val Loss: 0.6107\n",
      "Epoch 235  Train Loss: 0.3717  Val Loss: 0.6141\n",
      "Epoch 236  Train Loss: 0.3725  Val Loss: 0.6135\n",
      "Epoch 237  Train Loss: 0.3726  Val Loss: 0.6139\n",
      "Epoch 238  Train Loss: 0.3689  Val Loss: 0.6162\n",
      "Epoch 239  Train Loss: 0.3635  Val Loss: 0.6177\n",
      "Epoch 240  Train Loss: 0.3622  Val Loss: 0.6159\n",
      "Epoch 241  Train Loss: 0.3620  Val Loss: 0.6067\n",
      "Epoch 242  Train Loss: 0.3630  Val Loss: 0.6194\n",
      "Epoch 243  Train Loss: 0.3594  Val Loss: 0.6176\n",
      "Epoch 244  Train Loss: 0.3626  Val Loss: 0.6126\n",
      "Epoch 245  Train Loss: 0.3521  Val Loss: 0.6215\n",
      "Epoch 246  Train Loss: 0.3604  Val Loss: 0.6138\n",
      "Epoch 247  Train Loss: 0.3539  Val Loss: 0.6176\n",
      "Epoch 248  Train Loss: 0.3493  Val Loss: 0.6136\n",
      "Epoch 249  Train Loss: 0.3483  Val Loss: 0.6206\n",
      "Epoch 250  Train Loss: 0.3484  Val Loss: 0.6168\n",
      "Epoch 251  Train Loss: 0.3512  Val Loss: 0.6169\n",
      "Epoch 252  Train Loss: 0.3460  Val Loss: 0.6215\n",
      "Epoch 253  Train Loss: 0.3410  Val Loss: 0.6192\n",
      "Epoch 254  Train Loss: 0.3454  Val Loss: 0.6112\n",
      "Epoch 255  Train Loss: 0.3392  Val Loss: 0.6231\n",
      "Epoch 256  Train Loss: 0.3430  Val Loss: 0.6195\n",
      "Epoch 257  Train Loss: 0.3381  Val Loss: 0.6144\n",
      "Epoch 258  Train Loss: 0.3360  Val Loss: 0.6215\n",
      "Epoch 259  Train Loss: 0.3372  Val Loss: 0.6196\n",
      "Epoch 260  Train Loss: 0.3399  Val Loss: 0.6218\n",
      "Epoch 261  Train Loss: 0.3334  Val Loss: 0.6067\n",
      "Epoch 262  Train Loss: 0.3350  Val Loss: 0.6164\n",
      "Epoch 263  Train Loss: 0.3317  Val Loss: 0.6290\n",
      "Epoch 264  Train Loss: 0.3344  Val Loss: 0.6176\n",
      "Epoch 265  Train Loss: 0.3222  Val Loss: 0.6284\n",
      "Epoch 266  Train Loss: 0.3202  Val Loss: 0.6229\n",
      "Epoch 267  Train Loss: 0.3274  Val Loss: 0.6277\n",
      "Epoch 268  Train Loss: 0.3219  Val Loss: 0.6284\n",
      "Epoch 269  Train Loss: 0.3134  Val Loss: 0.6143\n",
      "Epoch 270  Train Loss: 0.3230  Val Loss: 0.6191\n",
      "Epoch 271  Train Loss: 0.3191  Val Loss: 0.6097\n",
      "Epoch 272  Train Loss: 0.3098  Val Loss: 0.6363\n",
      "Epoch 273  Train Loss: 0.3076  Val Loss: 0.6160\n",
      "Epoch 274  Train Loss: 0.3165  Val Loss: 0.6108\n",
      "Epoch 275  Train Loss: 0.3108  Val Loss: 0.6156\n",
      "Epoch 276  Train Loss: 0.3058  Val Loss: 0.6104\n",
      "Epoch 277  Train Loss: 0.3111  Val Loss: 0.6209\n",
      "Epoch 278  Train Loss: 0.3056  Val Loss: 0.6149\n",
      "Epoch 279  Train Loss: 0.3035  Val Loss: 0.6219\n",
      "Epoch 280  Train Loss: 0.3053  Val Loss: 0.6207\n",
      "Epoch 281  Train Loss: 0.3010  Val Loss: 0.6231\n",
      "Epoch 282  Train Loss: 0.2986  Val Loss: 0.6211\n",
      "Epoch 283  Train Loss: 0.3060  Val Loss: 0.6182\n",
      "Epoch 284  Train Loss: 0.2995  Val Loss: 0.6357\n",
      "Epoch 285  Train Loss: 0.2939  Val Loss: 0.6103\n",
      "Epoch 286  Train Loss: 0.2916  Val Loss: 0.6400\n",
      "Epoch 287  Train Loss: 0.2954  Val Loss: 0.6230\n",
      "Epoch 288  Train Loss: 0.2950  Val Loss: 0.5990\n",
      "Epoch 289  Train Loss: 0.2887  Val Loss: 0.6320\n",
      "Epoch 290  Train Loss: 0.2896  Val Loss: 0.6205\n",
      "Epoch 291  Train Loss: 0.2886  Val Loss: 0.6319\n",
      "Epoch 292  Train Loss: 0.2849  Val Loss: 0.6255\n",
      "Epoch 293  Train Loss: 0.2821  Val Loss: 0.6524\n",
      "Epoch 294  Train Loss: 0.2797  Val Loss: 0.6297\n",
      "Epoch 295  Train Loss: 0.2789  Val Loss: 0.6299\n",
      "Epoch 296  Train Loss: 0.2754  Val Loss: 0.6452\n",
      "Epoch 297  Train Loss: 0.2691  Val Loss: 0.6193\n",
      "Epoch 298  Train Loss: 0.2774  Val Loss: 0.6391\n",
      "Epoch 299  Train Loss: 0.2729  Val Loss: 0.6297\n",
      "Epoch 300  Train Loss: 0.2689  Val Loss: 0.6367\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.25      0.28       151\n",
      "         1.0       0.81      0.87      0.84       551\n",
      "\n",
      "    accuracy                           0.74       702\n",
      "   macro avg       0.57      0.56      0.56       702\n",
      "weighted avg       0.71      0.74      0.72       702\n",
      "\n",
      "Test ROC AUC: 0.5903054049831131\n"
     ]
    }
   ],
   "source": [
    "# 3 layer mlp\n",
    "\n",
    "def to_tensor_dataset(X, y):\n",
    "    Xt = torch.from_numpy(X).float().to(device)\n",
    "    yt = torch.from_numpy(y).float().unsqueeze(1).to(device)\n",
    "    return TensorDataset(Xt, yt)\n",
    "\n",
    "train_ds = to_tensor_dataset(X_train, y_train)\n",
    "val_ds   = to_tensor_dataset(X_val, y_val)\n",
    "test_ds  = to_tensor_dataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3) Define the 3-layer MLP\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ThreeLayerMLP(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# 4) Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5) Training & Validation Loop\n",
    "n_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # -- Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # -- Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "    val_loss /= len(val_dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Optional: save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 6) Load best model and test evaluation\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_probs = []\n",
    "y_true  = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        probs = model(xb)\n",
    "        y_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "        y_true .extend(yb.cpu().numpy().flatten().tolist())\n",
    "\n",
    "y_pred = (np.array(y_probs) >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_true, y_probs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
